[{"categories":null,"content":"Modern Python, less modern me I’ve been working for the past few months at The DataLab — now Bennett Institute for Applied Data Science. There, Python is the main language for development and collectively the developers have a lot of Python experience. Because of that, I’ve seen a lot more modern Python than I was accustomed to seeing or writing. What I’ve found is what I knew about Python is a little outdated. Certainly there have been a lot of new Python features introduced that I didn’t know about or only vaguely had heard of, e.g.: dataclasses assignment expressions, i.e. := breakpoint() instead of having to import and use the pdb package directly It seemed wise to me then to watch some recent conference talks. I may write up notes on them. All that said, let’s look at the first one. ","date":"2021-10-15","objectID":"/posts/pycon-2021-secure-software-supply-chains/:1:0","tags":["PyCon","Python"],"title":"PyCon 2021: Secure software supply chains","uri":"/posts/pycon-2021-secure-software-supply-chains/"},{"categories":null,"content":"Secure Software Supply Chains for Python This is a summary of this talk by Dustin Ingram, a PyPI maintainer. The talk describes: What software supply chain attacks are. The current best Python practice for developes. What improvements could be made to pip and PyPI in future. ","date":"2021-10-15","objectID":"/posts/pycon-2021-secure-software-supply-chains/:2:0","tags":["PyCon","Python"],"title":"PyCon 2021: Secure software supply chains","uri":"/posts/pycon-2021-secure-software-supply-chains/"},{"categories":null,"content":"Supply chain attacks They are a thing. Involve a compromise of your dependencies, where they are distributed and everything used to build them. This applies recursively to your dependencies' dependencies and so on. ","date":"2021-10-15","objectID":"/posts/pycon-2021-secure-software-supply-chains/:2:1","tags":["PyCon","Python"],"title":"PyCon 2021: Secure software supply chains","uri":"/posts/pycon-2021-secure-software-supply-chains/"},{"categories":null,"content":"Types of supply chain attack MITM attack: intercepting a package download and replacing it with something malicious. Typosquatting: uploading malicious packages with names similar to well-known packages. Dependency confusion: where internal, non-public packages are used, but pip is misconfigured to check PyPI first and someone registers a malicious public package with the same name as the internal package. “Research”: people deliberately introducing vulnerabilities into code. SolarWinds was mentioned as an example: compromise of build system. Compromised maintainers. Compromised source control. Leaked passwords/API tokens. Social engineering. ","date":"2021-10-15","objectID":"/posts/pycon-2021-secure-software-supply-chains/:2:2","tags":["PyCon","Python"],"title":"PyCon 2021: Secure software supply chains","uri":"/posts/pycon-2021-secure-software-supply-chains/"},{"categories":null,"content":"Python best practices For Python, best current practices are: Use pinned requirements and with hashes, e.g. via pipenv lock, or pip-compile --generate-hashes. Use some notification service that monitors dependencies in your source, e.g. Dependabot (this is convenient, but is an extra component in the supply chain: you have to trust that Dependabot isn’t creating malicious pull requests). There are other possible future improvements to PyPI and pip: Removal of setup.py support to stop arbitrary code being run at install time. That might save you if you realise you’ve installed a compromised package, before importing and using the package. Signed repository metadata (PEP 458). Having namespaces on PyPI. ","date":"2021-10-15","objectID":"/posts/pycon-2021-secure-software-supply-chains/:2:3","tags":["PyCon","Python"],"title":"PyCon 2021: Secure software supply chains","uri":"/posts/pycon-2021-secure-software-supply-chains/"},{"categories":null,"content":"“UNIX: A History and a Memoir” is from the perspective of Brian Kernighan, who was around at the inception of UNIX, and is well-known in computing himself as one of the three authors of the AWK language and the author of a number of other influential books (e.g. “The C Programming Language” by Kernighan and Ritchie). The title covers most of what the book deals with: the formative years of UNIX, with explanation of the key software that was developed as part of it. It explains where UNIX was developed and who did that work. Initially, UNIX was one of Ken Thompson’s side projects, quickly capturing wider interest and sparking collaboration within Bell Labs. This was all amid a milieu of Bell Labs having considerable operating system development experience as a collective, yet having no operating system to work on. (Bell Labs had been working on the Multics project, but eventually withdrew its support.) Eventually, management managed to gather funding for UNIX, initially with a view to improving how patent applications within Bell Labs were prepared. From there, UNIX developed sufficiently that it found wider use within Bell Labs, within AT\u0026T — Bell Labs' then parent company — and within many universities. Eventually UNIX was sold commercially. Later, while various commercial UNIX vendors expended much efforts in commercial wrangling of various parties, the appearance of the free Linux kernel in the early 1990s paved the way for Linux distributions to render the commercial squabbles largely obsolete. Kernighan’s rundown of the features that made UNIX distinct from many of the competitors at the time explains why UNIX was a key development that transformed computing at the time, offering: a hierarchical file system; accessible system calls; abstraction of other concepts, such as hardware devices, that they could be worked with much as regular files; a command-line shell that could be used composably, along with a suite of programs as useful tools, pipes and scripting to connect those programs together to effectively create new commands or programs. Half a century on and these features are still evident in Linux and other descendants that are still being developed today. Kernighan closes the book by describing factors that made Bell Labs such a successful research institution. That in itself is an interesting story that entwines with the development of UNIX. My distillation is that Bell Labs had an extended period where the organisation: had interesting and challenging problems to solve; hired very talented staff to solve those problems; had sufficient resources that the staff did not need to worry about funding, while management could take a longer view on projects without demanding immediate results; had a technically competent management; offered a collaborative and fun environment for this research. All easier written than done, of course. If it were so simple, that research model would be simply replicated everywhere given the resources. Nonetheless, this goes some way to summarise why Bell Labs were so extraordinarily influential and tremendously prolific in 20th century technology developments. Finally, the book also puts the development of UNIX into context in modern-day computing. Many computing devices today run operating systems that have some connection back to UNIX. Even Microsoft has adopted several of the ideas made popular by UNIX, and has, at times, veered closer to UNIX and its descendants, offering Windows Services for UNIX in the past, Windows Subsystem for Linux in the present and, long ago, even distributing XENIX, Microsoft’s very own UNIX distribution! While UNIX is over 50 years old, it continues to influence today’s computing: this well-written book certainly helps to understand why that is so. ","date":"2021-05-08","objectID":"/posts/a-review-of-unix-a-history-and-a-memoir/:0:0","tags":["book","computing","history","Linux","review","UNIX"],"title":"A review of \"UNIX: A History and a Memoir\"","uri":"/posts/a-review-of-unix-a-history-and-a-memoir/"},{"categories":null,"content":"Having recently gone through an extensive process — well, thirty minutes or so of searching — to find a piece of music, I wanted to document the approaches I know of for this process. Here, for “music”, I’m primarily thinking about relatively modern Western dance music from the 1970s to the present day — disco to dubstep, and beyond. Some ideas detailed here may well apply to other genres and time periods. ","date":"2021-04-24","objectID":"/posts/tracking-down-tracks/:0:0","tags":["DJ","music"],"title":"Tracking down tracks ","uri":"/posts/tracking-down-tracks/"},{"categories":null,"content":"Take a recording What if you’re listening to something right now and want to track it down? Record it. There’s a lot to be said about the ubiquity of phones. Here though, having a portable recording device in your pocket is really useful. Someone you want to ask or some tool you want to use will have a much easier time with a real recording, even if imperfect, over merely a description or your best guess at humming or singing the music. There are several music recognition applications that are very effective and well-known these days. These typically work by submitting an audio fingerprint to the application owner’s servers and searching for that in their databases. I don’t personally use these applications, though I have tried them occasionally in the past. It is possible to use these applications live while the music is playing. But you still might favour taking a recording first so that you can easily share that elsewhere. Audio recognition applications may not cover every piece of music ever recorded, but they are potentially a useful first look. As these applications often run on mobile devices, there is a privacy-related caveat1, however. With an audio clip of interest — whether by recording it yourself, or snipping it from an existing piece of digital audio — an alternative is to upload the audio to a site that detects content, e.g. SoundCloud, Mixcloud, YouTube etc. Primarily the reason is for these services to detect copyrighted content and either block your submission, or at least monetise it for the copyright owner. But you can repurpose this to identify the audio for you. This is also useful if you want to identify lots of tracks from one source, e.g. a DJ mix, in one go. Often, I’ve listened to things on YouTube and spotted the video description details several of the tracks in there. And what to do if recording isn’t possible, the recording fails or is not clear enough to be useful? If there are lyrics or a vocal sample, then remembering the most frequent lyric and/or distinctive lyrics is helpful. A frequently repeated lyric, or a modification of it, may well be the music’s title. Lyric fragments are easily searched for. ","date":"2021-04-24","objectID":"/posts/tracking-down-tracks/:1:0","tags":["DJ","music"],"title":"Tracking down tracks ","uri":"/posts/tracking-down-tracks/"},{"categories":null,"content":"Narrowing down Searching for the text used in lyrics or repeated vocal samples can be a good start. Because vocals are often sampled from elsewhere, you may end up first locating the original sample source. If you do, that’s a lead: you can next try and search for which tracks sampled that vocal. Especially in early 90s tracks, certain acapellas that were used many, many times.2 You can also search for the name of the genre, along with the lyric snippet. This might also help get you to some place on the web. That might be where the release is indexed in some kind of database, e.g. Discogs. It might occasionally be some place where someone else is asking the same question as you, possibly with a helpful reply. Try restricting a search to some of the bigger music-related databases online. You can try using the site: operator on search engines that support it to narrow your search. Looking for a combination of the artist and name of original track, optionally with a genre on Discogs, site:discogs.com, might help. If it’s a mix with a dated year, you can probably narrow down your search to a year or two; the release may well be from that same year or previous years. It does roughly narrow it down, but not precisely. If the recording is from towards the end of a year, the release might have been the next year (or even later) if it has been shared with DJs ahead of release. If there are commercial issues, e.g. trouble with sample clearance or labels, it might never have been officially released, or may have been released considerably later. If it’s a genre you know well and particularly for Western electronic dance music, especially historically, you can probably have some kind of guess as to when a piece of music was produced to the nearest couple of years. Even for today’s dance music, where there might not be the same huge difference in recording techniques as there was from the 1980s to 2000s, there may certainly be trends, common production styles or popular sounds or samples. That said, there’s also a fashion for making things sound like they used to — producers can use the same techniques and equipment that were popular during a certain period to replicate it — so don’t always assume your guess is correct. Another approach that might work if others don’t and the track was in a DJ set is finding more sets by the same DJ from around the same time period. Narrow down by looking for sets with tracklistings and then skim through to see if the same track appears. This requires some luck, but might just work. ","date":"2021-04-24","objectID":"/posts/tracking-down-tracks/:2:0","tags":["DJ","music"],"title":"Tracking down tracks ","uri":"/posts/tracking-down-tracks/"},{"categories":null,"content":"Getting help from others If you hear the music at an event you’re actually attending and a DJ is playing, or played the track, well, you can try asking them either at the event, or after the fact online. This is more likely to be successful at small events. You can always try this if it’s a bigger show or the DJ in question is a bigger name, though that may be less successful. If it’s a recorded DJ mix, there may well be a tracklisting somewhere. That goes especially if it’s a commercial mix, or if it was a radio broadcast (where stations often list show details on their websites). If the mix is on SoundCloud or YouTube, you can look at the description or the comments. Sometimes, particularly on SoundCloud, you might get the DJ themselves, the artist or someone else who follows that DJ or genre naming some or all of the tracks. If all that fails, you could always try asking somewhere online. If it’s a mix that’s already posted online, you could ask in the comments of that mix. If it’s audio that you’ve recorded elsewhere, you can upload it somewhere, or describe it as best you can. Find a suitable place to then post that question. That could be somewhere more general, like the Discogs forums, or more specific where certain music genres are the focus. The caveat is that there are possible privacy issues with using these types of applications. These applications may request permissions to record audio, as well as locating you via GPS or to a lesser extent, IP address. Incidentally, Shazam were bought by Apple, and Apple seem to place some priority on user privacy. Having not used Shazam much, if at all, I’m not necessarily advocating it though. ↩︎ A good question, that I can’t answer right now, is why certain vocals were reused so much. Was it because of the availability of the original acapella? Was it because the vocal has a particular sound? Was it a deliberate reference to the existing popular reuse? ↩︎ ","date":"2021-04-24","objectID":"/posts/tracking-down-tracks/:3:0","tags":["DJ","music"],"title":"Tracking down tracks ","uri":"/posts/tracking-down-tracks/"},{"categories":null,"content":"Despite working with computers (supposedly) professionally, I don’t often build PCs or upgrade PC hardware. A few years ago, I learned a few lessons from building a PC. Over the 2020 Christmas holidays, I was upgrading someone’s PC for them, so here are my lessons I’ve thought about in 2021. ","date":"2021-02-26","objectID":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/:0:0","tags":["PC","build"],"title":"Things I have learned from rebuilding a PC in 2021","uri":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/"},{"categories":null,"content":"Buying PC parts is not easy In the UK at least, the latest processors (CPUs) and graphics cards (GPUs) were out of stock at the time of buying. That is probably due to some or all of: COVID-19 related supply chain issues; releases of hardware in the run up to the holiday period; the current trend for people to buy coveted, in-demand trinkets (fashion, new generation video game consoles, hand sanitiser) and resell at higher than retail price; demand from people stuck at home and spending money on PC hardware instead of other things. Adding to all that, the 2021 changes in UK trade due to Brexit may not help either. However, the fundamental issue of shortages applies globally from what I’ve read. Pent-up demand then drove up prices of older kit. By chance, I managed to spot AMD’s new Ryzen 5 5600x CPUs available just before Christmas from a retailer at slightly higher than the billed retail price. Somehow I also managed to order while holding my nose at the inflated cost. I wasn’t actually intending to buy the latest CPU, but the previous generation Ryzen 5 3600x was being priced at around 75-80% of the cost of the newer kit anyway. This might all resolve itself in time. I could do with a new desktop build myself, but not really inclined to do so right now. Reading around, it seems like stock shortages, particularly for GPUs, will continuing well into 2021. ","date":"2021-02-26","objectID":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/:1:0","tags":["PC","build"],"title":"Things I have learned from rebuilding a PC in 2021","uri":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/"},{"categories":null,"content":"Buying from a retailer with good returns policies can save time Here’s a long and dull story; feel free to skip to the lessons below. As part of the same build, and after a lot of research, I ordered a mainboard that was on offer, as researching it showed it to be a good buy. As lots of mainboards are, it seemed to be one with lots of RGB LEDs — nice if you want that, but unnecessary to me — and I figured you could disable the LEDs easily. Shortly after ordering, I discovered that you couldn’t simply toggle the LEDs off in the BIOS. Instead, you had to have the mainboard’s manufacturer’s clunky software — nice if you want that, but unnecessary to me — running constantly on Windows to control the LEDs. I tried to cancel the board order. The order hadn’t yet moved to the stage of being prepared for shipping and was temporarily out of stock anyway. So I sent a message via the website and hoped they would cancel on the next working day. This was almost out of office hours, and I figured that everything would be OK. About an hour later, I got another email saying the order was dispatched. From there, things became more of a mess. The next morning, the courier shipping the order sent me a message with delivery options including delaying the delivery. I delayed it, the courier still arrived with the parcel and I explained that I’d deferred delivery. After a phone call to the company I ordered from, confirming I could refuse delivery and they would create an returns number, I waited a week, and heard nothing at all. There was no repeat delivery and I assumed the parcel was in limbo somewhere. In the end, it took over a week for the company to get the parcel back from the courier, it then took a further two written requests to customer support to finally receive my money back. I told you it was a long and dull story. No refunds. ","date":"2021-02-26","objectID":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/:2:0","tags":["PC","build"],"title":"Things I have learned from rebuilding a PC in 2021","uri":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/"},{"categories":null,"content":"A long and dull story with two lessons learned Ordering from a company that has an automated online system for cancelling orders is useful for sudden onset of buyer’s remorse. Ordering from a company that has a good and simple returns process is useful if you find the hardware isn’t right after you’ve received it. There is one obvious market-leading online retailer that has these attributes, but may not be everyone’s first choice for various reasons. Nonetheless, on customer service, these attributes are fantastic from a buyer’s perspective. I don’t frequently make regrettable purchases. But you can do all the reading of specifications, manuals, reviews and opinions that you like, and still find two PC components just will not get along for whatever reason. Having no-fuss returns can save you a lot of time and frustration chasing up customer support. In the UK, there is substantial current legislation regarding consumer rights. However, it’s far simpler as a customer if the vendor already provides better than the minimum customer service. ","date":"2021-02-26","objectID":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/:2:1","tags":["PC","build"],"title":"Things I have learned from rebuilding a PC in 2021","uri":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/"},{"categories":null,"content":"Selling PC parts is easy… After the upgrade, we advertised the old removed parts online. They all sold within a week, and the proceeds could be spent on a new graphics card, if you could buy one (see above). I was surprised at both how quickly these old components sold and for what we sold them for. Some parts sold for not much less than they cost at retail. It’s possible that people are looking for budget builds, or upgrading/repairing existing builds. ","date":"2021-02-26","objectID":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/:3:0","tags":["PC","build"],"title":"Things I have learned from rebuilding a PC in 2021","uri":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/"},{"categories":null,"content":"…but make sure you upgrade any firmware before selling After the mainboard sold, the buyer asked about what CPU was used with it; it wasn’t working for them and they were trying to diagnose it. When they booted up the PC, nothing happened. Between the details the buyer gave and reading around the BIOS release notes for the mainboard, we concluded that it was probably a combination of the mainboard BIOS never being updated, and the buyer using a newer CPU than had previously been installed. It was lucky that there was a route to flashing the BIOS without a CPU via one of the USB ports. Decent mainboards often feature this recovery option. That meant the buyer could solve this problem without borrowing or buying another CPU to upgrade the BIOS, or, worse for us, returning the item. The wise thing to do is to upgrade the firmware, if there is any, before removing components. ","date":"2021-02-26","objectID":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/:3:1","tags":["PC","build"],"title":"Things I have learned from rebuilding a PC in 2021","uri":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/"},{"categories":null,"content":"What tedious CPU cooler installations should tell hardware manufacturers about marketing Two very disparate ideas in the heading there; let’s see if there’s anything like a cohesive argument here. Last time I wrote about building a PC, I had lots of fun installing Intel’s push pins. This time I had fun with fitting a cooler to an AMD setup. It didn’t use screws. Instead the cooler used long metal tweezer-like clips to fit over the tabs on the AM4 socket on the mainboard. How this all was assembled to secure the cooler was not obvious at all. The lack of clear instructions did not help one bit. The text basically said “build the thing and look at the diagram”. The diagram wasn’t clear. Nor were there many decent YouTube tutorials on this. Eventually I managed to piece together what to do from looking collectively at several installation videos in several different languages. Even then, it was a delicate task to keep the assembly together, while placing the cooler contact directly on the CPU package and not smudging thermal paste everywhere. As an esteemed non-influencer, hardware manufacturers are doing whatever the opposite is of queuing up to ask me what I think. But if there ever did suddenly appear an itinerant queue of manufacturers on my doorstep, what I’d advise them is: Instead of relying on buyers of the hardware to provide tutorials, manufacturers should create or commission their own videos. Make sure these videos have appropriate metadata (title and description) to make them easy to find by those who need them. Provide videos in multiple languages, whether by audio, subtitles or both. Show less obvious steps from multiple angles. It’s probably justifiable to spend a relatively small portion of a marketing and/or support budget to ensure that there is a decent installation video available. This is marketing and brand awareness: you’re demonstrating, hopefully, that your kit is easy to use, and, importantly, providing support for users. I think it’s a wider lesson for anyone selling anything — whether hardware or software — that might require some tricky user installation. Whether a prospective buyer seeing how to install something and deciding to buy based on that, or an actual buyer not complaining online or requesting a refund, there’s a benefit for the vendor too. ","date":"2021-02-26","objectID":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/:4:0","tags":["PC","build"],"title":"Things I have learned from rebuilding a PC in 2021","uri":"/posts/things-i-have-learned-from-rebuilding-a-pc-in-2021/"},{"categories":null,"content":"A simple task made complicated This isn’t a particularly interesting post. That is, unless you reading this now, like me at the time of writing this, have spent the best part of an hour trying to solve the same issue. Like a few other posts I’ve made in the past, the purpose is really to make this easier to search for, if others encounter the same problem. Someone had recorded a video on an iPhone that they wanted to transfer to a Windows PC. Naturally, they connected the phone via a USB cable to the computer, navigated through the existing DCIM directories to copy the files across and found, well, nothing. So they asked me about it. What was going on? They used the same process before and it worked fine. And the videos were definitely still on the phone: you could play them there. As these videos had large file sizes, I suspected there was some limit being encountered. ","date":"2021-01-25","objectID":"/posts/copying-large-iphone-videos-to-a-windows-pc/:1:0","tags":["iOS","iPad","iPadOS","iPhone","Windows"],"title":"Copying large iPhone videos to a Windows PC","uri":"/posts/copying-large-iphone-videos-to-a-windows-pc/"},{"categories":null,"content":"No easy answers Many of the workarounds posted online seem to fit one of the following categories: Moving the video via iCloud, or some other uploading route. (Too slow and too tedious.) Using some other app to get the video to a PC. (May involve spending money and requires auditing the app to ensure it and the developers are reputable.) Re-encoding the video on the phone to a smaller size somehow, possibly via iMovie? (Didn’t try this and may have been slow. This would have been the next thing I suggested if the solution below had not worked.) The Apple documentation itself suggests making sure iTunes is installed, using the Windows Photos app and then importing from there. This didn’t work. ","date":"2021-01-25","objectID":"/posts/copying-large-iphone-videos-to-a-windows-pc/:2:0","tags":["iOS","iPad","iPadOS","iPhone","Windows"],"title":"Copying large iPhone videos to a Windows PC","uri":"/posts/copying-large-iphone-videos-to-a-windows-pc/"},{"categories":null,"content":"The fix I found someone posted a fix on a forum. On the iPhone’s Settings, select Photos, and then under “Transfer to Mac or PC” setting for the Photos app, change “Automatic” to “Keep Originals”. That was it. Much simpler than any of the other suggestions. The import actually worked through the Photos app then. In fact, you did not need Photos at all: checking the DCIM directory listed the file. What “Keep Originals” does is take a direct copy of the photos and videos, instead of doing some unspecified magic and converting them on copy. (Though not tested, iPadOS has the same setting and presumably behaves in the same way.) As someone unfamiliar with iOS, I’m not entirely sure how you would connect the setting to the behaviour. But the main thing is that it resolved a very frustrating and very badly documented issue very, very quickly. ","date":"2021-01-25","objectID":"/posts/copying-large-iphone-videos-to-a-windows-pc/:2:1","tags":["iOS","iPad","iPadOS","iPhone","Windows"],"title":"Copying large iPhone videos to a Windows PC","uri":"/posts/copying-large-iphone-videos-to-a-windows-pc/"},{"categories":null,"content":" comparing with the Forerunner 15. ","date":"2020-11-13","objectID":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/:0:0","tags":["Forerunner","Garmin","review","running"],"title":"Garmin Forerunner 35: a competent budget running watch","uri":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/"},{"categories":null,"content":"Replacing a Garmin Forerunner 15 watch Unsurprisingly, I’ve been doing a lot more exercise outdoors this year than last, including a lot more running. So I’ve had a lot more time to try out the Garmin Forerunner 35 GPS watch I’ve had since the end of 2019. I have owned a Garmin Forerunner 15 watch for a few years and, especially considering what it cost, it served me well. The main issue with it was that the battery life seemed to have deteriorated over its lifetime. Changing the battery isn’t too complex a procedure — there are YouTube videos that show you how — but since the battery life was never that great from new, it seemed worthwhile looking into getting a different watch. And late last year, I saw an offer on a Garmin Forerunner 35. The Forerunner 35, like the Forerunner 15, is again an entry-level GPS watch, but it’s a considerable upgrade from the Forerunner 15. ","date":"2020-11-13","objectID":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/:1:0","tags":["Forerunner","Garmin","review","running"],"title":"Garmin Forerunner 35: a competent budget running watch","uri":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/"},{"categories":null,"content":"Comparing the Forerunner 35 to the Forerunner 15 Though the Forerunner 35 is comparably priced to what the Forerunner 15 cost when I bought it, there are a number of improvements and extra features of the Forerunner 35 over the Forerunner 15. The Forerunner 35 is a less chunky, and more subdued design than the Forerunner 15 — looking around, there’s a Forerunner 25 that looks like an odd hybrid of the 15 and 35 designs — and has walking and outdoor cycling modes. The screen on the 35 is also higher resolution. There’s a built-in wrist heart rate monitor. I’m not entirely sure the heart rate measurement is always that accurate, but it removes the need for an external sensor, if a rough idea of heart rate is what you want. The backlight is also an improvement over the Forerunner 15. There’s also the option to pair a phone via Bluetooth, though that’s not something I’ve bothered to use. On using the 35, the battery life does seem much improved over the 15. I never wore the Forerunner 15 daily, only for runs. By contrast, I’m wearing the Forerunner 35 I’ve used daily: it does need a charge every week or so, but that doesn’t usually take long. Even if low on power, you can give it a quick boost before you head out and it will probably tide you over for your run. With the Forerunner 15, there were numerous times where I found it would run out of battery on longer runs. ","date":"2020-11-13","objectID":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/:2:0","tags":["Forerunner","Garmin","review","running"],"title":"Garmin Forerunner 35: a competent budget running watch","uri":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/"},{"categories":null,"content":"Using the Forerunner 35 on Linux Much like the old Forerunner 15, this watch still works fairly well even if you’re not using Garmin’s own software. That might be because you’re not keen on sharing your data or because you’re on Linux and can’t easily run the software; the Windows version may run with WINE, however. Even if you don’t use Garmin Connect, the watch gives you access to everything you need as it appears as a USB storage device. You can retrieve and apply GPS updates by other means; without a regular update, GPS locking can be slow and take several minutes, as it did with the Forerunner 15. It’s also straightforward to copy the Garmin .FIT files from the watch and convert them to .gpx with GPSBabel or similar. ","date":"2020-11-13","objectID":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/:3:0","tags":["Forerunner","Garmin","review","running"],"title":"Garmin Forerunner 35: a competent budget running watch","uri":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/"},{"categories":null,"content":"Maintenance The two things that usually fail on digital watches are the strap and the battery. The Forerunner 35 has a reasonably durable strap. And the strap’s replaceable: you can at least buy third-party replacements. The battery is another matter: Garmin don’t want you to replace the battery in any of their watches at all — and I can’t find any posts or videos on doing so for the 35. However, if the Forerunner 35 is built like the Forerunner 15, a battery replacement could be possible if you’re willing to take the watch apart. The battery replacement issue is one reason why I’m not inclined to spend much more on specialist watches. You can easily spend two to four times as much as the Forerunner 35 costs on more expensive Garmin watches. I’m not sure how much value you get for spending more. For me, the only significant features missing from this watch are directions to follow a GPX route, and a swim mode. (That said, the Forerunner 35, like the Forerunner 15, is waterproof). Neither of those omissions are enough for me to spend considerably more. My primary use is tracking pacing and distance while running, and the Forerunner 35 does well here. ","date":"2020-11-13","objectID":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/:4:0","tags":["Forerunner","Garmin","review","running"],"title":"Garmin Forerunner 35: a competent budget running watch","uri":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/"},{"categories":null,"content":"Overall? Whether you’re looking at replacing an older, budget running watch, or you’ve not ever had such a watch and want to try one out without spending too much, the Forerunner 35 is a quietly competent choice. It’s been a substantial upgrade from the Forerunner 15, which I also liked a lot. ","date":"2020-11-13","objectID":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/:5:0","tags":["Forerunner","Garmin","review","running"],"title":"Garmin Forerunner 35: a competent budget running watch","uri":"/posts/garmin-forerunner-35-a-competent-budget-running-watch/"},{"categories":null,"content":" to a remote repository provider ","date":"2020-08-02","objectID":"/posts/continuous-integration-with-github-actions/:0:0","tags":["automation","continuous delivery","continuous integration","GitHub","GitHub Actions"],"title":"Continuous integration with GitHub Actions","uri":"/posts/continuous-integration-with-github-actions/"},{"categories":null,"content":"A quick review of GitHub Actions I’ve been using GitHub Actions a lot recently. It’s a continuous integration and continuous delivery/deployment (CI/CD) tool that’s, unsurprisingly, part of GitHub. As the vagueness of the “D” in “CD” might indicate, different people have different interpretations of how you actually define these terms. For this discussion, let’s just take it that these tools encourage developers to automate many software development processes that don’t involve writing code. There’s an aim of frequently getting changes into the main development branch, with minimal manual labour and aiming for a good level of quality control, trying to avoid broken code getting deployed. So, I’ve been using GitHub Actions a lot, both for personal projects and at work. And I’ve enjoyed using it. It’s reasonably easy to use, and there’s a quick feedback loop from you pushing a workflow to seeing it run. This in turn makes it fun to work with; it puts you in a frame of mind where you ponder what development tasks you could automate. Where GitHub Actions lacks is often for want of a little polish and refinement are needed. I’ll mention three examples. First, the documentation occasionally lacks simple concrete examples for some of the features. The early parts of the documentation are actually example-focused. But then many of the features are simply all piled into a reference section at the end. Additionally, I don’t know what exactly it is about the page design or the structuring of the documentation, but I did find it difficult to find precisely what I was looking for. That was the case even when I’d actually read some information before and knew it was hidden there, somewhere. Often I’ve found more useful information elsewhere. Other helpful places I’ve found have included: GitHub’s Architecture Decision Records in the GitHub Actions Runner repository, Stack Overflow, other people’s GitHub Actions workflows, and documentation for third-party actions. Second, at time of writing, there is no simple way to remove repetition within workflows and between workflows. For instance, you might want to run tests before you merge pull requests, and might want to do the same when you build and publish a new release. At the moment, this means duplicating part of one workflow in another, or creating a custom action. GitHub are planning to add “composite” actions soon which should help simplify this type of common reuse. Finally, workflow steps do not have a non-failing “neutral” outcome. (Apparently, this did use to be a feature during the GitHub Actions beta.) You might design a workflow with a step that could determine whether subsequent steps should run or not. Without a neutral outcome, there is no easy way to indicate “no error, but we have stopped the workflow early”. The only way to directly exit the workflow at that point is to fail. But this isn’t semantically what is intended: this type of exit may be an expected valid state and not a workflow failure. Alternatively, you can continue without failing by specifying continue-on-error: true for a step. But now all subsequent steps will run, which we don’t want: maybe those steps will waste a sizable chunk of our Actions minutes quota and/or maybe those steps will fail too. To skip the rest of the steps when they’re not needed, conditional checks on “success” of steps can be threaded through the subsequent steps of your workflow. But this complicates the workflow, a “neutral” outcome would simplify this entirely by allowing us to exit, without failure, at the point where we really do want the workflow to stop. ","date":"2020-08-02","objectID":"/posts/continuous-integration-with-github-actions/:1:0","tags":["automation","continuous delivery","continuous integration","GitHub","GitHub Actions"],"title":"Continuous integration with GitHub Actions","uri":"/posts/continuous-integration-with-github-actions/"},{"categories":null,"content":"Competitive pricing and features GitHub Actions does well on bundling free usage minutes, even for private projects. For something like, say Travis CI, you have to pay a hefty fixed price per month to get that option. Though it should be noted that Travis CI pricing includes unlimited minutes, whereas you have to pay if you exceed your GitHub Actions monthly quota. However, at current pricing, you could get almost 10,000 monthly minutes on GitHub Actions using a Linux runner — $0.008 per minute, with 2000 included in a GitHub Free plan — for the cost of the cheapest Travis plan ($63). So, unless you’re using a lot of minutes, when comparing on price alone, there’s no contest. The other big advantage of GitHub Actions is that it permits far more concurrent jobs than Travis CI: at the time of writing, a GitHub Free account has twenty concurrent (non-macOS) jobs. Travis CI offers only ten concurrent jobs even at the highest “Premium” plan, and just one on their lowest priced plan. This is important as, particularly if you’re collaborating in a team, your request for a CI job may be blocked by other outstanding jobs, possibly delaying the completion of your requested job. Both Travis CI and GitHub Actions offer free usage in some cases. GitHub Actions pricing refers to “free for public repositories”. Travis CI’s own site mentions “free for open source”. I suspect by that this Travis CI really just mean “your code must be public”. And not that anyone actually checks that your code is published under some kind of valid open source license. ","date":"2020-08-02","objectID":"/posts/continuous-integration-with-github-actions/:1:1","tags":["automation","continuous delivery","continuous integration","GitHub","GitHub Actions"],"title":"Continuous integration with GitHub Actions","uri":"/posts/continuous-integration-with-github-actions/"},{"categories":null,"content":"Coupling automation to the remote repository host Price and concurrency aside, what might be consequences of using the same service — here GitHub — for hosting the repository’s code and that repository’s automation? From a quick search around, there doesn’t appear any standard format for specifying these types of automation workflows. That’s not too surprising. One reason might be that every provider implements their own features and therefore wants to be able to flexibly specify the configuration to incorporate those features. There aren’t any transpilers for different formats: I can’t take an existing .travis.yml and automatically convert it to a GitHub Actions workflow.yml. (However, there is an R project that aims to specify workflow configurations for R packages agnostically, outputting configuration for different providers.) The consequence is that using GitHub Actions means that you now have a coupling of your code’s organisation to GitHub, and adds work should you migrate to another Git repository host. To be fair, this is likely true already, even without using GitHub Actions. If you move to a different host, say GitLab, at the least, you’ll also probably want to move the existing issues from GitHub to the new host. In mitigation of this, there are often tools or automatic imports for migrating these other non-code features, like issues. But, as we’ve seen, any GitHub Actions workflows will probably need a manual rewrite to use a similar process elsewhere. This, then, is probably where using an external automation tool like Travis CI offers a benefit. If you migrate to a new remote repository host, to get your automation running, you just need to connect your new host to your existing automation system. No rewrite required. On the other hand, it’s reasonable to think that external tools are more awkward to configure and monitor as opposed to comparable tooling that is a key part of your existing repository host. With an external service, there will also be some requirement for developers to create an account there, and then allow the external application to access their GitHub account. With GitHub Actions, it’s just another tab on your GitHub repository. (Travis CI certainly didn’t help themselves here by previously having two separate domains, to independently handle free and paid services.) And an external tool is an additional point of failure. It’s possible GitHub is up and running, while Travis CI is unavailable, or vice versa. Either service failing can block active development. ","date":"2020-08-02","objectID":"/posts/continuous-integration-with-github-actions/:1:2","tags":["automation","continuous delivery","continuous integration","GitHub","GitHub Actions"],"title":"Continuous integration with GitHub Actions","uri":"/posts/continuous-integration-with-github-actions/"},{"categories":null,"content":"So what’s best? Like many choices, there’s no definitive answer. If the CI services have roughly equivalent features, then it comes down to what other aspects you prioritise. Is tight integration of your software development processes a positive — making for potentially easier configuration — or a hindrance to migration? Is pricing the most critical aspect? For funded development teams, the pricing issue is perhaps less important since it will likely still be a small part of an organisation’s budget. Where GitHub Actions is particularly well placed is in the giving of individual developers an allocation of free automation job time for their private projects. Together with that allocation being easily used as part of the service developers are already likely using, it is challenging for external automation services to compete. And that’s where my personal view is. I’m still sticking with GitHub Actions, notwithstanding the slight risk of getting too tied into GitHub. GitHub is incredibly popular right now, it’s where open source projects are developed, and that network effect is an important one. ","date":"2020-08-02","objectID":"/posts/continuous-integration-with-github-actions/:2:0","tags":["automation","continuous delivery","continuous integration","GitHub","GitHub Actions"],"title":"Continuous integration with GitHub Actions","uri":"/posts/continuous-integration-with-github-actions/"},{"categories":null,"content":"Lost… At work recently, I was trying to find a GitHub repository that I knew I had previously seen a while back. But I couldn’t find it. Normally, I would “star” possibly useful repositories: it wasn’t in that starred list. After spending a fair length of time trying to rediscover the repository by many searches on GitHub and on search engines, I concluded that maybe the author had simply deleted the repository or their account. And so I gave up. ","date":"2020-06-19","objectID":"/posts/oh-fork-it/:1:0","tags":["GitHub"],"title":"Oh, fork it","uri":"/posts/oh-fork-it/"},{"categories":null,"content":"…and found Sometime later, while reading a related GitHub issue, I spotted I had written a comment linking to the repository. It was still there, just not easily discoverable.1 ","date":"2020-06-19","objectID":"/posts/oh-fork-it/:2:0","tags":["GitHub"],"title":"Oh, fork it","uri":"/posts/oh-fork-it/"},{"categories":null,"content":"Lessons Things shared on the internet have no guarantee of longevity. You are subject to the whims either of service providers either disappearing entirely, or removing a user and all their content. Users themselves may also delete things they’ve previously shared. Especially in a post-GDPR world, where people are likely far more aware of their ability and rights to do just that. Even if I had starred the elusive repository, that would not have helped if the user deleted the repository or the user’s account disappeared. For popular repositories, there is no likely threat of them vanishing entirely overnight, because there are probably several existing forks. And users may well restore a copy of such a deleted repository from local clones. But if a repository is obscure, then that published version may be the only source readily available. So, if there is some GitHub — or other online Git remote — repository that looks interesting or useful, but is relatively obscure, then forking it is prudent, and a one click operation without requiring you to store a copy locally.2 It may not be that often when people or organisations decide to delete all their code, but it does happen. Even if you may not necessarily have the final version before deletion, something may be better than a distant memory. After all that effort, I actually decided to use a different approach anyway. ↩︎ Of course, cloning locally is another valid approach, but requires you to store content that may just be clutter on your local storage. I’d wager if GitHub was to end their service, then there would be more than a day’s notice. ↩︎ ","date":"2020-06-19","objectID":"/posts/oh-fork-it/:3:0","tags":["GitHub"],"title":"Oh, fork it","uri":"/posts/oh-fork-it/"},{"categories":null,"content":"Missing semester: singular? Though I have a science background, I don’t have a formal computer science education at all. In that sense then, a course entitled “The Missing Semester of Your CS Education” is an underestimation. In my case, they’re all missing semesters. (I’d love to learn more and I do keep trying to learn what I can at work, and pick up new things in my spare time.) But, looking over the course syllabus, I realised that, actually, I’d already covered a fair amount of what it covers. Much of this had been via learning at work, often from colleagues telling me about particular tools. That actually sold the course to me: I was already aware that some of the chosen topics were particularly useful. So, I figured, what I didn’t know might be worth knowing too. The central theme of the course is around the tooling that developers, especially on Linux, might use while developing software, or working at the command-line. The course covers: working with the command-line shell and shell scripting, text editors, version control with Git, using debugging and profiling tools, and a little introduction to cryptographic tools. ","date":"2020-06-07","objectID":"/posts/the-missing-semester-of-your-cs-education-a-course-review/:1:0","tags":["computer science","course","Linux"],"title":"\"The Missing Semester of Your CS Education\": a course review","uri":"/posts/the-missing-semester-of-your-cs-education-a-course-review/"},{"categories":null,"content":"Course format With eleven lectures of about an hour each, the course is well presented and not too lengthy. The lecture notes available on the course site are a useful reference and usually fairly comprehensive. The notes don’t always cover everything in the lectures, particularly where there are worked examples. But, a skim through the notes should give you a good impression of what’s discussed, if you prefer reading to videos. ","date":"2020-06-07","objectID":"/posts/the-missing-semester-of-your-cs-education-a-course-review/:2:0","tags":["computer science","course","Linux"],"title":"\"The Missing Semester of Your CS Education\": a course review","uri":"/posts/the-missing-semester-of-your-cs-education-a-course-review/"},{"categories":null,"content":"What I liked about the course Many of the lectures are self-contained. There are a few that are easier to follow with some basic command-line shell knowledge, but that too is covered in the first couple of lectures. Consequently, this means the course works well as a survey of topics. There is a little detail imparted to give you some deeper background, and sometimes other recommended readings. However, the level of presentation and the rate at which the material is worked through is very approachable without being overwhelming. The exercises for each lecture apply the ideas covered in a very direct and practical way. Despite the problems being artificial, it is possible to envisage that you might solve real problems with similar approaches. ","date":"2020-06-07","objectID":"/posts/the-missing-semester-of-your-cs-education-a-course-review/:3:0","tags":["computer science","course","Linux"],"title":"\"The Missing Semester of Your CS Education\": a course review","uri":"/posts/the-missing-semester-of-your-cs-education-a-course-review/"},{"categories":null,"content":"Do I recommend it? Yes, definitely. It will probably take about ten to twenty hours covering this material, depending on whether you watch all the videos or just read the notes. I should emphasise again that, even if you don’t want to go through the entire course, you can easily pick out the few lectures that might interest you. I did learn a few things that I didn’t know. And the exercises allowed me to try working with some tools I hadn’t used before (e.g. Linux kernel cgroups). If you’re an experienced Linux developer and constantly keep up-to-date with tools, there may not be much new here. However, for developers starting out, those who are starting to use Linux, those who are still in education or those, like me, who think they might have some knowledge gaps, this course is a nice compliment to other learning resources. ","date":"2020-06-07","objectID":"/posts/the-missing-semester-of-your-cs-education-a-course-review/:4:0","tags":["computer science","course","Linux"],"title":"\"The Missing Semester of Your CS Education\": a course review","uri":"/posts/the-missing-semester-of-your-cs-education-a-course-review/"},{"categories":null,"content":" Windows Reliable failure It’s sometimes a reassuring comfort to know that computers and computer-related things are entirely reliable. In that they are anything but. As I spent about an hour looking into this problem, and now think it has been fixed, perhaps it is helpful to document the symptoms such that anyone searching might find this page and save them some time. Especially if, like me, you don’t have a spare enclosure to test with and are thinking whether to buy a new one or not. (Yes, you probably should, is the answer, if you don’t want to read further.) Some background My enclosure was a cheap and cheerful once with USB and eSATA ports. eSATA is a bit dated these days, but is useful if you’re working with old PCs that don’t have USB 3 ports. ","date":"2020-05-09","objectID":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/:0:0","tags":["hard drive","hardware","Windows"],"title":"Strange symptoms of hard drive enclosure failure","uri":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/"},{"categories":null,"content":"Symptoms There were different failure modes depending on how the drive was connected: When connected by USB, the drive was not recognised by Windows. That is, it did not show up in File Explorer when connected, as if it wasn’t attached at all. In Disk Management, Windows was claiming the partition was a GPT Protective Partition, whereas it was a real GPT partition, and incorrectly had a size of 16,777,216 MB, i.e. the NTFS volume limit with default cluster size, which was also incorrect. When connected by eSATA, the drive was correctly recognised by Windows. Creating folders and, I think, copying moderate sized files apparently worked OK, though I didn’t try reading the files back from the drive. However, running the Windows 7 Backup and Restore tool reliably caused an error. Often, as soon as the backup process started to write to the drive in the enclosure, the drive acted as if it had been disconnected. That is, the backup failed, and the drive was no longer visible in File Explorer. ","date":"2020-05-09","objectID":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/:1:0","tags":["hard drive","hardware","Windows"],"title":"Strange symptoms of hard drive enclosure failure","uri":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/"},{"categories":null,"content":"Possible causes? As far as I could tell, there were three possibilities: Something was wrong with Windows. Not impossible, but this felt like more of a hardware fault, especially with the strange difference in behaviour between USB and eSATA, and with observed failures when different parts of Windows were running. It wasn’t just the Backup and Restore tool that was failing, but Disk Cleanup had also caused the drive to disconnect. The hard drive itself was failing. Not impossible either, but unlikely. Though a magnetic disk, it is enterprise-grade and hadn’t had much usage. In fact, the drive’s SMART readings indicated a few hundred hours of time spent powered on. Something was wrong with the enclosure. Probably most likely, given the enclosure was inexpensive. A cabling issue was ruled out on the basis that both USB and eSATA weren’t working, and had very different behaviour via the two connections. ","date":"2020-05-09","objectID":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/:2:0","tags":["hard drive","hardware","Windows"],"title":"Strange symptoms of hard drive enclosure failure","uri":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/"},{"categories":null,"content":"Closing off the matter In the end, not having a spare enclosure to test with, I bought a new enclosure to try out. Out of curiosity, I chose one with both USB and eSATA to test both out, in case the problem recurred. This did resolve part of the problem: the drive showed up both via eSATA and USB in the new enclosure. On the other hand, the Windows 7 Backup tool on Windows 10 was still failing. My hunch was that the faulty enclosure had somehow got the drive into an odd state. I never figured out what exactly the problem was here. A disk check did show errors, though I can’t remember whether those were able to be repaired, or whether the drive disconnected while trying to do so. Maybe there was some corruption of the file system? In the end, formatting the drive, encrypting the drive again and then backing up finally resolved the — by-now very tedious — problem. So, not all that exciting, but if you are witnessing similar symptoms, you can probably fix it with a new enclosure. ","date":"2020-05-09","objectID":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/:3:0","tags":["hard drive","hardware","Windows"],"title":"Strange symptoms of hard drive enclosure failure","uri":"/posts/strange-symptoms-of-hard-drive-enclosure-failure/"},{"categories":null,"content":"A slow boot problem Recently I was helping a friend with an older laptop running Windows 10 that seemed unusually slow to start. Though Windows seemed nimble once it started, the boot and shutdown times felt excessively slow: minutes instead of the seconds it should take. And it wasn’t like Windows itself was doing anything in this state either: the machine was silent, instead of the fans powering up, and there was just a black screen following the Windows logo. Fortunately, with some frantic searching, I stumbled on a potential solution: the cause might be a particular AMD graphics problem, especially with two graphics cards, integrated and discrete. And the laptop I was looking at did indeed have AMD Radeon hardware. ULPS ULPS is an initialism used by AMD to refer to Ultra Low Power State, and is the feature that can lead to this slow boot problem. As the name suggests, this is a power saving feature. With the particular Windows install I was looking at, I didn’t try updating drivers beyond those that were automatically found by Windows itself. However, reading around, the drivers may never have been fixed for legacy hardware anyway. The fix The fix I found was to disable this ULPS feature in the registry. First, open the Windows Registry Editor. (In the Windows 10 search box, search for “regedit” and then run “Registry Editor”; I can’t remember, but these edits may require you to be running the Registry Editor as administrator, so you may need to right-click on “Registry Editor” and then choose to run it as administrator.) Search for EnableULPS, set any 1 values to 0. There are also EnableULPS_NA settings but I read conflicting reports of whether to set these to 0 or not; in the end, only changing the EnableULPS settings was sufficient to resolve this issue for the laptop I was working with. Hopefully, that should cure the problem. It’s certainly a possibility that subsequently installed graphics drivers could reset these overrides and cause the symptom to recur, but, if that’s the case, the problem is easy to spot. After disabling ULPS, both booting and shutdown were much, much faster. The downside is that disabling ULPS may make the battery deplete more rapidly than if it were enabled. But the boot up and shutdown times were so slow without this fix that I think it’s a necessary trade-off to make your PC usable day-to-day, should you require it. There are other causes of a slow booting Windows installation, but if you are dealing with a slow starting PC with Radeon graphics, particularly with two display adapters, it’s worth trying to update the drivers and, failing that, seeing if this fix works. ","date":"2019-12-21","objectID":"/posts/a-ulps-take-windows-10-black-screens-and-slow-boot/:0:0","tags":["Windows"],"title":"A ULPS take: Windows 10 black screens and slow boot","uri":"/posts/a-ulps-take-windows-10-black-screens-and-slow-boot/"},{"categories":null,"content":"Lack of updates At the time of writing, this blog uses Pelican to convert the Markdown blog content into publishable HTML. And, if you noticed from the dates on my posts, there has been just over a year between the last post and this one. This was mainly because there was considerable work needed to get this blog setup up-to-date: both in upgrading Pelican to the latest version, and pulling in updates to the theme. Updating the theme was made slightly more complicated by the migration of the theme from its original repository. The new home became the pelican-themes repository, not as a submodule but as the definitive repository for that theme. In the past, it was simpler for me to simply pull request changes from the parent of my theme’s fork directly into my fork. Instead, I had to restructure my repository slightly and start to git cherry-pick individual commits that were applied to the theme within the repository. Anyway, I’ve caught up on the maintenance. Bootstrap 3 is now end-of-life so there is a longer term issue of this theme still being stuck on it. If I end up migrating to a new theme, it might be worth investigating using Hugo instead of Pelican at the same time. Hugo has increased in popularity considerably since I started using Pelican. ","date":"2019-12-16","objectID":"/posts/learning-from-other-projects-pelican-themes/:1:0","tags":["git","project management"],"title":"Learning from other projects: pelican-themes ","uri":"/posts/learning-from-other-projects-pelican-themes/"},{"categories":null,"content":"Learning from other projects While spending a fair amount of time updating my theme and working with the pelican-themes repository, I had chance to think about how the pelican-themes repository is structured and lessons I’ve learned just from having to work with it. Reading and understanding — or at least attempting to understand — other people’s source code is a way of picking up new ideas or concepts. What might also be useful for learning is looking at projects you use as a whole. How are they organised? How does that makes them easy or difficult to work with? There is a caveat that even a public source code repository doesn’t necessarily yield all the decision making that went on to get it to such a state. Some of that process may indeed be located with the source repository, e.g. pull request comment threads or issues. Some of that process may be separate to the source repository, but still public, e.g. public mailing lists. Some of that process may be entirely private, e.g. emails or private working documents. Some of it may be undocumented at all and reside entirely within maintainers' heads. ","date":"2019-12-16","objectID":"/posts/learning-from-other-projects-pelican-themes/:2:0","tags":["git","project management"],"title":"Learning from other projects: pelican-themes ","uri":"/posts/learning-from-other-projects-pelican-themes/"},{"categories":null,"content":"pelican-themes ","date":"2019-12-16","objectID":"/posts/learning-from-other-projects-pelican-themes/:3:0","tags":["git","project management"],"title":"Learning from other projects: pelican-themes ","uri":"/posts/learning-from-other-projects-pelican-themes/"},{"categories":null,"content":"A note The tone of this post shouldn’t be considered as a ranting “why, oh why, is this project not doing things the way I suggest?” post, but primarily an exercise to just consider and note other ways it could have been structured. It’s an actively maintained project, and the maintainers are volunteering considerable amounts of their own time doing just that. In many projects, whether by a single developer or a team, whether commercial and proprietary or open source, it’s also entirely believable that there’s no long-term maintenance plan from the outset. At the start, it’s not even known whether the project will have a long-term future. The first commit of pelican-themes was in February 2011, making it nine years old in 2020. Often, things do just get worked out along the way. ","date":"2019-12-16","objectID":"/posts/learning-from-other-projects-pelican-themes/:3:1","tags":["git","project management"],"title":"Learning from other projects: pelican-themes ","uri":"/posts/learning-from-other-projects-pelican-themes/"},{"categories":null,"content":"The current state pelican-themes is a mixture of submodules and themes whose files exist entirely in the repository. The included non-submodule themes sometimes (if not often or even always) use the pelican-themes repository as their definitive home. There are a few problems I’ve seen with this approach, exemplified by the theme I am currently using my own fork of, pelican-bootstrap3. Back when pelican-bootstrap3 was still maintained by its original developer, there were actually changes made to the version in pelican-themes that diverged from pelican-bootstrap3. pelican-bootstrap3 within pelican-themes was then updated by simply a straight copy over of the then-current version of the upstream theme, losing the changes made to the pelican-themes version. Apart from losing work, this process can be potentially confusing for users not directly following the development process; you can have something that was fixed, then unfixed. Later, the official home of pelican-bootstrap3 became the pelican-themes repository as the original developer no longer wanted to support it. This had a couple of consequences. First, it meant that the pelican-themes maintainers adopted the extra maintenance work of a popular theme. Those maintainers may not even use the theme. This means that pull requests can languish, perhaps because the maintainers don’t feel that comfortable merging substantial changes or don’t have strong opinions on whether the proposed changes are worthwhile. For pelican-bootstrap3, at the time of writing in December 2019, there are several unmerged pull requests on pelican-themes right now, even as far back as 2016. Pull requests left unloved and unmerged are a deterrent to other developers who may be considering contributing other changes. Second, even if pull requests are regularly merged, there may be less of a definitive direction taken than if maintainers have strong opinions about where that subproject is headed. It can also lead to a lack of quality assurance. For pelican-bootstrap3, an example is this translation feature. Such a feature is one that’s useful to lots of people, but unfortunately it broke things for users who didn’t have the same plugin configuration as the original author, affecting those users who did not intend to use the new feature. ","date":"2019-12-16","objectID":"/posts/learning-from-other-projects-pelican-themes/:3:2","tags":["git","project management"],"title":"Learning from other projects: pelican-themes ","uri":"/posts/learning-from-other-projects-pelican-themes/"},{"categories":null,"content":"How else might pelican-themes be managed? That’s how things are structured in pelican-themes now. I did think about other ways the repository might have been structured taking the above features of how it works right now into account. pelican-themes could simply have been a list of links to themes, solely a reference resource. The advantage of this is no management on the part of the pelican-themes maintainers is required except to add new themes or remove themes that are no longer supported or unavailable. In many cases, once a theme is added, that would be as much as is ever required to be done for that theme. The big disadvantage is that you can’t just clone the entire repository and have all the themes ready to use. There could be a “more strongly” monorepo version of what there is now, that is, pelican-themes could be a pure monorepo with no submodules. This now means that every theme has to be hand-maintained by copying from the upstream version repository, if pelican-themes isn’t the original repository for a theme. This doesn’t, however, solve the problem of having a mixture of maintained and unmaintained themes. Going completely in the opposite direction is an option too: have every theme in pelican-themes included as a submodule. This has the advantage that pull requests into pelican-themes would be simple, simply reflecting a submodule update to the parent repository’s current release. It would mean that every theme in pelican-themes where pelican-themes is the definitive repository for that theme would need to be moved to its own individual repository. However, breaking out the themes in this way might make it easier for those themes to be managed by specific maintainers for each theme only (provided volunteers could be found), as opposed to the current agglomerate repository maintained by developers who may not use most of the themes present. Of course, the problems described above are easy to spot when using a project that’s been around a while, but perhaps subtle at a project’s inception. I think this is symptomatic of software development. It is possible to build something that takes on a direction beyond that originally envisaged, especially projects that have many contributors or maintainers over time, each of which may have their own opinions as to how such a project should be nurtured. ","date":"2019-12-16","objectID":"/posts/learning-from-other-projects-pelican-themes/:3:3","tags":["git","project management"],"title":"Learning from other projects: pelican-themes ","uri":"/posts/learning-from-other-projects-pelican-themes/"},{"categories":null,"content":" making Windows 10 crash with a blue screen error caused by tdklib64.sys. (NB: There’s some background here just by way of introduction. If you encounter this problem and don’t want to endure my waffling, check the setting I mention below.) My Fujitsu laptop currently dual boots Windows and Ubuntu, but it’s rare that I boot into Windows. Nonetheless, I had a little time spare last week, I figured I might as well catch up on things everywhere and update the Windows install. Dropping into that did update Windows, and all well and good. Or so I thought. ","date":"2018-12-10","objectID":"/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10/:0:0","tags":["fix","Windows"],"title":"Stopping Fujitsu's Battery Charging Control Update Tool from crashing Windows 10","uri":"/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10/"},{"categories":null,"content":"The problem On restarting, I noticed there was a Fujitsu prompt that appeared, and was a little unexpected. What I was being asked to install was the very precisely named “Battery Charging Control Update Tool”. It seems Fujitsu have had some issues with battery quality, leading to a potential fire risk; this tool is supposed to mitigate that. It seems that, at least for my model of laptop, the tool was attempting to update the BIOS. And you can see how the tool should work on Fujitsu’s Hong Kong site (and that page I could only find on the Hong Kong site for some reason). In the previous paragraph, I say attempting to update the BIOS, because what happened, after the initial preparation stage occurred with the “Continue to update BIOS?” prompt, was that shortly after I clicked “Yes”, I saw a lovely Windows 10 blue screen which I think mentioned tdklib64.sys as the cause. There’s nothing I could find relating to this failed BIOS update and these blue screens — although there were mentions of BIOS update failures and tdklib64.sys relating to other manufacturers machines. Finding nothing struck me as strange: it’s a fair bet that you’ll find at least one person’s already complained loudly somewhere about a problem you’ve encountered too. Anyway, I rebooted and tried the update again. Same result. Blue screen. OK then. Third time lucky, maybe? No. Just the same (reliably) unreliable result. ","date":"2018-12-10","objectID":"/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10/:1:0","tags":["fix","Windows"],"title":"Stopping Fujitsu's Battery Charging Control Update Tool from crashing Windows 10","uri":"/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10/"},{"categories":null,"content":"The fix In a moment of fortunate (and rare) inspiration, I remembered that I’d turned on a relatively new security feature — Memory Integrity — in Windows Defender Security Center as I’d spotted it as a setting I’d not enabled already. Perhaps that was the culprit, especially as no-one else seemed to have encounter this failure yet? Yes. Yes, it was. Disabling that again meant the update proceeded without a problem, and then I just re-enabled the Memory Integrity setting once again after the BIOS update completed. If you’re having a similar problem, check this setting before attempting to update. Maybe this tip helps you avoid the hour I spent figuring this out. ","date":"2018-12-10","objectID":"/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10/:2:0","tags":["fix","Windows"],"title":"Stopping Fujitsu's Battery Charging Control Update Tool from crashing Windows 10","uri":"/posts/stopping-fujitsus-battery-charging-control-update-tool-from-crashing-windows-10/"},{"categories":null,"content":"Can you feel the --force? Force pushing to a remote repository that others may be using should always be done with care. Even if it’s a completely private repository that only you use, you maybe should double check your thinking before going ahead. The double edged sword of a force push is that you’re changing the state of the remote repository history irrevocably. This can be good, for instance, if you’re working on a non-master repository branch that you “own” and have perhaps cleaned it up: rebasing it, removing or squashing unneeded separate commits. It can be bad if you force push to master on a repository, and cause, at minimum, considerable inconvenience for other developers. Those developesr may now face working out what exactly has happened to the repository, when they try and integrate their future changes, or may be baffled that a previously existing commit has now mysteriously disappeared. If you’re working on a non-master development branch, you may be a little bit more lax in how you force push. Certainly, the way I’ve used branches with other people is that generally a branch is owned by one particular person, and those are free to be amended by that branch owner (usually the creator of that branch, although ownership may be passed from person to person). Then, as the agreed owner of such a branch, provided I know that I’m happy with the local changes, I can just force push to that development branch. However, that’s not always the case. Perhaps two people are working on the same branch, maybe working on slightly different things, e.g. one could be working on frontend changes for the site, while another works on backend changes, but these changes are part of the same feature, and therefore need to be part of the same branch. ","date":"2018-11-10","objectID":"/posts/git-nice-and-lease-y/:1:0","tags":["git "],"title":"git: nice and lease-y","uri":"/posts/git-nice-and-lease-y/"},{"categories":null,"content":"What --force-with-lease offers over vanilla --force What I discovered recently, although it’s been around for ages, is that git has another option for forcing push: --force-with-lease. What this option does is checks that the remote branch is still in the same state it was when you last pulled it, and refuses to force push if not, i.e. there have been no other changes to the branch in the intervening time. Of course, you can always still override this check by just using plain old --force. But --force-with-lease at least gives you another safety check before force pushing, just in case someone else has altered the remote branch (giving you chance, for example, to pull that branch, and rebase your changes on it), and avoiding any confusion between developers, and potentially loss of work. Note though, as this answer highlights, if you have an editor or other scheduled task running git fetch in the background, --force-with-lease won’t offer any protection as the remote tracking branches that would be stored locally are being periodically updated. ","date":"2018-11-10","objectID":"/posts/git-nice-and-lease-y/:2:0","tags":["git "],"title":"git: nice and lease-y","uri":"/posts/git-nice-and-lease-y/"},{"categories":null,"content":"!!! article-edit \"\" Edit 2020-06-19: These days, I just tend to use a manually created virtualenv when working on Python, and a bash one-liner to activate the appropriate virtualenv. As mentioned below, [poetry](https://github.com/python-poetry/poetry) is another option for managing dependencies and I've seen a lot of positive things written about poetry since I originally wrote this post. Pipenv1 is a tool that aims to remove the hassle of using virtualenvs (Python runtime environments to keep separate Python setups for different projects independent), and also help manage requirements. It also aims to help provide deterministic builds of software. I had read about Pipenv previously, but couldn’t ever really understand how it worked from just reading about it. So I switched from my existing virtualenv setup to this to try it out and figure out whether I can replicate the same behaviour I had with virtualenv and virtualenvwrapper. ","date":"2018-08-26","objectID":"/posts/a-look-at-pipenv/:0:0","tags":["Pipenv","Python"],"title":"A look at Pipenv","uri":"/posts/a-look-at-pipenv/"},{"categories":null,"content":"Differences The main difference of Pipenv to the way you might work with virtualenvs — which, for me, is switch to a particular virtualenv for a project, then run commands in the shell as normal, just with a self-contained Python setup — is that Pipenv is more contextual. With Pipenv, you change to the appropriate project directory and then run commands directly in that virtualenv by preceding them with pipenv run, e.g. pipenv run myscript.py. Pipenv knows which virtualenv to use based on the location you’re in. (You can also get more virtualenv-like behaviour by going to a project directory and doing pipenv shell where it effectively activates the virtualenv in the shell. However, one downside with this is that the subshell command history there only exists within that subshell; it is not stored in your main shell.) Another difference is that you would also tend to favour using pipenv over pip for installing packages; pipenv install \u003cSOME_PACKAGE\u003e also adds packages to your project’s Pipfile, which replaces the older requirements.txt file of specifying dependencies. ","date":"2018-08-26","objectID":"/posts/a-look-at-pipenv/:1:0","tags":["Pipenv","Python"],"title":"A look at Pipenv","uri":"/posts/a-look-at-pipenv/"},{"categories":null,"content":"Usage You can create a new Pipenv either implicitly by pipenv install --dev which installs dependencies for that project (including dev dependencies) in a new virtualenv it creates, or more explicitly by: pipenv --two or pipenv --three which gets you a new virtualenv with that version of Python. You can also use --python \u003cVERSION_NUMBER\u003e to use a specific point release of Python of your choice you have installed, e.g. 3.7. Furthermore, if you have pyenv installed, it will install the requested version of Python for you, if that version is not installed already. There are two broad uses of virtualenvs I had: for Python development to avoid any clash of package versions, which is covered quite well by the default behaviour of Pipenv. for running standalone Python software I want to run, e.g. Docker Compose, but keeping their installations entirely independent of each other to avoid any conflicts. This isn’t quite covered as well, because pipenv run requires you to be in the directory or a subdirectory of that directory2 where you’ve already run Pipenv and a Pipfile exists, as that’s how I assume it figures out which virtualenv to use. Otherwise, running Pipenv creates a new virtualenv! I think pipenv shell works around this dropping you into a new subshell where the virtualenv is activated. Alternatively, in bash, you can do source $(pipenv --venv)/bin/activate in the directory with the virtualenv, to work at a slightly lower level, with the virtualenv directly, without pipenv shell. These use cases are both handled reasonably well by Pipenv. ","date":"2018-08-26","objectID":"/posts/a-look-at-pipenv/:2:0","tags":["Pipenv","Python"],"title":"A look at Pipenv","uri":"/posts/a-look-at-pipenv/"},{"categories":null,"content":"Summary Pipenv seems to work well enough if you want to simplify using virtualenvs and management of dependencies. I think there are two groups of users that it might particularly suit: those who are newer to managing Python packaging and virtualenvs — as a means of abstracting away virtualenv management — and those who are more experienced but want to have a tool that integrates other features, e.g. checking for dependency vulnerabilities via pipenv check (which is provided by the safety package). However, if you read around, there is still some contention about Pipenv being recommended by the PyPA. Certainly, there are a considerable number of small issues that remove some of Pipenv’s sheen; I encountered a small one already reported within just a short time of using Pipenv. There are also several issues relating to dependency resolution, which seem a little more critical seeing as dependency management is one of the tool’s core goals. Pipenv then is no panacea for Python’s still byzantine dependency management. PEP 20’s call for “one obvious way to do it” is not yet fully heeded. But maybe it’s a step roughly in the right direction, even if there’s still some meandering to do before there’s a really simple and transparent workflow for Python. (However, I don’t think it’s uncommon for dependency management being tricky to get right either; Go has been around for a decade and is only just getting there. Python’s had considerably longer than Go to get it right though.) Finally, it is worth noting that there are other alternatives too. Poetry is a newer, and perhaps less well known, tool whose goals intersect with those of Pipenv. It fixes some of the existing issues of dependency resolution that pip-tools has (pip-tools is the underlying package that pipenv actually uses for this task), and therefore may be another useful contender to keep in mind. Confusingly, Pipenv’s name is capitalised, while pip’s is not. ↩︎ Caveat: it only looks a certain number of subdirectories deep by default, though this number is configurable. ↩︎ ","date":"2018-08-26","objectID":"/posts/a-look-at-pipenv/:3:0","tags":["Pipenv","Python"],"title":"A look at Pipenv","uri":"/posts/a-look-at-pipenv/"},{"categories":null,"content":"PyCon 2018: A quick note As PyCon 2018 happened last week, I’ve been watching a few talks. It’s sometimes difficult to keep up with changes to technology if you’re not paying constant attention. These kinds of talks are a useful way of trying to catch up a little. With that in mind, I’m writing up summaries of a few talks here. ","date":"2018-05-20","objectID":"/posts/solve-your-problem-with-sloppy-python/:1:0","tags":["PyCon","Python"],"title":"Solve your problem with sloppy Python","uri":"/posts/solve-your-problem-with-sloppy-python/"},{"categories":null,"content":"Solve your problem with sloppy Python The talk is available to watch here, and an easy watch. This was a pragmatic and enjoyable talk that would be easy for beginners in Python to follow. It’s a nice contrast from other programming talks too. Often, presenters, quite reasonably, focus on telling or teaching an audience what they believe to be best practice on a topic. Concentrating on a more practical side made for a light and refreshing approach. ","date":"2018-05-20","objectID":"/posts/solve-your-problem-with-sloppy-python/:2:0","tags":["PyCon","Python"],"title":"Solve your problem with sloppy Python","uri":"/posts/solve-your-problem-with-sloppy-python/"},{"categories":null,"content":"Code being discussed is for personal automation Writing code to solve your problem. Not writing code in a professional context. These are throwaway scripts. You’re the only person writing it and likely to see it. Rules Fail early and noisily. Use Python instead of shell scripts. Have fun! Guidelines Use latest Python version to take advantage of new features. Try automating even more, push as far as you can go, but may have some human intervention too. These projects are an excuse to try or learn new libraries/technologies. It doesn’t matter if the code’s hacky; you may not bother with tests. Quick and dirty code to get a job done. A couple of examples of this from the renaming files problem in the talk: using ls to generate a list of filenames in Python; add triple quotes around string, strip() and split() on newlines gives a list. using exceptions lists in renaming files (e.g. to add apostrophes to words), run through each check for each filename. Most won’t trigger, so you’re doing lots of unnecessary checks, and could refine that, but performance unlikely to be an issue for these kinds of tasks. Cutting corners is OK in this context, if it gets the job done. ","date":"2018-05-20","objectID":"/posts/solve-your-problem-with-sloppy-python/:2:1","tags":["PyCon","Python"],"title":"Solve your problem with sloppy Python","uri":"/posts/solve-your-problem-with-sloppy-python/"},{"categories":null,"content":"Example uses Renaming files. A general tip for file renaming tasks — in this case, downloaded radio shows — back these up via creating hard links with the same filenames in a backup directory. That way you can always revert to the original names, without having to backup the files. Could use Python’s os.link() but also possible via a simple shell loop: mkdir backup \u0026\u0026 for a in * ; do ln $a backup/$a ; done. Provisioning new machines, e.g. running apt install and configuring programs, e.g. copying license keys. Tidying audio files, creating playlists etc.; possibly with human intervention here. Ripping CDs, using simple metadata format that can be parsed easily. ","date":"2018-05-20","objectID":"/posts/solve-your-problem-with-sloppy-python/:2:2","tags":["PyCon","Python"],"title":"Solve your problem with sloppy Python","uri":"/posts/solve-your-problem-with-sloppy-python/"},{"categories":null,"content":"A final note Sometimes Python was used to call out to the shell. subprocess.run(s, check=True, shell=True) is better than os.system() because os.system() does not fail noisily, but subprocess.run() does. ","date":"2018-05-20","objectID":"/posts/solve-your-problem-with-sloppy-python/:2:3","tags":["PyCon","Python"],"title":"Solve your problem with sloppy Python","uri":"/posts/solve-your-problem-with-sloppy-python/"},{"categories":null,"content":"A thought One thing that didn’t get asked in the Q\u0026A at the end: how should you estimate whether the automation is actually costing you time? xkcd has covered this well in these two comics. In some cases, it can be reasonably predicted that, if the process is either time consuming or will be repeated often, it’s worthwhile to spend the time to write code as a solution to the problem. But, the “Rogue’s Gallery” radio show renaming given as the main example presumably took a few minutes to write, find the exceptions (by hand) and verify (probably also by hand) the code is doing what it should. There are only actually a few shows to rename, and these themselves could be done by hand, as a one-off task. ","date":"2018-05-20","objectID":"/posts/solve-your-problem-with-sloppy-python/:2:4","tags":["PyCon","Python"],"title":"Solve your problem with sloppy Python","uri":"/posts/solve-your-problem-with-sloppy-python/"},{"categories":null,"content":" and dealing with `rc.local` not running tasks as planned. ","date":"2017-12-10","objectID":"/posts/upgrading-raspbian-and-coderc.local/code-failures/:0:0","tags":["Raspberry Pi","Raspbian","Linux"],"title":"Upgrading Raspbian and \u003ccode\u003erc.local\u003c/code\u003e failures","uri":"/posts/upgrading-raspbian-and-coderc.local/code-failures/"},{"categories":null,"content":"A Raspberry Pi as a wireless bridge for old hardware This is something I did a couple of months back, but figure it’s worth documenting, to maybe help anyone with a similar problem search for it. One use for my Raspberry Pi is as a wireless bridge, and this is mainly for a now ancient Xbox 360. Yes, the one without built-in wifi, it just has an Ethernet port for networking; not much good when your router is nowhere near a TV. Originally, and as is often customary when solving technology problems, I used instructions that I cobbled together from various places. I’d been slack on upgrading my Raspberry Pi’s Raspbian. It was working perfectly fine for what it was doing, so there was no immediate need to touch it. Because of this, it has languished on a distribution based on the now aging Debian Wheezy. More recently, with the recent WPA2 exploit in October, and the then lack of any as-yet patch even for Debian Wheezy (which has since been patched, not sure about Raspbian itself), I thought I should finally upgrade, so the Pi was patched against this and any other future security issues. I didn’t particularly want to go to the trouble of doing a fresh install, so I tried an in-place operating system upgrade, deciding that the worst that could happen is the upgrade fails and I’d have to do a fresh install anyway. ","date":"2017-12-10","objectID":"/posts/upgrading-raspbian-and-coderc.local/code-failures/:1:0","tags":["Raspberry Pi","Raspbian","Linux"],"title":"Upgrading Raspbian and \u003ccode\u003erc.local\u003c/code\u003e failures","uri":"/posts/upgrading-raspbian-and-coderc.local/code-failures/"},{"categories":null,"content":"The upgrade process I actually upgraded in two stages. First from Wheezy to Jessie, and then I figured if that went OK, I could try upgrading again, from Jessie to Stretch too. Upgrading the distribution actually worked well. With my Raspbian Wheezy install, I upgraded to Jessie as described on the Raspberry Pi forum. Everything seemed to work, including the Xbox’s internet connection. Since I was feeling brave, I decided to upgrade again. However, I didn’t feel so brave that I would do that without first backing up the working system, so I backed up the SD card with dd to a gzipped image in case something went wrong and I could recover a working system quickly. Next, I upgraded from Jessie to Stretch. This is the same process as for Wheezy to Jessie, just with different repository sources used. Once again, the upgrade worked fine, the Pi booted up, and I could connect to it wirelessly. ","date":"2017-12-10","objectID":"/posts/upgrading-raspbian-and-coderc.local/code-failures/:2:0","tags":["Raspberry Pi","Raspbian","Linux"],"title":"Upgrading Raspbian and \u003ccode\u003erc.local\u003c/code\u003e failures","uri":"/posts/upgrading-raspbian-and-coderc.local/code-failures/"},{"categories":null,"content":"A stretch too far But, the Xbox’s internet connection no longer worked. Checking the Pi, I spotted what looked like error messages appearing at boot time. The first problem is checking the boot log messages that quickly disappear from the screen. Since Debian now uses systemd, you need sudo systemctl to review those messages. What I spotted there is that there was an error that indicated that commands I had in rc.local were not running correctly. As mentioned, in my description of my Xbox and Pi configuration, I was using rc.local to run the commands on boot that ensured the bridge worked, instead of having to run anything by hand every boot. That seemed strange. More so when running the script by hand caused it to work as normal, which allowed the Xbox to once again connect to the internet as it did on Raspbian Jessie. Why did this script fail on boot via rc.local? As mentioned on the Raspberry Pi forums, the difference presumably is that the Pi’s networking isn’t configured at the time of the script trying to run. Trying to then change this configuration when it’s not ready will then fail. So there are no guarantees about when rc.local is run. If you have dependencies on other things, the moral of this longish story is that you should create a systemd service and have systemd launch it after everything else; this fixed the problem and everything finally was working again. ","date":"2017-12-10","objectID":"/posts/upgrading-raspbian-and-coderc.local/code-failures/:3:0","tags":["Raspberry Pi","Raspbian","Linux"],"title":"Upgrading Raspbian and \u003ccode\u003erc.local\u003c/code\u003e failures","uri":"/posts/upgrading-raspbian-and-coderc.local/code-failures/"},{"categories":null,"content":"Configuration For reference, here’s what I have configured: wireless_bridge.sh is saved in /usr/local/sbin, with 755 file permissions, root as owner and group, and contains: #!/bin/sh -e /usr/sbin/ifplugd eth0 --kill /sbin/sysctl -w net.ipv4.ip_forward=1 /sbin/ifconfig eth0 192.168.1.1 /sbin/iptables -t nat -A POSTROUTING -o wlan0 -s 192.168.1.0/24 -j MASQUERADE wireless-bridge.service is saved in /etc/systemd/system, with 644 file permissions, root as owner and group, and contains: [Unit] Description=Wireless Bridge Service After=multi-user.target [Service] Type=idle ExecStart=/usr/local/sbin/wireless_bridge.sh [Install] WantedBy=multi-user.target You could possibly change multi-user.target to one of systemd’s network targets, but I didn’t bother testing this out, as the configuration above just seemed to work (with, I suppose, the possible cost of the bridged connection not being available until slightly later). I also have the Xbox network settings as: IP address: 192.168.1.2 Subnet mask: 255.255.255.0 Gateway: 192.168.1.1 and the primary DNS server set to my router’s IP address. ","date":"2017-12-10","objectID":"/posts/upgrading-raspbian-and-coderc.local/code-failures/:4:0","tags":["Raspberry Pi","Raspbian","Linux"],"title":"Upgrading Raspbian and \u003ccode\u003erc.local\u003c/code\u003e failures","uri":"/posts/upgrading-raspbian-and-coderc.local/code-failures/"},{"categories":null,"content":" for when buying a digital piano, and the advantages of buying used. As I’ve documented earlier here, I’ve been taking piano lessons since the start of this year. There I also mentioned I’d been using a keyboard to learn. Since then, I’ve switched to a second-hand digital piano for practising. So why? What I discovered is that a keyboard with velocity sensitive keys (i.e. capable of producing different key volumes depending on how you play) is fine when starting out. Any music you play at first will be simple, but needing to play notes at varying volumes is likely something you’ll be doing almost from the start. If you’re planning on trying out piano lessons, if you have such a keyboard already or can borrow one for a few months, that’s a sensible initial step. Don’t let not having a piano right now put you off starting learning now: it’s a substantial commitment to get one. It’s probably sensible to ensure that you’re likely to stick with learning first, before plunging in. But, a few months on, once you’ve made a little progress, the inadequacies of using a keyboard as a substitute for a piano soon become apparent. One big problem is that the feel of hitting the keys is much different, which is difficult to get used to when playing on a real grand piano for lessons. Another is that, like the keyboard I used, many keyboards only have 61 keys, not the 88 that standard pianos have. This can be confusing, and can hamper you relatively early on. Even on the simple pieces I was learning, there were occasional cases where you’d just have to miss or mime playing a note out, because it didn’t exist on the keyboard. The difference in the number of keys also made orienting myself difficult when sat at the real piano for lessons, and I’d get “surprised” by the presence of the extra keys. At that point, it felt like that the keyboard was hindering my progress, and a natural juncture to replace it in my practice. ","date":"2017-10-22","objectID":"/posts/a-digital-piano-buyers-guide-for-learners/:0:0","tags":["music","piano "],"title":"A digital piano buyers' guide for learners","uri":"/posts/a-digital-piano-buyers-guide-for-learners/"},{"categories":null,"content":"Why a digital piano? If you’re not too aware of digital pianos, and I wasn’t before I started looking into this, Wikipedia is a good starting point. There, we learn, digital pianos are really high-end keyboards that are designed to be good facsimiles of real pianos, in both sound and feel of how it plays. (I’d add that sometimes, though not always, they are designed to mimic the look of a real piano too.) That explains why you might want a digital piano over a keyboard. But why not just get a traditional acoustic piano? Especially when looking around, you can likely find pianos on offer for the literal giveaway price of £0, providing you can collect. Reading on, the Wikipedia entry explains good reasons for this. Acoustic pianos weigh a lot. It’s a considerable task to get one of these to where you live, especially without damage to the instrument. Likewise, if you need to move for whatever reason, you face this problem again. Likewise, acoustic pianos can take up a lot of room. Some digital pianos can do too (you can even get digital grand pianos, for reasons that aren’t that clear to me), but there are digital pianos available that are just the keyboard, and are relatively easy to move. For example, Yamaha’s current low end keyboard-like models weigh a little bit more than 10 kg. Acoustic pianos will also need tuning. That’s an ongoing cost and one that’s also fairly pricey. Digital pianos could also require repair too, although not likely with the regular frequency that an acoustic would need tuning (pardon the pun). One downside with digital pianos is that you’re somewhat reliant on having local availability of technicians who specialise on those models, and the parts being available to repair the piano; reasons why you might want to opt for a well-known brand. Almost all (or maybe even all) modern digital pianos, and even lots of older ones, will have some means to use it as a MIDI keyboard. A digital piano in a given price range likely has fewer voices (i.e. types of sound it can play, e.g. piano, organ, strings) than a comparably priced keyboard, but you can always hook it up to a computer and use it to control whatever instruments you have available there in a digital audio workstation. Digital pianos have headphone outputs, so that you can practice quietly. This is not usually an option on acoustic pianos. I say usually, because you can get hybrid acoustic-digital pianos that allow switching between acoustic and digital modes, though these are very expensive and still suffer from the problems of tuning, size and weight. Hybrids aside, the downsides with digital pianos are that the feel and sound is a simulation of a real piano. Even with today’s technology, they’re never going to be quite as authentic as the real thing. But especially when still a beginner, a decent digital piano is probably more than sufficient. ","date":"2017-10-22","objectID":"/posts/a-digital-piano-buyers-guide-for-learners/:1:0","tags":["music","piano "],"title":"A digital piano buyers' guide for learners","uri":"/posts/a-digital-piano-buyers-guide-for-learners/"},{"categories":null,"content":"What to look for It’s daunting spending a considerable amount on a piano, if you still feel like you don’t know what makes for a good instrument. From reading around, there are some general considerations you can think about: size and cabinet; as I mentioned above, digital pianos come in different sizes. If you’re pushed for space, then you might have to go for one of the keyboard-like pianos. There are also cosmetic choices to make; if a piano comes built-in to a cabinet, do you like the appearance and colour of it? pedals; acoustic pianos usually have three pedals. I think most electric pianos usually have three too, though lower end models might only have two. Three, then, really is the magic number, so you may want to go for a model with the pedals you’d expect on a acoustic. voices; this is maybe less important. The main thing for learning here is that the piano sounds realistic. Having other sounds to use is a nice bonus, but not essential. polyphony; this is the number of notes that can sound simultaneously without earlier played notes that are still sounding just being dropped. Current low end models may have 64 note polyphony, whereas better models will have 128, 192 or even more. It’s not quite as simple as the number of notes you play simultaneously, since, for example, using multiple voices (e.g. piano and strings) at once, or using the sustain pedal to prolong the sound of non-held notes will eat into this note “budget”. More is better, especially if using multiple voices simultaneously. But given that 64 note polyphony seemed to be a satisfactory standard in the past, anything above that is likely very good for a beginner piano. headphone noise; after I had an experience with a keyboard’s headphone output being distractingly noisy, I knew I should check this on any piano I bought. Just remember to take the pair of headphones you use with you when looking at a piano to test this. Trying to concentrate with a distracting buzz in your ears isn’t a great feature to have. the feel and action of the keyboard; I can appreciate that higher end pianos aim to more realistically replicate the feel of a real piano. For instance, the weights of the keys, and the texture. And if you’ve experience of several pianos, this is probably easy to discern. For me, having played on one acoustic piano, this was far less easy for me to know what was good and bad. More expensive digital pianos tend to use different hardware than cheaper variants, whether you can tell much of a difference as a beginner is difficult for me to know. branding; common brands are those you might expect, the likes of Roland, Korg and Yamaha. There are also no-name branded pianos available, but I suspect the quality of these can vary wildly. I’m not always one to go for big name brands, but I think in cases like this, where there’s likely much expertise gone into the research and development, it’s probably easy to cut corners and end up that might look the part, but is in fact considerably inferior. ","date":"2017-10-22","objectID":"/posts/a-digital-piano-buyers-guide-for-learners/:2:0","tags":["music","piano "],"title":"A digital piano buyers' guide for learners","uri":"/posts/a-digital-piano-buyers-guide-for-learners/"},{"categories":null,"content":"And with used pianos? My teacher’s advice was to stick to a low budget and pick up a used model (given that it would be almost certainly an improvement on the keyboard I was using), rather than a brand new, but low end model. When I was looking at the market, digital pianos seemed relatively mature. The manufacturers continue to churn out regular updates to their product lines, but, much like mobile phones now, it doesn’t seem that there are any real technological revolutions that have happened over the past couple of years. Buying used seems like a good way to get more for your money. When buying used, the condition of the instrument is important. Spares may be few and far between and expensive if anything does break. That’s another reason not to maybe spend a huge amount on old kit: the repair costs may also be very expensive or repair may rely on you getting hold of a second broken model for spares. (This also is another reason why no-name digital pianos may not be a good investment, if there are no parts available or indeed anyone who knows how to repair those models.) Bearing all that in mind, the most I wanted to spend was the cost of the current low end Casio and Yamaha electric pianos, and I didn’t really want something more than about a decade old. ","date":"2017-10-22","objectID":"/posts/a-digital-piano-buyers-guide-for-learners/:2:1","tags":["music","piano "],"title":"A digital piano buyers' guide for learners","uri":"/posts/a-digital-piano-buyers-guide-for-learners/"},{"categories":null,"content":"Buying a used piano In the end, after a week or two of searching on eBay and Gumtree — if you’re in the US, Craigslist is probably a good substitute for Gumtree — I managed to find a Yamaha Clavinova that seemed ideal. It was within driving distance to collect, seemed reasonably priced, was in my budget, and claimed to be in good condition. When looking around, I found that used digital pianos were generally claimed to be well cared for. My hunch is this is because these are reasonably expensive bits of kit new and therefore buyers want to look after them. Another reason is that a lot of sellers are getting rid of them because the piano is no longer in use, either because they have bought a newer instrument or have given up learning. Not in use means not becoming worn. This was much the case for the piano I ended up buying. The seller never really took to it, was looking to replace it with a synthesizer and so it looked almost brand new. There was plastic wrapping still on the pedals, hardly a visible scratch on the cabinet, all the manuals were present, and even a catalogue of various Clavinova models that the original purchaser had been given while they were originally deciding which one to buy. All a good portent for the piano being treated well. ","date":"2017-10-22","objectID":"/posts/a-digital-piano-buyers-guide-for-learners/:3:0","tags":["music","piano "],"title":"A digital piano buyers' guide for learners","uri":"/posts/a-digital-piano-buyers-guide-for-learners/"},{"categories":null,"content":"Some other tips on buying used Make sure you try as much as possible before handing over cash. Do all of the buttons work? Does every key plays smoothly? As mentioned above, I took my own headphones to test the headphone output too. Don’t be afraid to haggle a little. Prices of pianos vary a lot as there are lots of different models and often relatively few sellers around (although major cities will tend to have more). Unless someone has expressly stated the asking price is the final price, even if the asking price is reasonably sensible, I don’t think there’s any harm in offering something like 10-15% below to give them room to counter-offer, and maybe give you a small discount. Since pianos are big, and sellers may well want to get rid of them quickly, if they sense a possible sale, they may well be happy to negotiate a little. Make sure you’re clear on what’s included for the price. Sometimes sellers will sell piano stools or stands separately from the piano itself. Going to a manufacturer’s website and getting hold of a PDF copy of the manual, if available, is a good way of answering questions you may have prior to purchase (e.g. is the MIDI via USB? does the piano need drivers for connecting to a PC? are they available?) or finding detailed specifications. You can usually find these in the support section of a manufacturer’s site. Similarly, if you’re intending to ever use your piano as a MIDI keyboard, make sure that drivers are available for the piano for your operating system from the manufacturer. Don’t be too set on going for a particular model or brand. The market depends on what’s available in your area, so I’d just stick with a budget and features you’re not willing to compromise on. Most sellers want you to arrange collection, which likely means you travelling there to collect in person (unless you pay someone to move it, which will probably cost you a fair bit). ","date":"2017-10-22","objectID":"/posts/a-digital-piano-buyers-guide-for-learners/:3:1","tags":["music","piano "],"title":"A digital piano buyers' guide for learners","uri":"/posts/a-digital-piano-buyers-guide-for-learners/"},{"categories":null,"content":"Three months on In the end, I went with the first Clavinova I looked at, and have been very happy with it since. While a little unusual to switch from the previous keyboard initially, it didn’t take that long to adjust. Since then, it has made a noticeable difference in my confidence going into piano lessons and being sat at an acoustic grand piano. It now feels much more a natural transition to go from playing at home to playing in lessons. ","date":"2017-10-22","objectID":"/posts/a-digital-piano-buyers-guide-for-learners/:4:0","tags":["music","piano "],"title":"A digital piano buyers' guide for learners","uri":"/posts/a-digital-piano-buyers-guide-for-learners/"},{"categories":null,"content":" Regardless, here's why I think you should play it. ","date":"2017-08-20","objectID":"/posts/full-of-heart-an-undertale-review/:0:0","tags":["game","review"],"title":"Full of heart: an Undertale review ","uri":"/posts/full-of-heart-an-undertale-review/"},{"categories":null,"content":"Playing Undertale fills you with determination Undertale’s almost two years old. Yet I only bothered to play it this year. If you have a PS4 and never got it on PC, it was released this week. While the PC version will have been much cheaper while on sale, and runs on just about any hardware, if you really must play it on a console, now you can. Why didn’t I bother sooner? Hype is one reason. When it’s that vocal — and with Undertale, it’s near deafening — I always tend to do as Public Enemy instructed. The other is its resemblance to 16-bit Japanese RPGs (JRPGs). While I appreciate those games are much loved, I’ve never really shared that affection. When I did actually get round to Undertale on PC, it happened to be on sale. I figured it was cheap enough to see if they hype was warranted and I wanted an escapist diversion from real world events. It seemed as good a game to try out. Without spoiling too much, you’re a human that finds yourself fallen down a mountain, and lost somewhere in the underground world of monsters. The rest of the game sees you journeying through to try and find some route home. Mostly this is done by exploration, although there are a few puzzles dotted along the way that need solving before you can progress, and monsters that you can fight. So much, so familiar. But it’s a wonderful game. Those aspects of JRPGs that I never got on with are kept to a minimum. There’s a minimum of backtracking. The random battles, a staple of that genre, that prevent you from walking more than a couple of footsteps — because swoosh swoosh swoosh TWO SLIGHTLY SURLY ENEMIES IN MATCHING OUTFITS APPEAR, and cue battle music — are thankfully omitted too. When you actually battle, the system is novel: a mixture of strategy and shmup bullet dodging. Battles can even be avoided, and these options are left open to the player to decide. Both in and out of battle mode, the dialogue is predominantly witty and charming, and avoiding lazy territory, instead of prolonged and awkwardly translated. Another thing that the game does well is, without any spoilers, is continually confronting you and challenging your expectations of the situations you’re placed in, at times akin to what something like The Stanley Parable does in trying to break the fourth wall. Without playing and without spoiling it, it’s hard to convey that without being anything other than handwavingly effusive. Characters and combatants that only appear briefly don’t just feel like they’ve been arbitrarily placed there, but like they have a real back story, even if that’s not ever entirely revealed. And while the game has this rich assortment of characters, it also has character itself, full of soul and heart. ","date":"2017-08-20","objectID":"/posts/full-of-heart-an-undertale-review/:1:0","tags":["game","review"],"title":"Full of heart: an Undertale review ","uri":"/posts/full-of-heart-an-undertale-review/"},{"categories":null,"content":"Fantastic Mr. Fox Undertale’s singular cohesion is likely because it was largely the product of one person’s imagination, Toby Fox. And while its graphics are simple, but functional, it is a polished game, backed by an incredibly strong soundtrack also composed by the developer. I’ve been fairly lucky that the few games that I’ve played through in recent years have been particularly good choices for one reason or another. Undertale’s another, and one of the more memorable. It genuinely feels like an experience that would only work as a game, not a book or film. True, you could describe a similar journey with similar characters in print or on screen. But it’s the act of playing through and having to make decisions on how you play through that’s intrinsic to how it works and how it makes you feel; that’s very different to a more passive experience. Where many otherwise good games fail is extending their gameplay through repetition and tedium, churning out more of the same. Undertale avoids this, keeping the core game short and sweet, but giving you the opportunity to play through in a very different play style. I’ll definitely look forward to revisiting that world in a few months. (That’s a really strong compliment from me. I have so little time to play games these days that there are very few that I’d ever return to once I’ve played through.) Undertale is the perfect game for a Sunday afternoon where rain is beating down your windows, a dull Tuesday where you’re miserably plagued with cold, or maybe even a snowy Thursday where you have the luxury of being snowed in, unable to get to school or work, and are just warming up with hot chocolate after already having been out to play in the snow. It’s that kind of game. ","date":"2017-08-20","objectID":"/posts/full-of-heart-an-undertale-review/:2:0","tags":["game","review"],"title":"Full of heart: an Undertale review ","uri":"/posts/full-of-heart-an-undertale-review/"},{"categories":null,"content":"Note that I actually learned how to flip (or tumble) turn a while ago, drafted this back then and only just got round to posting it. ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:0:0","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":"Turn down for what? I have been swimming for probably the best part of twenty years. That is regularly over a period of twenty years, not constantly; an hour usually is enough. Because I didn’t gain confidence in the water until much later than I should have done, I’ve never had anything much in the way of serious coaching. I’ve swum for fitness and, apart from swimming lessons for the absolute basics, have made pretty much any improvements to my swimming by myself. Though swimming feels quite natural to me now, this lack of training and racing means I never had any real incentive to learn how to properly tumble turn. It always looked really daunting when I saw others doing them. But I more recently thought that I wanted to have a go at learning them, and eventually got there. My turns are far from perfect, but the fact that I have the co-ordination to flip myself over is impressive enough to me, to be honest. What’s particularly nice is that flip turns are a more fluid way of turning, than grabbing the side with your hands and pushing off. It means you can maintain more speed, and just feels smoother. Bear in mind, I’m hardly a good swimmer. And I’m definitely not a swimming coach either. But, since I taught myself and can remember it reasonably well, I thought it was worth documenting how I learned. It took me a while to figure all this out myself. It’s the learning experience of someone who taught themselves very recently as an adult, rather than coaches who have a lot more knowledge and experience than me, but may have less of a memory of how they first learned the skill. ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:1:0","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":"Turning There are lots of videos, such as this tutorial but, when I was learning, I found that videos tended to go over the mechanics of the turn itself, not so much how you go about learning. If you watch that video, you’ll see that for a front crawl flip turn, all that’s really involved is something like a forward roll, albeit in the water. As you approach the wall, you flip yourself over forwards, push off the wall with your feet and swim off in the other direction. Notice that if you just roll over forwards, your body will flip from face down to face up. So you also have to rotate yourself back to face down as you push off from the wall. And ideally all this is in a smooth motion. Simple, eh? Like many things, once you grasp it, it’s easy. But it’s difficult to pick up at first. You’re coordinating your body in a way which you may not have before, along with thinking about keeping on swimming, breathing and without injuring yourself. Putting all that together is surprisingly difficult. ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:2:0","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":"What do you have to worry about? ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:3:0","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":"Hitting the wall Fortunately, this hasn’t happened to me yet, not even while learning, but misjudging your position may lead to unexpected bumps. And the worry of doing that is an impediment to learning, because you may be overcautious and hold back too much. In fact, you can actually get quite close, probably about an arm’s length without too much worry. The best way I found to build confidence is to try from too far out and then progressively get closer to decide the right distance to turn without colliding. Often, pools have markers on the floor for swimmers to judge the distance. If yours doesn’t then just find your own marker. Maybe that’s by counting tiles from the side or just by remembering how far in front you need to be at the point where you should turn. You may not need these guides once when you’re more comfortable with how many strokes you are from turning, or when your instincts improve. There’s a bit of fine tuning with the ideal distance: if you’re too close, you may end up hitting some part of your body against the wall or at least take measures to awkwardly prevent yourself from colliding. While if you’re too far, you’ll either miss the wall completely, or have to stretch your legs out to reach the wall and get a weak push off as your legs can’t spring you off the wall. A counterintuitive thing that helped me was sometimes trying this with breaststroke, not frontcrawl.1 This means you’re approaching more slowly, so less worried about hitting the wall. It’s also easier to keep the wall in sight, instead of while you’re rotating in frontcrawl. Related to this, I incorrectly assumed that you need to be going fast to keep your momentum moving through the turn. Even going slowly, you can still flip over without too much trouble. ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:3:1","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":"Rolling Some of the video guides I watched on YouTube also seemed to suggest flipping yourself over in the water, not even being near the wall, just from standing. This also stops any possibility of your forehead becoming closely acquainted with the swimming pool wall, but I actually found this quite difficult when I was learning to turn (though easier now I can actually turn). This is also useful since you can actually use the same motion to flip direction even without a wall which can be handy if you’re in a busy lane and want to avoid overtaking a cluster of slower swimmers in front. The most useful exercise for me was doing forward rolls. If you haven’t done forward rolls in a while (and why would you?), doing a few of those on dry land as practice helped me get the idea of the required motion. ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:3:2","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":"Breathing Having water rushing up your nose is not a pleasant feeling that’s not dissimilar to spraying carbonated drinks up there. (It always reminds me of Irn-Bru; that drink seemed to be notorious for sending jets of eye-watering bubbles up my nose.) At first, it’s irritating and distracting, and unfortunately may well happen almost every time you attempt to do a flip turn. Breathing out through your nose slightly while flipping stops this, but knowing exactly when you start needing to breathe out takes some practice, especially when you’re possibly slightly disorientated from flipping. At the same time, you don’t want to breathe out unnecessarily and leave yourself gasping for breath. Initially, you probably overcompensate and breathe out too much. All that’s need is a gentle and brief air flow while you’re turning, possibly far less than you imagine. (When I first drafted this post, even after a good few months of turning like this, I was still occasionally getting caught out, and it takes a few seconds to have the awkward fizzy feeling dissipate. A considerable time later, this isn’t the case now; it’s relatively rare that I get the breathing wrong.) ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:3:3","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":"Practice Practice isn’t anything unique to this, but it’s probably the most important thing. You’re learning several new physical skills and having to employ them all seamlessly to get the result you want. I think I spent a few minutes of several swim sessions over several weeks to start getting a feel for turning. This was a little demoralising at times, especially as failing means you’re often getting nowhere, metaphorically and literally (I’d often end up stuck at the side of the pool, bursting up for air, to get the water out of my nose.) What you have to remember is that getting things horribly wrong is a key part of learning physical skills. Eventually, you condition yourself to not do the Wrong Things, and you slowly improve. Slowly, the motion that was needed to get myself round started to become apparent. After that, I spent a couple of sessions where I was focusing pretty much exclusively working on this only, swimming from end to end, but not particularly exercising hard and just concentrating on the turns themselves. Even then, I kept getting worried that I’d forget what I’d learned, so I ensured that I attempted to turn on just about every length I was swimming for a while after that. It can be frustrating at first, but when it does click, then you’ll be able to turn every time you want to, rather than making a mess of it. After that, you can spend a lot more time refining aspects of it, optimising where you start turning from, trying to push off to get the most distance from the wall, and timing your breathing so you can stay underwater without it being uncomfortable. Despite me finding it tricky to learn, I’d say it’s well worth the effort, even if you’re swimming just for fitness. Tumble turns give you another way of mixing up your training, can help constant lengths flow together more seamlessly, and aid you maintaining your speed when swimming in shorter length pools. If you need any more convincing, just compare flip turning swimmers to those not when you’re at the pool; you’ll see it makes for a considerable drop in lap times once you get it right. If you’re racing breaststroke, then you don’t actually flip turn, but touch the wall with both hands, and then push back off the wall with your feet, but using breaststroke is still a nice way to learn. ↩︎ ","date":"2017-07-29","objectID":"/posts/flipping-out-over-swimming-turns/:4:0","tags":["swimming"],"title":"Flipping out over swimming turns","uri":"/posts/flipping-out-over-swimming-turns/"},{"categories":null,"content":" copyright infringement tells us both about music industry short-sightedness and hosting creative projects online. !!! article-edit \"\" Edit 2017-07-18: SoundCloud have now restored Rinse FM’s account. Nonetheless, the points below still stand in relation to hosting content under online accounts that have the potential for copyright takedowns. ","date":"2017-07-15","objectID":"/posts/rinse-fms-soundcloud-account-takedown/:0:0","tags":["music","radio","Rinse FM","SoundCloud"],"title":"Rinse FM's SoundCloud account takedown","uri":"/posts/rinse-fms-soundcloud-account-takedown/"},{"categories":null,"content":"And it’s shutdown For many dance music lovers, the taking down of Rinse FM’s SoundCloud profile yesterday was a sad event. Along with their account, a huge archive of shows they hosted on SoundCloud have all gone due to copyright infringement complaints. It’s difficult to overstate Rinse’s influence in dance music. The station was heavily instrumental in pushing grime, and later dubstep in the UK, and contributing to the massive spread of these genres outside of these shores. While much of the UK radio broadcast frequency space is pumped full of the same cloying pop music being endlessly recirculated,1 Rinse FM is a refreshing breath of fresh air. Rinse FM started life as a pirate station, broadcasting illicitly via aerials perched on tower block rooftops, but more recently acquired a legal license. Peculiar then that they are now legitimate under the traditional broadcasting regulations, but don’t have the same freedoms when distributing those same broadcasts via the internet. Fortunately Rinse has an established physical presence as an FM station and a fanbase built long before the internet became popular for distributing music. Rinse also has a sizable following on other social networks, allowing them to communicate what’s going on, and, probably as important, a web site that lets them continue to put out their shows uninterrupted, via live streaming and podcasts. Without seeing their SoundCloud playback statistics or their web analytics before and after this change, it’s difficult to know how much of an impact that has on the number of people listening to Rinse FM online. But, given the news of SoundCloud’s redundancies this week, it may well be that everyone’s profiles are getting deleted from SoundCloud sometime soon like it or not. Perhaps any impact on Rinse’s future prospects may well be minimal. But, such an account deletion could be disastrous for the progress of a smaller project, say a independent producer, or a niche podcast, who are maybe start to make waves. All of a sudden their content evaporates (easy to replace somewhere else) together with all of their following on that site (perhaps more difficult to rebuild). ","date":"2017-07-15","objectID":"/posts/rinse-fms-soundcloud-account-takedown/:1:0","tags":["music","radio","Rinse FM","SoundCloud"],"title":"Rinse FM's SoundCloud account takedown","uri":"/posts/rinse-fms-soundcloud-account-takedown/"},{"categories":null,"content":"What can we take away from this takedown? ","date":"2017-07-15","objectID":"/posts/rinse-fms-soundcloud-account-takedown/:2:0","tags":["music","radio","Rinse FM","SoundCloud"],"title":"Rinse FM's SoundCloud account takedown","uri":"/posts/rinse-fms-soundcloud-account-takedown/"},{"categories":null,"content":"It seems a backwards step for the music industry Take a look at the podcast page and, in any given week, you’ll see a diverse range of artists hosting shows, and invariably a top line up. Today, you have a house legend, Masters At Work’s Kenny Dope, on only a couple of hours after alternative electronic producer, Forest Swords. Whether old hands or not, the hosts are often influential taste makers. Because of that, Rinse’s audience no doubt overlaps with who the music industry are targeting: young and early adopters of new music. Why then does someone, somewhere presumably think it’s a smart move to hinder that? These listeners likely form the dedicated audience that same industry is crying out for: those that will go out and track down music, share it with their friends, go to shows to see acts playing out, or even become the next big DJs or producers of the future. Here’s another thing: many of Rinse’s shows are in a mix format. That’s because mixes are a natural way of presenting modern dance music. Maybe I’m naive as to the machinations of the music industry, but both as a listener, and as a (mediocre) DJ, it’s difficult to see the logic behind taking down DJ mixes at all. As well as creative expression and entertainment in their own right, mixes can act as promotional tools. The featuring of tracks in mixes may well encourage listeners to buy the tracks, or at least to head over to a streaming service to hear it uninterrupted. Keep in mind that a track in a mix isn’t usually a substitute for hearing the track end-to-end: they’re different contexts. And DJs listening in may well decide to shell out for the full track to play in their own sets. When there’s so much music, past and present competing for attention today, you’d think that anything that might nudge listeners to a given label’s artists would be welcome. ","date":"2017-07-15","objectID":"/posts/rinse-fms-soundcloud-account-takedown/:2:1","tags":["music","radio","Rinse FM","SoundCloud"],"title":"Rinse FM's SoundCloud account takedown","uri":"/posts/rinse-fms-soundcloud-account-takedown/"},{"categories":null,"content":"Where to host projects? So if you’re starting with a project that you’re serious about and you want it to reach the audience you’re aiming for, what do you do? What this situation tells me is that it’s vital to be cultivating a following in multiple places. That way you’re not back to square one should your profile on a single site be taken down. Right now, I have an idea for a music-related project I’m intending to put together. To keep things simple starting out, I’d considered just having the project exist as a SoundCloud account, instead of creating a blog to go with it. After this week with the Rinse FM takedown, coupled with the possibility of SoundCloud disappearing, my thinking has shifted entirely. Owners of accounts on online services, as I’ve written before,2 have little real ownership of what happens with those accounts. If the site’s owners or moderators don’t like what you’re doing for whatever reason, you’re gone. Someone might complain about what you’re doing on the service, just as happened to Rinse FM, and likewise you’re gone. For what I’m planning, it’s very unlikely that copyright claims would be an issue, but the potential of having your account taken down for some other reason, and perhaps outside your control, still stands. Certainly, radio stations like Rinse and DJs could try their luck on Mixcloud or other alternative sites, but there’s nothing to say that the same copyright issues might occur there in future, even if those sites are relatively permissive now. It’s difficult. The big services are those where the users are. Creators are often tied to publishing using those services if they want to get people interested, as those services are where people are browsing, searching and listening. But, over-reliance on a particular service means that if it fails you, either destroying your account on a whim, or because the site itself is shutdown, the knock-on impact for getting visibility (or audibility) for creative projects can be a big blow. Hedging your bets seems the only viable solution. Don’t get me wrong. I’m very much not against pop music. The “same” in that sentence is an important qualifier of why I dislike those stations so, so much. It’s the lack of individuality and the repetition that gets me. If you flick through the commercial pop stations, could you tell them apart? When there’s so much music that I know I’ll never have enough time to discover and hear, it’s such a pity that show producers and station bosses are often so unadventurous. ↩︎ Look what happened to my GitHub account last year, for example. ↩︎ ","date":"2017-07-15","objectID":"/posts/rinse-fms-soundcloud-account-takedown/:2:2","tags":["music","radio","Rinse FM","SoundCloud"],"title":"Rinse FM's SoundCloud account takedown","uri":"/posts/rinse-fms-soundcloud-account-takedown/"},{"categories":null,"content":"Why learn now? Other posts on my blog highlight the small theme of me very, very slowly trying to get better at making music. When you’re a novice, it’s natural to ask what’s the best way to improve? Some common advice I’ve found on music production forums goes like this: focus on develop piano playing skills first, and less on learning software or production itself. I think that’s for two big reasons. An analogy is that being unable to touch type would be a hindrance for writing and coding. You can still do these things regardless, but the lack of being able to express yourself quickly does slow your progress. Likewise with music, yes, you can painstakingly draw notes in software,1 but being able to play a melody or chord progression means that you can transcribe something in your head much more quickly and directly, as well as improvise effectively. Being able to play also gives you hard fought experience of both picking up music theory as you learn and seeing it constantly applied. When playing pieces of music that “work” while seeing how they are made, it’s surprising sometimes just how some melodies can be pleasing, whether in spite of, or indeed, because of their simplicity. With that motivation in mind, I’ve been taking piano lessons since the start of this year. And as I’ve just this week played a very small part in a concert, albeit a very simple accompaniment, it seemed as good a point as any to document how I’ve found this as an adult learner. ","date":"2017-06-11","objectID":"/posts/do-you-even-liszt-five-months-of-learning-piano/:1:0","tags":["learning","music","piano"],"title":"Do you even Liszt? Five months of learning piano","uri":"/posts/do-you-even-liszt-five-months-of-learning-piano/"},{"categories":null,"content":"Tuition or self-taught? Lessons can be expensive for what is a relatively short time with a teacher, so why would you bother paying? You’re doing most of the learning by yourself. Very true, but lessons have a lot to offer. It makes a commitment to sit down and learn something for the next week, which is very important. Aside from the odd day when I’ve been ill or away, I’ve usually managed to get in at least a daily half-hour practice session. Having a scheduled lesson with a teacher really does act as a promise to ensure you keep time aside for practice. At times, I’d have the idea in my head that I’d not yet got enough done for the next lesson. That’s usually enough motivation for me to sit down and work on piano. You have a deadline to meet. Teachers can provide guidance of specific pointers, on technique, on how to play tricky sections, or on explaining notation. Particularly with piano, at least to me, it doesn’t always seem obvious whether what I’m doing is “correct” or most efficient. If I was learning in isolation, things I find difficult could well contribute to me giving up. Knowing I can go and get advice at the next lesson is a really positive and powerful resource to have. Having someone capable of competently observing and listening to you play, assessing that and pointing out things you’re doing wrong (even when you think you’ve mastered something) is also a boon. Likewise, this means you should be encouraged with reasonably good habits, e.g. posture and finger positioning, instead of getting by for now, and struggling later. A teacher can also tell you when you’ve got the most out of learning a piece. You could spend a long time playing something to near perfection, but is that practice time most usefully spent doing that? Importantly, it’s not just having that feedback from someone else, but one who has taught others and knows what’s reasonable to expect too. It also, again, takes this decision out of your hands. Removing some of the decision making of learning is another benefit. Instead of having to figure out what books to get and which pieces to try next — is this too difficult? too easy? — they can set tasks for you. Again, a teacher should know what works well for other learners, and it means you don’t have to concern yourself with that at all. That said, for me, lessons work best with a two-way involvement. As time is short, I’ve found a good strategy is going in with a clear idea of things I’ve been struggling with or don’t quite understand, to discuss and work on in the lesson. Disregarding all those benefits, could you teach yourself? It’s not impossible. There are musicians that have successfully done just that. But, it requires a lot of discipline and the persistence to battle through the tricky skills you’ll need to learn. The only downside I can think of taking lessons is their financial cost. ","date":"2017-06-11","objectID":"/posts/do-you-even-liszt-five-months-of-learning-piano/:2:0","tags":["learning","music","piano"],"title":"Do you even Liszt? Five months of learning piano","uri":"/posts/do-you-even-liszt-five-months-of-learning-piano/"},{"categories":null,"content":"Starter books The first book I was using was The Classic Piano Course Book 1 by Carol Barratt. Beginning from basics, it introduces new ideas throughout every couple of pages, moving briskly, without being overwhelming. Progress feels regular and positive. Certainly, except towards the last third or so of the book, with a little practice each day, it was possible to get most of the simple pieces to a decent standard in a week or so. Going back to tuition versus being self-taught, you could use it for self-teaching as it is mostly self-explanatory, although at times it’s a light on details. For instance, it provides an exercise on hand extending and contracting, which, in the absence of helpful pictures, I had to check with my teacher that I’d understood it correctly. From completing about three-quarters of that book, I’ve moved on to 2017 \u0026 2018 ABRSM Grade 1 assessment pieces. Not necessarily with a view to taking the exams, but to get to that standard. These are challenging, probably taking me maybe a couple of weeks to be able to play through from start to finish at all (i.e. stumbling through them with hands together), with another week or two to refine them. Even then, I’m aware of shortcomings that I could still improve on. Popular books, such as these, are a good choice. You’ll easily find competent performances on YouTube as a useful reference for what the music you’re playing should sound like. ","date":"2017-06-11","objectID":"/posts/do-you-even-liszt-five-months-of-learning-piano/:3:0","tags":["learning","music","piano"],"title":"Do you even Liszt? Five months of learning piano","uri":"/posts/do-you-even-liszt-five-months-of-learning-piano/"},{"categories":null,"content":"How is learning? I usually enjoy practice. It can be frustrating, particularly when you first start out on something and struggle, which can be demoralising. What you have to do is retain the faith that you’ll improve just like you have on previous pieces when you first encountered them. Once you’re a bit more comfortable that most new things you play could be the most difficult that you have tackled, the process starts to feel less daunting. Once you begin to get to grips with a piece, playing becomes a lot more fun to work on as it comes together, and you make fewer mistakes. Picking out the correct notes, adding the correct rhythm, introducing dynamics and accents, and pedalling all feel almost like separate layers of complexity. This can feel overwhelming at times, although focusing on tackling a subset of these and adding the others on later does help. Coordinating two hands is definitely a tricky aspect of piano. It takes considerable practice to be able to play two different things with your hands independently. That’s very different from guitar (where I have a little past experience),2 where left and right hand perform very different tasks, and it’s easier to isolate their movements. That said, as much as I find certain aspects of piano tricky, I found barre chords on guitar a real barrier (sorry) to cross.3 Piano feels difficult generally, but I’ve not yet encountered any single thing that is a comparable obstacle. On the whole, learning piano been an enjoyable and rewarding experience. To go from almost no experience to being able to stumble through Grade 1 pieces, which sound pleasant (if played well), and really feel like you’re playing piano, has been an interesting journey. I’m now feeling a bit more confident when I’m attempting to make music. Knowing that my hands are likely in the correct position, or being able to noodle around on the keyboard with a more decisive feeling of what I’m doing as being musical, are some of the early benefits I’ve gained from studying piano. Recalling little melodies or chord progressions also seems a simpler task, since I’m more familiar with keyboard layout. As a late starter, there’s no way I’m going to be ever as good as if I’d begun when much younger. I think it’s important to keep your possible achievements realistic, so as not to stretch too far and give up in frustration. Nonetheless, for what I really want to do, create music,4 the skills being developed are valuable, and, importantly, I’ve found learning piano enjoyable enough in itself to continue with it. Some take this to extremes too, treating a musical score as a canvas that must be filled and painting in an inhuman number of notes. ↩︎ One notable thing from learning guitar is that I only ever read treble clef, but not bass clef (although I did see it and struggle through with it a while back in a Coursera music theory course). Because of that, I’m much less comfortable reading the bass clef. The best way I can describe this feeling is: imagine the confusion if you were reading a book where a mischievous publisher had turned every second line upside down. ↩︎ If you’ve not picked up a guitar, barre chords are difficult because they require clamping some or all of the strings down at one fret position, usually with the index finger. Here, you need to strengthen and toughen up your finger to do this, as well as placing it correctly so the strings ring cleanly, instead of with an unsatisfying dull thunk. It can be tricky. ↩︎ It’s certainly worthwhile and enjoyable to play other people’s music well. But I think there’s something about the possibility of being able to create something as an individual. There’s no-one else that is you, or can ever be you. Therefore it is possible to create something that’s really your own and couldn’t be made by anyone else, even if your technical prowess is a hindrance. And sometimes that lack of competency, to a degree, can be a key part of what marks out what you create as distinct and ","date":"2017-06-11","objectID":"/posts/do-you-even-liszt-five-months-of-learning-piano/:4:0","tags":["learning","music","piano"],"title":"Do you even Liszt? Five months of learning piano","uri":"/posts/do-you-even-liszt-five-months-of-learning-piano/"},{"categories":null,"content":"Previously I looked at Ableton’s book of ideas for music producers that came out a while back, geared to people with some experience of making music already. That did well in summarising suggestions for creativity into short, easily digestible chapters. Ableton have now followed on by developing a free tutorial site that actually takes a step back. It’s instead aimed at complete beginners who might be interested in creating music which, in the spirit of Ronseal, Ableton have called it Learning Music. Unlike the previously published book, this resource is accessible for free. Is it worth your time to work through? ","date":"2017-05-21","objectID":"/posts/looking-at-abletons-learning-music/:0:0","tags":["Ableton","music","music theory"],"title":"Looking at Ableton's Learning Music","uri":"/posts/looking-at-abletons-learning-music/"},{"categories":null,"content":"How does the site work? You can always check it out yourself, but, briefly, each page is a mixture of a small amount of explanatory text with usually some interactive tool that mimics some aspect of a digital audio workstation (DAW) (particularly Ableton Live). Sometimes, there are music videos where some feature of that music is then broken down on the page. Where this DAW-lite interactivity works well is that it’s stripped down to the minimum framework of triggering samples or placing notes. Looking at a lot of modern audio software, it’s easy to be overwhelmed by options, even as a seasoned user. This paring down was a sensible decision so as not to deter the interested who might have little experience of music theory, or digital music making. (And no doubt made the site simpler to build too.) This functionality also provides an opportunity to practice and try altering the examples, if someone doesn’t yet have a digital audio workstation installed. These interactive tools often nicely assist the user, making it easy to make something sounding pleasing. This can be by, for example, restricting the permitted notes to ones within a scale, or, as on the first page, by carefully creating patterns that the user can trigger and overlay each other without any danger of the sounds clashing. Although the look and feel of the interactive sequencer widgets is obviously influenced by Ableton Live, and could segue nicely into trying out the Ableton Live demo, the ideas — much like the book — transcend any particular piece of software. Like the book, even if you’ve no interest in Ableton itself, and intend to use some other software (or hardware) for making music, that shouldn’t put you off. This tutorial does allow you to export sketches you make in the Learning Music web app to Ableton Live, should you wish — a nice touch — albeit one that’s geared towards steering users towards Live (which is fair enough). The other main feature of this tutorial is that context of the musical ideas is given by reference to contemporary music, often well-known electronic music. Some aspect of each is broken down. This can be quite revealing if you haven’t thought of music that way before: isolating one part (percussion, bassline, chords, melodies, structure) and briefly exploring what makes those aspects effective. It’s nice that’s it’s example led; it gives the reader something to quickly grab onto, and provides possible inspirations too. Active listening in this way was pushed throughout the earlier Making Music book. Perhaps the creators of the site’s material could have gone slightly further. Maybe deconstructing one simple track throughout the course of the tutorial would work well in addition to analysing different pieces of music (which admittedly does help avoid boredom). Referring back to a single piece of music and considering a different aspect each time might provide two things: a sense of familiarity as the learner progresses through the material, and a sense of achievement that they understand how most of the main aspects of a particular track are put together, once they’ve completed the tutorials. ","date":"2017-05-21","objectID":"/posts/looking-at-abletons-learning-music/:1:0","tags":["Ableton","music","music theory"],"title":"Looking at Ableton's Learning Music","uri":"/posts/looking-at-abletons-learning-music/"},{"categories":null,"content":"And does the site work? In terms of material, if you’ve ever covered some basic music theory or have reasonable proficiency on an instrument, then you’ll probably have encountered plenty of what is taught, and you won’t get that much out of it. However, there were still a few things I learned about, for example: the Indonesian Pelog scale; the opening notes of the melody from Kraftwerk’s Tour de France are apparently taken from Paul Hindemith’s Heiter Bewegt from Sonata for Flute and Piano. The tutorials, then, only skirt the surface of each topic. Read the wrong way, this might sound like a criticism. It’s not. In teaching aimed to give a survey of a topic, it’s actually a positive: crucially, it’s just enough to get the main point across to the target audience, make them aware of the ideas, and prompt the curious to go and learn more elsewhere, without being so detailed as to feel like hard work. So, for me, the site succeeds in its aims. If you’re a relative newcomer to music theory or composition, have got a spare hour or two and want to learn a little about creating music, or if you just want to see breakdowns of how parts of some pieces of music work — music that you may well have already heard and loved — it’s more than worth your time to work through the material Ableton are offering. ","date":"2017-05-21","objectID":"/posts/looking-at-abletons-learning-music/:2:0","tags":["Ableton","music","music theory"],"title":"Looking at Ableton's Learning Music","uri":"/posts/looking-at-abletons-learning-music/"},{"categories":null,"content":"What’s a GTFS feed? This week I’ve been attempting to reduce the size of a static GTFS feed. For the uninitiated, GTFS is the General Transit Feed Specification, a format for public transport providers to distribute details of their services. For example, a bus operator might want to share which routes they run on, and the times at which they arrive at certain stops. It’s not restricted to buses. You might find GTFS feeds published for trains, underground services, ferries and even gondolas. The “static” distinguishes those feeds from realtime1 feeds. Static feeds are just zip files that contain a number of text files in a comma-separated values (CSV) format, with a .txt filename extension. These CSVs detail regular timetables that should remain valid for some reasonable length of time, i.e. they change infrequently. Realtime feeds (GTFS-RT) report, for example, live trip updates (e.g. when a bus gets behind the published ideal schedule) or vehicle locations. Realtime feeds are in Google’s Protocol Buffers format, not CSV. It’s useful to be aware of the difference, but, beyond that, I won’t discuss GTFS-RT feeds further in this post. ","date":"2017-05-05","objectID":"/posts/trying-to-trim-down-static-gtfs-feeds/:1:0","tags":["GTFS","open data","transit","public transport"],"title":"Trying to trim down static GTFS feeds ","uri":"/posts/trying-to-trim-down-static-gtfs-feeds/"},{"categories":null,"content":"The problem Colleagues on a project were attempt to load the data we’d found for a particular country, and they were finding it far too large to be usable. GTFS files come zipped, and the total unzipped content of this file wasn’t far off a gigabyte, and the import into their software was taking too long. They were only interested in one geographic region, so maybe most content can be filtered out either by location, or by transport agency. Here’s a rundown of what I found with the existing tools. ","date":"2017-05-05","objectID":"/posts/trying-to-trim-down-static-gtfs-feeds/:2:0","tags":["GTFS","open data","transit","public transport"],"title":"Trying to trim down static GTFS feeds ","uri":"/posts/trying-to-trim-down-static-gtfs-feeds/"},{"categories":null,"content":"onebusaway GTFS transformer Oh, on seeing the onebusaway GTFS transformer, it seemed like a dream. It’s a Java program that lets you simply specify the agencies to retain from the feed in a simple JSON format, and it’s fairly comprehensive in how you can do that. So, hey, let’s write a simple transform JSON that keeps the bus and train agency for the region of interest, and drop the rest. Exactly what’s needed, and a job well done, so now I can relax, and take a well-earned rest. Oh, opens eyes to the smell of a burning PC, it’s time to wake up. The documentation does provide a warning that this transformation may be memory and CPU hungry with large GTFS feeds, and that’s what I found. Unfortunately, the transformer seemed to choke on this large GTFS file. It ran for a long time (hours, across multiple CPUs), got stuck spinning its wheels on the huge stop_times.txt file (millions of lines) and never managed to escape that mud. As well as that, out of memory errors were a problem. In the software’s defence, the free memory on the instance I was running on was fairly limited. If you’ve simpler GTFS feeds to work with, it may do the trick, but, for large ones, you’ll need a beefy PC to possibly get anywhere with it. That said, there’s little status output while it’s running, meaning it was unclear whether the transformer was ever doing any useful work or just getting stuck in some loop. ","date":"2017-05-05","objectID":"/posts/trying-to-trim-down-static-gtfs-feeds/:3:0","tags":["GTFS","open data","transit","public transport"],"title":"Trying to trim down static GTFS feeds ","uri":"/posts/trying-to-trim-down-static-gtfs-feeds/"},{"categories":null,"content":"gtfstidy Next, gtfstidy. It can reduce the size of static GTFS feeds by removing redundant content, simplifying identifiers and doing clever things to optimise shape data. It’s written in Go, although there are no builds provided, so you’ll have to have Go to install or compile it yourself, which is easy enough. Once again, it took a long time to process; here, about five hours on my 2013 Core i5 laptop. Then, gtfstidy crashed right at the end of processing the data because as I later discovered, it expected the output directory to exist and just failed to write the output otherwise. And then it took another few hours after I created said output directory to run again and actually get the cleaned feed data. In the end, gtfstidy reduced the total size of the GTFS files by about 150 MB, which is a considerable saving. My initial idea was maybe it might clean the data up enough to make running the onebusaway transformer a little bit quicker, but it didn’t seem worth bothering trying that again, as the data files overall were still large. !!! article-edit \"\"\" Edit 2017-12-29: The author of gtfstidy emailed me to say: \u003e gtfstidy actually comes with a very handy option which I use \u003e extensively for filtering. If you run gtfstidy with `-DO`, it will \u003e delete any entity which makes any problems during validation and \u003e also delete any entity which is not referenced anywhere. This \u003e cascades through the feed. This means that, if you want to only \u003e keep data for certain agencies, for example, you can just delete \u003e all agencies you don't want to keep from the agency.txt file. \u003e `gtfstidy -DO` will now remove any route that references one of the \u003e agencies you have deleted, and any trip that references one of \u003e these routes, and any stop_time that references one of these \u003e trips, and so on. If any orphan stops, or orphan shapes, or orphan \u003e fare rules etc remain after this, the `-O` option will delete them \u003e too. \u003e \u003e This works for any of the GTFS files. If you just want to keep trips \u003e for certain stops, delete every other stop from stops.txt. If you \u003e just want to keep data for some routes, delete all the other routes. \u003e After `gtfstidy -DO`, you will have a new error-free, filtered subset \u003e of the original feed. ","date":"2017-05-05","objectID":"/posts/trying-to-trim-down-static-gtfs-feeds/:4:0","tags":["GTFS","open data","transit","public transport"],"title":"Trying to trim down static GTFS feeds ","uri":"/posts/trying-to-trim-down-static-gtfs-feeds/"},{"categories":null,"content":"gtfssplit Finally, gtfssplit is a PHP tool that breaks apart a static GTFS feed into trips, shapes and stops. Unlike the other two projects, this one hasn’t been touched for a while.2 This took a fair time to run, although you can see it’s at least doing something some of the time, because, contrary to the relatively shy and subdued onebusaway transformer, it displays a lot of warnings while running. Once these files are all on disk, maybe it’s possible to write code to recombine these back together into a GTFS feed, filtering out for agencies that you’re not interested in? I’m not sure. What I observed is that the dates.txt files that the gtfssplit documentation refers to in the output (that should be for a given service, i.e. to tell you which dates the service applies) didn’t seem to be present, so it might be possible that it’s losing data somewhere. Maybe that’s because the feed I was looking at has a calendar_dates.txt and no calendar.txt file, and this case isn’t handled. (It’s valid, just not the recommended way of putting together a feed.) Again, I’m not sure, and didn’t spend any more time investigating. ","date":"2017-05-05","objectID":"/posts/trying-to-trim-down-static-gtfs-feeds/:5:0","tags":["GTFS","open data","transit","public transport"],"title":"Trying to trim down static GTFS feeds ","uri":"/posts/trying-to-trim-down-static-gtfs-feeds/"},{"categories":null,"content":"Journey’s end? And that’s as far I got to, which is a little anticlimactic. That is, I haven’t found a good existing solution for solving this for large feeds, outside of the prospect of attempting to write code to do this myself. However, the fact that all the existing software runs slowly, doesn’t exactly instil me with much hope of doing a better job. A big difference is that what’s actually needed here to solve the problem is something relatively simple by comparison. The core issue is that the stop_times.txt is too large. Maybe it’s possible to filter that down based on working through the various files, which is a much more specific kind of tool than the generic transformer. Perhaps this could work along the lines of: Keep rows from routes.txt that match the relatively few transport agency_ids of interest giving a set of route_ids. Keep rows from trips.txt that match those route_ids from 1 in trips.txt, so now these give trip_ids of interest. Keep rows in stop_times.txt if they match a trip_id retained from 2, meaning most of the rows disappear, assuming the agencies that are of interest make up only a small proportion of the total rows. You could go further and then fix up the other files too to purge anything that’s now no longer relevant: for example, cleaning agency.txt to remove agencies that are no longer present. But perhaps gtfstidy might be enough to tidy up the remaining files and allow us to construct a clean and valid feed from the files we’ve mangled. There are also ways of wrangling GTFS feeds into SQL databases — haven’t linked any as I haven’t tried any and can’t vouch for them, but they’re easy to find by searching “GTFS to SQL” — so importing and writing queries to do this filtering, then exporting the result as CSV might be an alternative option instead of coding something. This is all speculation for now; I haven’t attempted any of this yet. If I do and it’s useful, I’ll share what I learn. !!! article-edit \"\"\" Edit 2017-12-29: The approach I actually used to solve this problem is on GitHub albeit scrappy. It’s possible to specify a latitude and longitude range, which you can then filter out stops in stops.txt, and then remove any then-irrelevant content in the other files, and rebuild the feed. I seem to recall this generated a feed with no serious problems when verified by a GTFS feed validator; you could always try sending the output of this through something like gtfstidy, of course. What would be nice is if feed providers offered feeds in smaller regional chunks. Often, transport companies are providing GTFS feeds at an agency level, so these should be relatively small. I suspect that the sources of countrywide feeds are usually efforts of communities or companies to compile the data, which is sometimes originally in a format other than GTFS, into a standard feed. So it feels ungrateful to comment that it would be nicer if they broke these feeds into smaller chunks for data consumers to import into software without further work because they’re providing a free service and their efforts are appreciated. At the same time, those making efforts to share this data may find more users for their data if it’s ready for import into other software without needing fast PCs with lots of RAM, or without consumers having to find ways to divide up the provided feed. For instance, applications like OpenTripPlanner can, at time of writing, still struggle with large GTFS files. The single word “realtime” is how the specification writes it. ↩︎ This isn’t always the case, but looking at the activity of an open source project can be a portent of how useful it might be, if there are several competing bits of software that do the same thing and you’re trying to narrow down to a few candidates to try out. How popular a project is, e.g. the number of GitHub stars, might be one indicator, although all projects will start out at zero on this, so it could well be a good project you’ve stumbled across that’s just relatively ","date":"2017-05-05","objectID":"/posts/trying-to-trim-down-static-gtfs-feeds/:6:0","tags":["GTFS","open data","transit","public transport"],"title":"Trying to trim down static GTFS feeds ","uri":"/posts/trying-to-trim-down-static-gtfs-feeds/"},{"categories":null,"content":" LaTeX, and how you might do that. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:0:0","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"ShareLaTeX Today I saw this documentation from ShareLaTeX linked on Hacker News and, as I haven’t used LaTeX in a while, thought I’d quickly skim through to see what I remembered. And I started reminiscing about LaTeX which I’d been using semi-regularly around a decade ago, eventually writing a circa 300 page thesis with it. If you’ve never encountered it before, LaTeX is a free software project that is described by the project’s site as: a high-quality typesetting system; it includes features designed for the production of technical and scientific documentation. There’s nothing to stop you using it for other types of documents too; those are just the areas where it’s traditionally used, particularly in academic circles. Back to ShareLaTeX. First, the ShareLaTeX documentation looks pretty good if you need something to get you started. I haven’t used ShareLaTeX recently although I think I tried it out a few years back and it seemed fine enough. It’s a web app that lets you prepare LaTeX documents and, if you pay, collaborate on them. One big advantage I can see of using a web app for LaTeX beginners is that it removes one of the barriers of starting with LaTeX: installing everything. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:1:0","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"Installing LaTeX on Windows I’ve not had cause to use LaTeX much recently, so haven’t tried installing on Ubuntu; a lot of my writing is done via Markdown, whether technical documentation or words like the ones you’re reading now on my blog. On Windows, it used to be reasonably simple to get a working system with TeXnicCenter and MikTeX, but you’d still have to get a PDF viewer, like SumatraPDF integrated into that for previewing. This Stack Exchange answer may help with integrating the PDF preview. Though that’s still three different bits of software you have to get working together, so not a trivial task by any means. From memory, install MikTeX first, as TeXnicCenter can detect it during install and configure its own settings to use your MikTeX installation without you needing to set things up by hand. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:2:0","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"Starting out The ShareLaTeX documentation just starts out with creating a basic document and has you working through following their example. That’s fine to see things piece by piece. Beyond that the single suggestion I’d have for a new user to learn effectively is to take a completed document you have written, representative of what you’d like to create using LaTeX, and then struggle through recreating that existing document in LaTeX. And it may well be a struggle, but this has a few benefits: It’s presumably a finished work, so you don’t have any time pressure for completion. With a deadline to meet, as soon as you encounter issues with LaTeX, you’ll likely fall back to whatever word processor or document creator you’re comfortable with to get the job done, and then give up on learning LaTeX. Using a document similar to that you’d usually be writing, you’ll encounter many of the issues in LaTeX that you’d hit doing so, and can solve these without the aforementioned time pressure. In my experience, it was graphics and, especially, table layouts that caused the most problems, but you may have particular typography requirements that require some package to be installed, or some subtle override of the LaTeX defaults to change. Finding these solutions is not always quick, but I imagine there are more forum posts, Stack Exchange questions and blog posts around than there were when I started, so it’s likely someone’s encountered and asked about the same issue you’re dealing with. The LaTeX Community forum was also a good source of helpful solutions from knowledgable people even in its early days ten years ago, and still lives on today. A completed document gives you a reference for getting started in future. You can use it as the basis for a new document, instead of starting from nothing, or you can just refer back to it and copy and paste appropriate bits of formatting markup you need. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:3:0","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"LaTeX versus Word ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:4:0","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"LaTeX installation As I saw it when learning back in 2007ish, LaTeX was more difficult to get started with than the likes of conventional word processors: the installation isn’t trivial, and you need several things other than your desired text in a LaTeX document before you can get that text to display. In a word processor, you can just start typing. So there’s some level of competence you need just to make any old document, let alone the one you actually want to make. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:4:1","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"Word woes Word is easy to get started with, but Word had, and maybe still has, its own problems: again, going back to 2007, to when Word 2003 was still popular, referencing support was limited, that you’d use external tools for (e.g. EndNote or Zotero). I’m not sure how Word looks with respect to that now. Back then, Word seemed to struggle and be sluggish with large documents with lots of images, which was always fun. Again, maybe that’s improved in recent versions? When editing LaTeX, you’re only editing the source markup in whatever text editor you choose to use, and this source, even for a lengthy document, will be relatively small; images for LaTeX are stored entirely separately from the source. Another issue with Word is ensuring formatting styles are correctly and consistently applied to text. You can create styles, but you then have to apply them by hand, selecting the text. Sometimes you may delete spacing between words and suddenly you have to reapply a styling, as the text has changed to the style of its neighbours. In LaTeX, anything to do with formatting is specified explicitly in the markup, making it easy to see that the correct styling is applied to the text. Furthermore, the LaTeX defaults are usually decent, not always perfect, but decent. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:4:2","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"The good and bad of LaTeX This explicitness and strictness of LaTeX is good and bad. It means that you know what is intended for your text, but it also means that it’s possible for you to break your document sufficiently such that it cannot be rendered correctly, or sometimes even at all, by LaTeX. If you’ve done programming and are used to finding and correcting your inevitable mistakes, this isn’t unusual. But the idea of having written a document that you can’t then display in the intended output format may be an intimidating concept to new users. Likewise, the error messages can be numerous and formal, and not much help to novice users. Tricks known to programmers like a regular edit-compile cycle to ensure you find issues as you create them, or commenting out markup to isolate the specific parts of markup that are the cause of difficult to understand problems are useful skills. (These ways of working are perhaps a result of the creators of LaTeX — and TeX, which LaTeX is built on — being computer scientists.) However, these concepts may not be immediately obvious by users with little experience of coding who just want to get on with writing, and don’t want tools getting in the way. On the other hand, working just with the LaTeX source and focusing on the text alone seems much more productive to me. You can concentrate on content first, and fix up the presentation at the end. Otherwise, I find that working alongside the printed document layout presents me with the continuous distraction of making things look “right”, fixing incorrect formatting or alignments, even though the content in question could be modified or removed entirely during later editing. This time might be better spent just writing. Talking of layout, in my opinion, a LaTeX document using defaults will be much more pleasant to read than a Word document using defaults. To some extent, in both cases, this can somewhat depend on templates that you may choose to create your document, but a default LaTeX document has more of the look of a professionally typeset book. Finally, many useful additions to LaTeX come in the form of a huge range of additional packages you can install to add new commands and features. Again, this is a mixed blessing. First, you need to be aware that they even exist — lists like this can help: there may be a package perfect for what you’re trying to do, but you just haven’t discovered it yet. Another hassle is that you can have incompatibility issues between certain packages, or they may require to be loaded in a certain order. Fixing problems like these is time, again, spent on tools, not on writing. You do need to be careful of package updates. Mostly these should be benign, but in one case I experienced, a package update suddenly broke my document, and I had to spend time discovering the problem and reverting the package version. (My memory’s hazy, but I think this was complicated by MikTeX only having the latest package versions, so in the end I think that I had to download from elsewhere and install by hand.) This may be an issue for compiling documents if you return to them much later in the future: if any of the packages have broken their backwards compatibility, you may be stuck. There’s also no easy way that I was aware of to pin package versions, like you might in a Python setup.py file. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:4:3","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":"Do you still need to learn LaTeX these days? Since I learned LaTeX, software like pandoc has appeared and made it simple to write in other formats — for instance, Markdown, reStructuredText, or HTML — and convert that to LaTeX. Markdown in particular is very simple, and easy to learn. For relatively straightforward documents, that’s an alternative to give you the benefit of nicely typeset output, without having to bother writing LaTeX directly. This can give you a handy subset of LaTeX in potentially a simpler markup, especially with Markdown. However, LaTeX remains flexible and powerful and, especially for creating more complicated documents, it still retains popularity more than thirty years after its first development. Furthermore, it’s also released under an open source license (the LaTeX Project Public License) so these professional-level tools are available to everyone at no cost. If you have the likes of long reports or a thesis to write, it’s certainly still worth a look. ","date":"2017-05-01","objectID":"/posts/learning-latex-why-and-how/:5:0","tags":["LaTeX","document preparation","word processor"],"title":"Learning LaTeX: why and how ","uri":"/posts/learning-latex-why-and-how/"},{"categories":null,"content":" budget, particularly for digital DJs. A couple of months back I wrote something on trying to find inexpensive music for DJ sets for DJ TechTools which just got posted there. There’s another two tips here that I didn’t think to include there. ","date":"2017-04-04","objectID":"/posts/building-a-music-collection-on-a-budget/:0:0","tags":["DJ","MP3","music"],"title":"Building a music collection on a budget","uri":"/posts/building-a-music-collection-on-a-budget/"},{"categories":null,"content":"Amazon Prime’s No-Rush delivery At least in the UK, if you have an Amazon Prime account, and you buy things shipped from Amazon, currently you’re probably eligible to opt for a slower delivery with the benefit of a £1 digital credit applied to your account. This credit can be used for music purchases, and as individual MP3s on Amazon are often just 99p or less, that usually means you can pick up a track for free in addition to whatever you actually were ordering in the first place. It’s possible to further benefit from this too. When you’re planning an order of multiple eligible items, breaking that one order up into orders of individual items allows you to multiply the credit by the number of items. Make sure you do use the credit though since they expire after some time. It’s maybe worth keeping a want list to ensure you know exactly what to spend your next credit on. ","date":"2017-04-04","objectID":"/posts/building-a-music-collection-on-a-budget/:1:0","tags":["DJ","MP3","music"],"title":"Building a music collection on a budget","uri":"/posts/building-a-music-collection-on-a-budget/"},{"categories":null,"content":"Compilations can be competitively priced With compilations, you’ll often find that, versus buying individual tracks, you’ll get more value for money, and can be a quick way to build up a collection, especially if you’re looking for well-known hits. If there are, say, five tracks that you’re after on a compilation, and you get another fifty or so bundled in for a few pounds more than buying the five individual digital tracks, that’s not a bad deal. Sometimes you get the case where the CD version of the compilation contains mixes, whereas the digital version may have the mixes (which you probably don’t care about too much; there are more mixes around than I’d ever have time to listen to) and the individual tracks (which you do want). I don’t want to sound like I’m just advertising Amazon here, as it may be that other digital music stores offer the same kind of deals, although I’ve not investigated them. But, Amazon can be particularly competitive, again when you “arbitrage” buying MP3s via lower priced CDs which feature AutoRip bundled MP3s, as I mentioned in the original DJ TechTools post. As an example: at the time of writing, this compilation is £5 for the CD which has MP3s provided via AutoRip, versus £9.99 for the MP3 version. Seeing Roy Davis Jr’s Gabriel on there, you might think that £5 for just that track is worth paying, but there are another 65 tracks on there too.1 Surprisingly, there’s very little filler on there, and while there are a few radio edits on there, most tracks are the full length versions. The bonus here is that you also get instant digital delivery on the bundled MP3s when buying the CD version too. The only differences are that you’ve paid less and get a redundant bit of plastic through the post a few days later. Since it was such ridiculously good value, I just impulse bought that compilation. ↩︎ ","date":"2017-04-04","objectID":"/posts/building-a-music-collection-on-a-budget/:2:0","tags":["DJ","MP3","music"],"title":"Building a music collection on a budget","uri":"/posts/building-a-music-collection-on-a-budget/"},{"categories":null,"content":"A journey through history Typography is ubiquitous and present in our daily lives, often without even noticing it. That’s a quote from Type:Rider, what the developers call a “game documentary” and what I’d call a slightly frustrating, albeit compelling museum-style whistle stop tour through fonts and typography across human civilisation. But the quote is spot on. Typography is everywhere, and though that’s difficult to imagine for us today — just take a look around — it wasn’t always like that. Type:Rider goes on a journey from cave paintings, through to early writing systems, such as cuneiform and hieroglyphics, then onwards to the spread of printing across late medieval Europe, and the continued development of fonts, typography, all the way to the current age where words being marked on media has made way for being displayed on screen instead. So then, this is as much a study of the spread of information. Not only is it a journey through history for the player, but for the two dots comprising a colon that the player controls. They roll along the level and can, somewhat inexplicably, jump and wall jump. ","date":"2017-03-29","objectID":"/posts/typerider-a-review/:1:0","tags":["game","review"],"title":"Type:Rider: A Re:view ","uri":"/posts/typerider-a-review/"},{"categories":null,"content":"Type:Rider is no NightSky Why two dots when, say, a full stop could have sufficed? Perhaps that’s to avoid accusations of being a clone of Nifflas' relaxing NightSky. If anything, the extra dot is a hindrance; it makes the clumsy platforming even clumsier, and doesn’t really add anything to the gameplay. Extra dot aside, it is reminiscent of NightSky (itself a game you should check out) and I think the difficulty and the slightly sparse, often monochrome foreground level layouts are why comparisons are drawn to Playdead’s Limbo. That, as well as the difficulty. The difficulty is another reason that Type:Rider seems to get compared to Trials, but this is a more loose association only. The Trials games are finely balanced and tuned, so as to allow for mastery of very delicate, precise controls. Type:Rider, unfortunately, is not. Quite often you’ll end up bouncing off uselessly from walls, or find your pitiful leaps are insufficient to jump the chasms that you need to. OK. It’s not actually that glitchy — it’s definitely no Sonic the Hedgehog 2006 — but the floatiness of how the dots “jump” and move, mean that there are some sections that are frustrating where you’ll repeatedly bounce to your doom. With the addition of a couple of cheap instant death sections which seem like they were thrown in as attempts to add tension, it would have been just nicer to have a relaxing, easy to progress through, game. On the plus side, restarts are quick and the checkpoints frequent, so it’s rare that you find yourself repeating familiar ground. It does still break the flow of what would be an otherwise relaxing experience, but this at least tempers player frustration. ","date":"2017-03-29","objectID":"/posts/typerider-a-review/:2:0","tags":["game","review"],"title":"Type:Rider: A Re:view ","uri":"/posts/typerider-a-review/"},{"categories":null,"content":"But Type:Rider’s still worth a look That’s all a pity, because the actual atmosphere, that of something like a very immersive museum exhibit is a pleasant one. The graphic designs of the levels, and the integration of fonts into them, are well thought out, and thematically cohesive in the ideas they’re trying to portray and show imagination. The historical background is provided via brief, but informative, texts that are collected throughout the levels. There are some peculiar issues, likely due to a quickly thrown together English localisation. There is one notably particularly bad translation that recurs throughout: the term wheelbase peculiarly gets mentioned in the text several times. The Steam discussion forum for the game actually answered this: it’s a badly proofread translation: it should be serif. Likewise, there are some weird text kerning issues in the history text too, and it’s likely that these too don’t occur in the French original. But that’s an oversight considering that the game is dealing with typography. The scope of the game means that it is limited to telling a few stories, but as an introduction to some of the big names in history and the technological advances made, it does a good job. It is interesting just how important the onset of widespread dissemination of information has been, and the impact that has had since. We take the immediacy of that for granted now — you could well be reading this text that I wrote seconds after I published it on this site, wherever you are in the world — but the distribution of printed works on any useful scale wasn’t even a concept until the late medieval period, just a few hundred years ago. It’s difficult to comprehend that when information is so central to society now. It’s also interesting where Type:Rider leaves off, which is the development of markup languages, and computer bitmap and vector fonts. A really big omission, perhaps because it was made a few years ago, back in 2013, is emoji. (However, emoji were big enough in 2013 to make it into the Oxford English Dictionary.) Electronic communication saw the incorporation of emoticons, constructed from text to hint at tone where plain text may be ambiguous. That’s been followed by the widespread use of emoji, where characters visually represent concepts or objects, and so that nicely brings us full circle: modern day communication harking back to how writing systems first began. If the idea of spending a couple of hours learning about typography is something that sounds appealing, Type:Rider’s worth spending the time with. Just take the gameplay for what it is, which is occasionally relaxing, sometimes frustrating, be prepared to work through the trickier sections. It’s certainly hard to dislike a work that even manages to make a valiant attempt of painting Comic Sans MS in a somewhat sympathetic light. ","date":"2017-03-29","objectID":"/posts/typerider-a-review/:3:0","tags":["game","review"],"title":"Type:Rider: A Re:view ","uri":"/posts/typerider-a-review/"},{"categories":null,"content":" card payments? Today I got a nice letter in the post informing me that my vehicle tax was due. Wonderful news. Maybe not. Still, off I dutifully went to the appropriate GOV.UK site to pay the tax: it’s not like it’s an option if you want to keep your car on the road legally. If you’re not in the UK, or you are but haven’t taxed a vehicle recently, it’s quite simple. They send you a paper reminder. This is also a form that you can take into a Post Office and pay there. Paying this online, you’re asked for a reference number from the reminder. There’s a check made that your car has a valid MOT certificate (which establishes your vehicle’s roadworthiness) and then asks you for payment. Because I’m using uMatrix in my browser, I can spot which sites other than the one I’m directly visiting have content which is supposed to be loaded into the current page. The payment form didn’t initially load and I noticed a blocked site: “ephapay.net”. OK, I’ve never heard of them: let’s check them out and make sure they’re someone I can trust. I didn’t anticipate that the UK government’s website would be using someone dodgy, but best to check anyway. A few searches via web search engines didn’t reveal anything about this domain. Even weirder, the WHOIS information is all hidden by a privacy service: Admin Name: PERFECT PRIVACY, LLC Surely a legitimate payment processor wouldn’t hide their details? What did reveal who was operating the site was right at the end of the WHOIS information, in the name server details: Name Server: NS1.THE-LOGIC-GROUP.COM and turns out to be benign. They’re a payment processor that got acquired by Barclaycard in 2014. And I’ve heard of Barclaycard1. And the WHOIS information for the-logic-group.com actually points to their business address. So, it seems that at least they’re a established company (since 1988, a Companies House search will tell you). But I had to dig through to establish all that. It would have been much easier if the GOV.UK vehicle tax site just stated clearly who the payment processor is. I almost considered just reverting to paying the tax in person when I first encountered the mysterious site. What the UK government has going for them is that road users who want to stay legal must pay this, so one way or another, online or offline, they’ll get your cash. But there are other sites that have similar behaviour and don’t have a captive audience. There have been several cases where I’m interested in paying for a product or service, and hit some anonymous looking page or iframe pointing somewhere else, where it’s down to my browser showing details of blocked requests and content that reveal who’s handling payment (yes, of course, you could also search in the source too). In my case, common culprits of having vague payment pages are sites that handle ticket payments for small shows, or sites that let you book a place at running events. I’m sure there are more. And it really does deter me: if I’m not sure if my payment details are safe, I’d rather not bother, especially if it’s something I can live without. If you’re designing a website, bear this in mind: for the sake of placing a small, clear explanation of how payment works, and why the user should trust you and your payment processor, you can remove another stumbling block that you’re putting in the way of getting more paranoid or technically savvy users to give you money. If nothing else, you have them to thank/blame for inspiring Rowan Atkinson’s Johnny English. ↩︎ ","date":"2017-03-06","objectID":"/posts/if-you-want-my-money-please-state-your-payment-processor-clearly/:0:0","tags":["security","UI","UX"],"title":"If you want my money, please state your payment processor clearly","uri":"/posts/if-you-want-my-money-please-state-your-payment-processor-clearly/"},{"categories":null,"content":"Today I was using my laptop on battery power and I remembered that I hadn’t disabled wifi power management since I reinstalled Ubuntu in late 2016. With power management enabled for wifi, I find I get laggy internet on battery power, and pings are noticeably higher. If you suspect this is affecting you, it’s easy to check: $ iwconfig 2\u003e /dev/null | grep \"Power Management\" Power Management:on Unfortunately, the method I wrote about previously to disable wifi power management no longer works. My guess is that this is due to Ubuntu switching to systemd in 2015. Instead, you need to use an alternative approach as suggested in this post. First, look at your existing configuration: $ cd /etc/NetworkManager/conf.d/ $ ls default-wifi-powersave-on.conf $ sudo cp default-wifi-powersave-on.conf wifi-powersave-off.conf The new .conf filename must follow any other files alphabetically, in this case the filename must be one that’s alphabetically later than default-wifi-powersave-on, so that it gets applied last, and your modified setting overrides the default. If we named this file, e.g. a-wifi-powersave-off-config.conf, our new settings wouldn’t apply since the filename would then be ahead of default-wifi-powersave-on.conf alphabetically. Now, edit the new file: $ sudoedit wifi-powersave-off.conf Change what is likely a 3 in the wifi.powersave line to 2. This disables wireless power management. It likely will look like this when you’ve edited it: [connection] wifi.powersave = 2 Now, restart NetworkManager and you should see that this change is now in effect. It should also persist across reboots. You can confirm this again with iwconfig: $ sudo systemctl restart NetworkManager $ iwconfig 2\u003e /dev/null | grep \"Power Management\" Power Management:off ","date":"2017-03-03","objectID":"/posts/fixing-horrible-wireless-network-pings-in-ubuntu-2017-edition/:0:0","tags":["high","ping","latency","wireless","wifi","Ubuntu"],"title":"Fixing horrible wireless network pings in Ubuntu (2017 edition)","uri":"/posts/fixing-horrible-wireless-network-pings-in-ubuntu-2017-edition/"},{"categories":null,"content":" still active. Last month, I’d been reviewing some of the shared accounts for online services we have at work. Often, users have an individual login for online services. Some services don’t offer that feature and there’s a single account that’s shared by everyone. While digging around, I found details for a shared Bitbucket account. If everyone has their own Bitbucket account, then what’s this one? Logging in worked, and allowed me access to an account for the team itself; there was no user, as such. Not only could this team account access the team, but with full permissions. Furthermore, this account had no settings of its own. If you accessed the settings page, you got the team’s settings, not that of this team account, so it wasn’t possible to change any security settings for the account. With one of these “legacy” accounts lying around, it’s possible that it is sat there with only password authentication — Bitbucket introduced two-factor authentication late, in 2015 — an unchangeable (and maybe weak, depending on your policies) password, and with full team access.1 What makes this particularly insidious is that if you’re not aware of this account, you’ll be blissfully unaware if you look at the list of users on your team. Even if you manage the team as an administrator, this team account didn’t appear as an admin user. I don’t even think it counts as a billed user. Someone who gains access presumably could maintain that silently without you being able to do anything about it, at least if they just sat there cloning your code. Mitigating that is the audit log which would be a giveaway if anything noisy happened. By that point though, someone malicious could still cause a lot of problems for you by, let’s say, booting out all other team members, or deleting all your repositories. In the end, I had to contact support and request them to remove this account, since there was no way I could fix this as a team administrator. Therefore, this is definitely something worth auditing if you think you might have such dormant accounts, even though, according to Atlassian, this shouldn’t be the case at all. They earlier stated: Starting February 18, 2014, Bitbucket will remove the ability for individuals to log into a team with a username/email and password. Insert your own Unicode shrug character here. Creating new repositories failed, but, for instance, creating a repo using my account (as a team administrator), then logging in as the “team account” and using the team account to delete the test repo was successful. I didn’t test removing team members, but don’t see why this would have failed. ↩︎ ","date":"2017-02-04","objectID":"/posts/the-dangers-of-long-forgotten-bitbucket-team-accounts/:0:0","tags":["Bitbucket","security"],"title":"The dangers of long forgotten Bitbucket team accounts","uri":"/posts/the-dangers-of-long-forgotten-bitbucket-team-accounts/"},{"categories":null,"content":" decrypt and re-encrypt a drive. ","date":"2017-02-04","objectID":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/:0:0","tags":["BitLocker","manage-bde"],"title":"Switching BitLocker protection methods without re-encrypting","uri":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/"},{"categories":null,"content":"Overzealous TPM protection I’d set up BitLocker for someone using the Trusted Platform Module (TPM) in their laptop with a PIN1 to decrypt the drive. Unfortunately, they found that, after some time, the system tended to lock the PIN out, unless they used a recovery key to bypass the TPM and PIN access altogether. As far as I can tell, this is some feature of the TPM in this particular laptop where too many incorrectly entered passwords results in the TPM locking out for some lengthy and possibly indefinite amount of time. Perhaps even permanently. I don’t think this is the case where incorrect passwords are continually entered in a short time period, but where incorrect attempts over a longer period are cumulatively logged. In this case, this state doesn’t seem to get reset even if you subsequently re-enter the correct password, or unlock with another method. (It seems reasonable that the TPM’s unaware of whether BitLocker’s been unlocked or not by other means.) ","date":"2017-02-04","objectID":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/:1:0","tags":["BitLocker","manage-bde"],"title":"Switching BitLocker protection methods without re-encrypting","uri":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/"},{"categories":null,"content":"Fixing the TPM You can rectify this by resetting the TPM lockout but this is only a temporary fix. Again, after some number of password failures the lockout may happen again. My conclusion in this case is that, although disabling the TPM makes the system slightly less secure, the greatest threat here is not an unauthorised user accessing the data, but an authorised user being unable to access the data. Switching to just a password unlock then seems more sensible. I wouldn’t recommend this downgrade in security otherwise. At first look, you might think that this is a chore to switch. There’s no obvious way of doing this in the BitLocker options for the drive, or under Control Panel, and your instinct might be to decrypt the drive and encrypt again. That can be time consuming, especially with large drives. ","date":"2017-02-04","objectID":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/:2:0","tags":["BitLocker","manage-bde"],"title":"Switching BitLocker protection methods without re-encrypting","uri":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/"},{"categories":null,"content":"Using manage-bde to change key protection methods Instead, you can run the command line utility: manage-bde. Since TPM plus PIN, or recovery key (or some other method of securing your BitLocker encryption key) are key protection methods, Microsoft terms them “protectors”. Also, here we are looking at removing a TPM and PIN protector, but you can use manage-bde to handle any BitLocker protector. Specifically, you want to remove the existing TPM and PIN protector: manage-bde -protectors -delete \u003cDrive\u003e -tpmandpin You have to do this first, as it’s not possible to have both TPM and PIN protector, and a password protector. (A caution that -delete without specifying -type removes all protectors and then will disable protection, so that you can still access your drive in future. If you do that, you’ll need to add new protectors, as below, and you’ll need to add new recovery protectors, should you wish. You can do by specifying -recoverypassword for a numerical recovery code, or -recoverykey for an external key in the -add command below. And then you must enable the protectors again with manage-bde -protectors -enable, including new recovery keys should you wish to have them.) Next, add a new protector, e.g. a password: manage-bde -protectors -add \u003cDrive\u003e -password (C: is the most likely \u003cDrive\u003e value here.) You’ll be prompted to enter, and then confirm, the new password. Now, you’ll have a password protector, which won’t be subject to a TPM lockout, as we wanted. Finally, you can also check BitLocker status: manage-bde -protectors -status to confirm that the drive is encrypted, and which key protectors are active. Well, password. You can configure Group Policy to allow passwords with TPM, instead of just numerical PINs. Microsoft’s documentation always refers to this option as “TPM and PIN” regardless. This doesn’t change the BitLocker prompt: it still asks for a PIN. ↩︎ ","date":"2017-02-04","objectID":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/:3:0","tags":["BitLocker","manage-bde"],"title":"Switching BitLocker protection methods without re-encrypting","uri":"/posts/switching-bitlocker-protection-methods-without-re-encrypting/"},{"categories":null,"content":" being opened in a browser. I’d got some unwanted spam message and wondered if it was possible to stop accidental clicking on any URLs that are displayed in those cases. Turns out you can easily. Go to the Edit menu \u003e Preferences \u003e Advanced \u003e Config Editor. Skip past the warning about changing advanced settings if it’s displayed (you must have already disabled it previously, if you don’t see this). Search for network.protocol-handler.external-default, right click the option of that name that appears under Preference Name and select Toggle to change it from the default (true) to false. Now, if you try to click on a URL now that’s displayed in an email, nothing will happen; your web browser won’t open. However, you can always copy the URL and paste into a browser yourself. ","date":"2017-01-29","objectID":"/posts/blocking-urls-in-emails-from-being-opened-in-thunderbird/:0:0","tags":["Thunderbird"],"title":"Blocking URLs in emails from being opened in Thunderbird ","uri":"/posts/blocking-urls-in-emails-from-being-opened-in-thunderbird/"},{"categories":null,"content":"Because I want to improve my skills in producing music, I’ve started taking piano lessons. To start out with, I’m using a borrowed 61 key, touch sensitive keyboard. I’d read around about this. Some people suggest you should always learn on a piano, but my teacher suggested that’s not necessary when first starting out, at least until reaching ABRSM Grade 1 level. This has the benefit of not having to invest a substantial amount of money into a digital piano, should learning prove too difficult. The first thing you realise when practising is that it’s unlikely that anyone else within hearing distance is going to appreciate the false starts and wrong notes of a beginner playing piano. This is a big plus of keyboards or digital pianos: a headphone socket means you can practise without irritating anyone nearby. So, duly plugging headphones in, I realised there was a substantial hum (possibly ground hum) from the keyboard’s output, even at zero volume, and white noise which appears as the volume is cranked up. The same happens in the speaker output, but is much less apparent and distracting as it tends to get masked by ambient background noise. I’m usually quite fussy about unwanted noise, but I found this particularly distracting when trying to concentrate. I would also say that you do tend to get used to it over time and can maybe forget it’s there a little, but ideally I was looking to reduce or eliminate it. ","date":"2017-01-21","objectID":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/:0:0","tags":["keyboard","music","noise"],"title":"The buzz (and hum) of learning a new instrument","uri":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/"},{"categories":null,"content":"Quietening things down ","date":"2017-01-21","objectID":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/:1:0","tags":["keyboard","music","noise"],"title":"The buzz (and hum) of learning a new instrument","uri":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/"},{"categories":null,"content":"Solutions I considered but didn’t try Try another power supply. It’s possible that this might not fix it, if the power supply isn’t the source, so I didn’t actually buy another to test this theory. (In this case, the keyboard can be powered by a substantial number of batteries, which would be an ideal test to diagnose whether the power supply is at fault here.) Route the keyboard to a PC via MIDI to USB, play the keyboard via a digital audio workstation, and use a computer audio output instead of the keyboard’s own output. This would work if you have PC audio that is noise-free. But, it means you then have to rely on having a PC around and powered on. Instead, I’d prefer to use an instrument directly; this way it actually keeps me away from a computer for once. If you’re on a Windows PC, also bear in mind that you’ll likely need decent ASIO drivers for your audio hardware, otherwise you might have to deal with considerable input latency. ","date":"2017-01-21","objectID":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/:1:1","tags":["keyboard","music","noise"],"title":"The buzz (and hum) of learning a new instrument","uri":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/"},{"categories":null,"content":"Solutions I used Use worse headphones. The budget Monoprice headphones I have did seem to reduce the noise a little compared with the Sennheiser HD25 I’d first tested out. Add a ground loop isolator into the headphone socket on the keyboard and plug headphones into that. This did the trick for me. Since it’s filtering the signal, it does also attenuate the volume, but that’s a small price to pay for quiet. The going rate seems to be under £5 for cheap and cheerful generic kit, you can spend more on ones from the likes of Behringer, while some people even make their own DIY solutions. ","date":"2017-01-21","objectID":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/:1:2","tags":["keyboard","music","noise"],"title":"The buzz (and hum) of learning a new instrument","uri":"/posts/the-buzz-and-hum-of-learning-a-new-instrument/"},{"categories":null,"content":" what you need for Python development cleanly. When I reinstalled my PC, I was thinking about the “cleanest” way that I could get the Python tools I needed on there. Previously, I’d used the pip bundled in the Ubuntu repositories, then found it was left to languish, and was hideously out of date. (I think it is possible to update if you’ve installed that way, but involves using sudo pip install -U all the time, which didn’t seem sensible.) A better option might be to install pip directly from the installer that’s provided by the pip developers, using pip to install virtualenv and virtualenvwrapper, and then using the virtualenv tools to create virtualenvs to actually install Python packages for development. This way should create the minimum level of clutter you’re adding to your system’s Python installation. But, reading around, the suggestions here on doing a clean installation seemed an even better idea. You download the virtualenv package from PyPI yourself, use Python to run the virtualenv Python code directly and use that to create a “bootstrap” virtualenv that you can then create new virtualenvs from. Note that these virtualenvs all include pip; no need for extra work to get it installed. All of this is possible without doing any installation into the system’s Python at all, not even requiring any pip install --user commands. It’s pretty simple to do. If you want to follow this along further and add virtualenvwrapper too, see the Stack Overflow post for more details (essentially, you need to get a copy of virtualenv via downloading it, e.g. from PyPI, using curl or a web browser and then run python virtualenv.py $YOUR_CHOSEN_VIRTUALENV_DIRECTORY), then continue here. ","date":"2016-12-30","objectID":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/:0:0","tags":["Linux","Ubuntu","Python","pip","virtualenv","virtualenvwrapper"],"title":"Installing pip, virtualenv and virtualenvwrapper tidily in Linux","uri":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/"},{"categories":null,"content":"Getting virtualenvwrapper working Though it’s not recommended in the documentation, someone in the Stack Overflow comments said they’d got it working, so I thought worth trying. What I did was create a Python virtualenv purely for virtualenvwrapper using the method in the Stack Overflow answer. I called the virtualenv “virtualenvwrapper” and created it inside $HOME/.virtualenvs. Next, I then activated this new virtualenv using source $HOME/.virtualenvs/virtualenvwrapper/bin/activate and did pip install virtualenvwrapper. If you modify your shell startup files as suggested in the virtualenvwrapper documentation, you’ll find you get an error: virtualenvwrapper.sh: There was a problem running the initialization hooks. If Python could not import the module virtualenvwrapper.hook_loader, check that virtualenvwrapper has been installed for VIRTUALENVWRAPPER_PYTHON=$VIRTUALENVWRAPPER_PYTHON and that PATH is set properly. As virtualenvwrapper isn’t installed in the default Python, the script looks for it and understandably can’t find it. It’s a simple fix: just activate the virtualenvwrapper virtualenv before this shell script is sourced. Even if you immediately deactivate the virtualenvwrapper virtualenv again, you’ll still have commands like workon that function correctly in your shell environment. So what I first tried was doing what the virtualenvwrapper documentation suggests, and modifying shell startup files, but with extra activate and deactivate steps: export WORKON_HOME=$HOME/.virtualenvs export PROJECT_HOME=$HOME/.local/src source $HOME/.virtualenvs/virtualenvwrapper/bin/activate source $HOME/.virtualenvs/virtualenvwrapper/bin/virtualenvwrapper.sh deactivate You will need to switch to some virtualenv first with the virtualenv package installed, before commands that require the Python virtualenv package to be installed will work (e.g. to create new virtualenvs). It’s simple enough to do workon py3venv and then e.g. mkvirtualenv virtualenv_name when you want to create one. However, workon to switch virtualenvs should always work. ","date":"2016-12-30","objectID":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/:1:0","tags":["Linux","Ubuntu","Python","pip","virtualenv","virtualenvwrapper"],"title":"Installing pip, virtualenv and virtualenvwrapper tidily in Linux","uri":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/"},{"categories":null,"content":"Limitations with this clean solution This works pretty well, but: The bootstrap environments are the ones you base your other virtualenvs on. If, for some reason, you suddenly need some system site package installed for some particular Python package, then you’ll need to create a new bootstrap virtualenv first, with the --system-site-packages option. (Even if you’ve already got a “system site packages” virtualenv, if you subsequently updated the system packages and want a virtualenv to reflect this, you’ll have to go back to the bootstrapping stage.) You’ll likely need to recreate the bootstrap environments in addition to any actual in-use virtualenvs if you want to move to a new Python version. I’ve read in places things that suggest the Python binary is a symlink to the system one; on my current virtualenvs, there are only symlinks to the local virtualenv copy of the python binary. ","date":"2016-12-30","objectID":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/:2:0","tags":["Linux","Ubuntu","Python","pip","virtualenv","virtualenvwrapper"],"title":"Installing pip, virtualenv and virtualenvwrapper tidily in Linux","uri":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/"},{"categories":null,"content":"Fixing virtualenvwrapper slowing terminal startup virtualenvwrapper does slightly delay bash starting up. On investigation, this is principally due to virtualenvwrapper.sh itself, not the activate/deactivate step. You can check this by running time on each of the commands, source bin/activate and deactivate are basically instant, whereas this script slows things down considerably. $ time source $HOME/.virtualenvs/virtualenvwrapper/bin/virtualenvwrapper.sh real 0m0.304s user 0m0.248s sys 0m0.032s A 0.3 s delay isn’t particularly long. But, for me, it makes bash feel slightly less snappy on opening a new terminal. To solve this, you can use the “lazy loading” script provided with virtualenvwrapper. For my setup, this means I would need to hack that lazy loading script to add the activate and deactivate command there. I tried this and it did work. But, it didn’t seem like a good solution. In part, it just felt hacky and, more importantly, these commands would be removed if you upgrade virtualenvwrapper, breaking it until you again hacked the script. Instead, my preferred option to work around the slightly slow loading was to create another script that I can source whenever wanting to work with Python virtualenvs: #!/bin/sh -e export WORKON_HOME=$HOME/.virtualenvs export PROJECT_HOME=$HOME/.local/src source $HOME/.virtualenvs/venvwrapper/bin/activate source $HOME/.virtualenvs/venvwrapper/bin/virtualenvwrapper.sh deactivate This is less direct. You can’t just directly access virtualenvs. Insteaad, you must source this script first so that you can then workon a virtualenv to workon. However, it keeps the clean install we have made, doesn’t slow bash from loading at all, and isn’t a hack of the existing virtualenvwrapper. Provided we keep it installed in the same place, it’ll work. ","date":"2016-12-30","objectID":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/:3:0","tags":["Linux","Ubuntu","Python","pip","virtualenv","virtualenvwrapper"],"title":"Installing pip, virtualenv and virtualenvwrapper tidily in Linux","uri":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/"},{"categories":null,"content":"Keeping clean Despite the few drawbacks, I do like the idea of this approach overall. You don’t even need to install any Python packages, even as a user, into Python outside of a virtualenv. Bear in mind if you’re intent on being particularly rigorous in keeping your system Python clean: installing software that is built on Python via your package manager (e.g. apt) may well end up pulling in the package manager’s version of dependencies that the software may use. A way around this is to install whatever software you want to use in a virtualenv instead manually, although this may be awkward if that software uses system packages. ","date":"2016-12-30","objectID":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/:4:0","tags":["Linux","Ubuntu","Python","pip","virtualenv","virtualenvwrapper"],"title":"Installing pip, virtualenv and virtualenvwrapper tidily in Linux","uri":"/posts/installing-pip-virtualenv-and-virtualenvwrapper-tidily-in-linux/"},{"categories":null,"content":"Despite “game” being in the official site’s domain name, I think recommending The Novelist as a game would be difficult. There are lots of games whose gameplay I don’t get on with which others adore, but the gameplay here seems a bit too simple to be enjoyable. You’re confined to a single, small environment throughout and you carry out the same hide and seek game for the entire one or two hours the game lasts: hiding from the occupants, and seeking clues. It’s a rudimentary stealth game, and there are games that do this much more successfully if that’s what you’re looking for. The core gameplay itself isn’t perhaps the point though. In The Novelist, a family are on a summer retreat, staying in a slightly unusual house. You’re some ghostly presence that’s confined to the walls of the house, and able to observe the family, read clues that are scattered around and even characters' memories. You’re not entirely free to roam around. With the stealth gameplay option enabled, the family can spot you, and you need to keep hiding out of sight. As I mentioned, this didn’t engage me at all — there’s little challenge — and I quickly turned it off having played through the first couple of chapters with it enabled,1 instead opting to explore freely. What you quickly learn is that there are ongoing struggles over the summer: the titular novelist wants to focus heavily on writing his novel, his partner wishes him to spend time with the family and wants to try and get back into painting, while their son is isolated and struggling at school. At various points over the summer, there is some way for the novelist to make one of the family happier, albeit at the expense of the others. As the player, you take on the role of decision maker and see what happens following that. If the fundamentals of The Novelist are based on choices alone, you might argue that it could be equally served by text. What then does the scenario add? Wandering around and exploring for clues adds a pacing to the decision making process. Instead of making a snap decision, it gives time to reflect on clues you’ve already discovered. You’re also confined to the small, slightly claustrophobic domestic setting the entire time; like the ghost restricted to the house, the novelist and his family are also trapped along certain paths by their choices. The Novelist could be compared to Passage as they deal with similar themes. I don’t recall being as affected by Passage. Maybe because I’m almost a decade older since I played Passage, or maybe because you have a greater involvement in the characters' lives, a couple of hours rather than a couple of minutes. Decisions here certainly do feel like they have consequence. Choices you make can be a big influence on where life takes the novelist and those around him. Stated like that, that’s an obvious conclusion to take away from the game: you may face sometimes difficult decisions where there are one of several mutually exclusive outcomes that you must choose from. Many games feature decisions to be made by the player, but often fail in making the decisions feel consequential. All too often, they reduce to a simple choice of whether the player’s character will appear on Santa’s nice or naughty list.2 It can also be easy to reverse these decisions too. In some games, inflicting retrograde amnesia on every non-player character in town is sometimes possible by, say, donating enough money (thanks Fallout 3). Certainly these types of difficult choices do arise in the real world. However, one thing perhaps missing here is that many choices are the result of more casual decisions made over longer periods of time. Do I go out and do something different, or stay in and save money? Do I spend more time on this hobby or that? Should I be spending more time with friends and family, or work harder on personal projects? Over time, where you set the balance on those choices can end up cementing into concrete consequences. But that kind of subtlety might b","date":"2016-12-24","objectID":"/posts/the-novelist-a-haunting-experience/:0:0","tags":["game","review"],"title":"The Novelist: a haunting experience","uri":"/posts/the-novelist-a-haunting-experience/"},{"categories":null,"content":" broken using efibootmgr. In writing this, I checked the valid spelling of “aieeee” and it is apparently with four e letters at the end. If you’re currently panicking how to fix a system where the UEFI boot entry for your operating system has disappeared (i.e. from what you might think of as the “BIOS” boot entries), you may be yelling a much more extended form of that exclamation than that correct spelling could ever suggest. In that case, go straight to “Fixing the problem” — go directly to that section and do not collect £200 — for advice. ","date":"2016-12-16","objectID":"/posts/uefaieeee/:0:0","tags":["EFI","UEFI","boot entry","efibootmgr "],"title":"UEFaIeeee ","uri":"/posts/uefaieeee/"},{"categories":null,"content":"Background Recently I’ve been undergoing the arduous process of getting a recent version of Ubuntu on my laptop and dual booting with Windows. While I was experimenting, I’d got everything working reasonably, when I’d installed some updates and then, on shutting down that time, to let them take effect, the laptop decide to lock up with the blinking boot/shutdown dots illuminating but nothing happening. I figured I’d check if everything was OK. I found that you can use debsums to check if installed packages are as expected. Running it (sudo debsums -s to ignore OK files) gave a few warnings. Since one of them referred to a kernel image, it was a little concerning. Since I hadn’t done much with the laptop except install from a DVD for which I’d verified the SHA256SUM (and verified the signature of the sum file too), I didn’t think anything untoward could have happened. And, looking around, it seems that this has happened in previous releases too: the mismatch was likely a genuine error in the release, not indicative of anything wrong with my install. Of course, my rigorous side stupidly got the better of me and I figured I might as well investigate further. Simple. I can just install Ubuntu again to a USB stick, not touching my existing installation and then rerun debsums there. That thought there turned out to be anything but simple. But I wasn’t to know that at the time. An hour and a USB installation later, yes, I’d demonstrated that the same warnings from running debsums came up on this install too. Satisfied with that explanation, I’d removed the USB stick, plugged my hard disk back in and decided to boot, and saw: Boot failure uh, OK. ","date":"2016-12-16","objectID":"/posts/uefaieeee/:1:0","tags":["EFI","UEFI","boot entry","efibootmgr "],"title":"UEFaIeeee ","uri":"/posts/uefaieeee/"},{"categories":null,"content":"Installing Ubuntu to a USB stick broke my existing Ubuntu install My guess as to what had happened is as follows. When I’d installed Ubuntu to the USB stick, it installed to /dev/sdb, not /dev/sda. The previous “ubuntu” boot entry that worked fine for my hard disk was referencing /dev/sda and looked for a bootloader on the EFI partition of my hard disk. The USB install had been to /dev/sdb and the currently existing “ubuntu” boot entry had been overwritten by one with the same name. Now that boot entry was referencing /dev/sdb and looking for a bootloader on the EFI partition of my USB install, couldn’t find it, and therefore no longer booted. The USB install could be booted without a hitch since the EFI entry was pointing correctly to it. Incidentally, the Windows install on the same hard disk was still booting fine, again as expected, because the Windows EFI entry hadn’t changed. ","date":"2016-12-16","objectID":"/posts/uefaieeee/:2:0","tags":["EFI","UEFI","boot entry","efibootmgr "],"title":"UEFaIeeee ","uri":"/posts/uefaieeee/"},{"categories":null,"content":"Fixing the problem Some PCs give you UEFI setup options to add a boot entry, so you can resolve the problem directly there. You’ll need to know the path to the bootloader for whatever operating system you want to boot, but that should be relatively easy to locate. However, my laptop is sparse in its configuration options and does not offer this option. In that case — and even if your PC does have an option to add boot entries directly) — you can use efibootmgr to add the entry back. See the answer here1 which shows that you’ll want something along the lines of: efibootmgr -c -d /dev/sda -p 1 -l '\\EFI\\ubuntu\\shimx64.efi' -L \"Ubuntu\" As the post says, you can equally use Microsoft’s bcdedit to do the same, but that requires you to have Windows. You can always run efibootmgr from a bootable Linux DVD or USB. And, you’ll need to change the disk (/dev/sda) and partition number (the -p 1) to reflect which one is the EFI System Partition, which you can find by doing fdisk -l in a terminal. Then the entry will be readded and your install should be bootable again. That answer was exactly what I needed. Unfortunately, it took me far longer to find than it should have, probably because it’s talking about “repairing GRUB”. Hopefully, the keywords I’ve used here might help others find this a bit more easily. ","date":"2016-12-16","objectID":"/posts/uefaieeee/:3:0","tags":["EFI","UEFI","boot entry","efibootmgr "],"title":"UEFaIeeee ","uri":"/posts/uefaieeee/"},{"categories":null,"content":"What are the failures here? This was unexpected. Yes, installing to a hard disk, then USB is probably not a common use case, but you certainly wouldn’t expect it to break an existing installation. Maybe it’s possible that it was because I had a slightly unusual dual boot setup, but I don’t see that should make a difference. I’d expect this to happen even if you’re single booting. (Feel free to correct me if you’ve tested and found otherwise.) ","date":"2016-12-16","objectID":"/posts/uefaieeee/:4:0","tags":["EFI","UEFI","boot entry","efibootmgr "],"title":"UEFaIeeee ","uri":"/posts/uefaieeee/"},{"categories":null,"content":"Hardware side For my laptop in particular, you can’t add EFI boot entries at all. Auto detection would be ideal, but you can’t even add entries manually. ","date":"2016-12-16","objectID":"/posts/uefaieeee/:4:1","tags":["EFI","UEFI","boot entry","efibootmgr "],"title":"UEFaIeeee ","uri":"/posts/uefaieeee/"},{"categories":null,"content":"Ubuntu Why does the installer not check for or warn of the possible issue of clobbering an existing valid boot entry? Wouldn’t it be preferable for this to be far more explicit, check for an existing boot entry and ask the user whether they want to leave the boot entries intact (and abandon the install), add a new entry or replace the existing entry (with the warning that this will probably break the existing installation)? The author of the answer maintains an EFI boot manager, so he definitely knows his stuff here. ↩︎ ","date":"2016-12-16","objectID":"/posts/uefaieeee/:4:2","tags":["EFI","UEFI","boot entry","efibootmgr "],"title":"UEFaIeeee ","uri":"/posts/uefaieeee/"},{"categories":null,"content":" watch was definitely a great budget choice. As I mentioned in a previous post just after Christmas last year, I bought a new Garmin watch. That’s also the first GPS watch I’ve owned. Now I’ve been using it regularly for the best part of a year, I’ve got a much better idea of how I use it, and what I like about it. And I like the Forerunner 15 a lot. Many of my favourite things focus on doing one or a small number of tasks well. It’s why I use Rockbox on dedicated audio player hardware as opposed to some music player app on a phone, and why I tend to use a separate camera, not a phone, for taking photographs. The FR15 is a slightly chunky digital watch, compared with the low profile, cheap and cheerful Casio I usually wear. But it’s not bulky enough that I’ve ever noticed it as an extra weight on my wrist. It’s easy to just forget that it’s there altogether. The strap’s also comfortable to wear. It’s also waterproof to 50 metres, which means that your investment won’t be ruined by using the watch in the rain, although it’s not a swimming fitness watch. The battery life could be better, but I usually get several runs out of it from a full charge. And it should be capable of lasting for a marathon, even if I’m not yet. ","date":"2016-12-14","objectID":"/posts/a-year-of-garmin-a-forerunner-15-review/:0:0","tags":["Garmin","Forerunner","review","running"],"title":"A Year of Garmin: a Forerunner 15 review","uri":"/posts/a-year-of-garmin-a-forerunner-15-review/"},{"categories":null,"content":"Connectivity Charging and data transfer are by USB only. There’s no Bluetooth connectivity, but I prefer that there’s no specific mobile or desktop application that I’m forced to use to get my data off the watch. As I don’t use them, I can’t comment on Garmin’s client software or their website where you can submit and view your data, as I copy the data off to a PC running Linux. But, I imagine they’d have most of the features you’d want, provided you’re comfortable with Garmin storing your location history. Alternatively, it’s simple to copy the data directly from the watch for your own processing, which is how I use it. One thing you miss out on if you don’t regularly use the official software is updates to the ephemeris Extended Prediction Orbit (EPO) data. This provides a cache of satellite locations for a short period of time and helps to speed up the initial location detection of the watch. It’s also possible to download this data from Garmin without using their software; I’ve mentioned solutions, including my own, in the previous post I made on Garmin watches and Linux. Without that data to hand, the time to get a location lock can occasionally be long enough that I’ve warmed up and just set off and waited for it to catch up ten minutes later. It does vary depending on where you are. That’s not too common a problem fortunately, but if you’re racing then having the EPO data and getting the FR15 ready well ahead of time is a good idea. Though you can prepare the watch this way for a run, you’ll need to occasionally press a button to keep the FR15 awake before starting a run, otherwise the watch’s power saving feature (which you can’t disable) will turn it off. Not a huge deal, but not a perfect design either. Why wear a GPS watch while running? There’s the slightly gimmicky thing of mapping runs and seeing how fast you were running. That said, having an archive of previous runs completed might be useful if you particularly enjoyed a route and want to file it away to revisit at a later date. The data collection itself isn’t really important for me. Having a live status of how far I’ve gone and my current pace is. That’s useful both in training runs to see if I should be trying to push myself more, or in races to know how long I’ve got to go before I can finally stop. It’s useful in races for pacing too. You can set a target pace and you get an audio alert when you’re on pace, or go above or drop below it. And keeping good track of your progress can be a useful motivator too. In a 10 km race this year, I worked out that if I pushed for the last 1.5 km or so, I might just beat that time and tried to up my pace. In the very last minute, I worked out it was possible if I ran the last 200 metres very quickly, and that encouraged me to both try and go on to succeed. Likewise, the watch keeping tabs on your recorded best times at various distances (e.g. 1 km, 1 mile, 5 km) is nice to have when you realise that you’ve improved on your previous best. With all that in mind, even a low-end watch like this is more than capable. You do lose bundled features like heart rate monitor and GPS route navigation, but you get the most essential and useful features, at a price that’s reasonable enough. At the same time, this means the watch is pared down to the essential features, implements them well and they are all easy to access through the simple menus. For a first-time user of GPS watches, I’m pleased with how good a purchase the Forerunner 15 was, especially considering it was on sale at the time. I looked around for a good couple of hours at reviews and comments on watches, and concluded that you could spend a lot and still not be entirely satisfied. It seemed most sensible to buy something cheap and cheerful, and not be disappointed by its flaws: I’m more forgiving of a sub-£100 watch, than maybe one that’s closer to £200. Finally, if you are shopping for GPS watches, the best resource I found for looking at these watches is the compreh","date":"2016-12-14","objectID":"/posts/a-year-of-garmin-a-forerunner-15-review/:1:0","tags":["Garmin","Forerunner","review","running"],"title":"A Year of Garmin: a Forerunner 15 review","uri":"/posts/a-year-of-garmin-a-forerunner-15-review/"},{"categories":null,"content":"In the days of the iPod, when travelling on public transport, in the UK at least, those trademark white earphones were everywhere. But it seems that portable audio players have become a thing of history; with the advent of phones as all-in-one devices, these have pretty much replaced audio players (and cameras too). Why carry extra devices when the one you’ve already got in your pocket will do? Returning back to those past days, I’d put up with a couple of very cheap MP3 players, which would often glitch in all kinds of infuriating ways. That ranged from failing to read certain MP3s, going further in their obstinance and just locking up entirely, or further still and actively protesting noisily. I’m not sure that “randomly firing a loud burst of white noise into my ears until power off” is a particular bullet point to stick on the box blurb. Fortunately, it didn’t take long before I’d spotted Rockbox, a open source replacement firmware for audio players. Since then, Rockbox has become one of the open source projects that I use most. I’ve used it near daily on some device or other for probably around a decade, and I don’t see that changing anytime soon. ","date":"2016-12-10","objectID":"/posts/still-rocking-rockbox/:0:0","tags":["open source","Rockbox"],"title":"Still rocking Rockbox","uri":"/posts/still-rocking-rockbox/"},{"categories":null,"content":"Easy listening As an audio player is one of the things I’m using all the time, I like having a separate device with its own, usually lengthy, battery life. With that in mind, I’m probably a little ignorant if there’s phone audio player software that’s as competent and fully featured as Rockbox. It’s certainly possible there’s a suitable competitor. But I’d be surprised. In terms of the range of audio formats supported and the number of features Rockbox offers, it seems difficult to match. There’s gapless playback, bookmarking, equalizer, compressor, crossfeed, control of pitch and speed, and ReplayGain support. On top of that, adjusting many aspects of a Rockbox-enabled device is possible. Even the display theme is fully customisable should you wish to create your own. Furthermore, if a device has the option of adding external media card storage, such as microSD, Rockbox sometimes goes beyond a device’s official support, enabling the use of larger memory cards. ","date":"2016-12-10","objectID":"/posts/still-rocking-rockbox/:1:0","tags":["open source","Rockbox"],"title":"Still rocking Rockbox","uri":"/posts/still-rocking-rockbox/"},{"categories":null,"content":"Installing Rockbox If you want to give it a try, the first step is to take a look at the official site and see what devices are currently supported. It’s also worth checking around the forums there too, particularly under “New Ports” as there may be newer devices where less stable ports-in-progress are available, but not listed on the main site. Devices that run Rockbox are commonly fairly cheap as it’s usually older, out of demand hardware that’s supported. It does mean getting hold of one in good condition might not be necessarily easy. I’ve used Rockbox on several different devices, first and second generation iPod Nanos, and the Sansa Fuze and Fuze+. There have been devices I have preferred, but most devices are quite usable since once you’ve selected what to play, there’s little more to do. The main differences really come down to screen size, ease of accessing the user interface (for instance, the Fuze+ has a touch sensitive pad which isn’t ideal compared with the definite precision of the iPod Nano or Fuze wheels), and whether a device supports storage expansion. When you’ve got a compatible device, it’s relatively straightforward to get things going. The Rockbox Utility is a program that you can use to install Rockbox directly to hardware that’s plugged into your PC, or you can install manually if you prefer, although it may involve using a compiler. The actual installation process varies depending on hardware, for instance, you may also have to download some official firmware file for your hardware first so that it can be patched to add Rockbox. After that, you should be ready to copy files onto your player’s storage and start using it. On a security point: when I last used the Rockbox Utility, only a few weeks back, it was using insecure HTTP URLs for downloading from the site. This is despite the download servers the Utility downloads files from during installation actually supporting HTTPS. I don’t have the changes I made to hand, but it’s possible to use your favourite find-replace tool to patch the source to replace URLs that match, for example (but not limited to) http://download.rockbox.org to use HTTPS. I really should have submitted a patch but I couldn’t face the hassle of creating a account to access Rockbox’s Gerrit instance at the time. ","date":"2016-12-10","objectID":"/posts/still-rocking-rockbox/:2:0","tags":["open source","Rockbox"],"title":"Still rocking Rockbox","uri":"/posts/still-rocking-rockbox/"},{"categories":null,"content":"The future? What’s the long, long term future of the project? If you’ve hardware for which Rockbox is claimed to be stable, you’ll probably find that’s the case. I’ve had devices occasionally lock up, but these haven’t happened frequently enough to be frustrating. As mentioned above, it’s as feature-filled as you’d likely require too, so even if development stopped on it entirely it would still continue to be usable too. Nonetheless, it’s still actively maintained with a steady run of code changes being made. You’re limited to what devices you can install Rockbox on, but these were often produced in large quantities (e.g. iPods) so there should be an abundance of them on the market. All this makes it relatively simple to pick up a working used one to install Rockbox. For now. Getting hold of one of these is unlikely to be a problem as there are so many out there, but getting one in good working order may prove increasingly difficult as time goes on. If it’s a failed battery that’s a problem, it’s probably easy to get a replacement, although replacing the battery may prove trickier: these devices weren’t often designed to be user-serviceable. There has been work on making Rockbox an Android app. Ultimately, that’s likely the way things will go in the far distant future as old audio hardware becomes scarce and fewer new devices are designed and manufactured. In the meantime, it’s nice to have software that does one thing and does it particularly well. As nothing better seems to have come along to usurp Rockbox’s place, it’s quite possible that I’ll be using it over the next ten years as I have for the last ten. ","date":"2016-12-10","objectID":"/posts/still-rocking-rockbox/:3:0","tags":["open source","Rockbox"],"title":"Still rocking Rockbox","uri":"/posts/still-rocking-rockbox/"},{"categories":null,"content":"Why? Sometimes you might need to record both the audio playing out on your computer as well as an input. Often, this might be if you’re recording a screencast or podcast, or if you want to record both sides of an online voice chat or video call, e.g. with Hangouts, Skype or some WebRTC app, for instance. In my case, I wanted to record an interview and, ideally, want to hear what both of us were saying, without having to record both sides of the conversation separately and later try to combine them into a single piece of audio. ","date":"2016-12-01","objectID":"/posts/conversation-conservation-recording-audio-input-and-output-simultaneously-in-linux/:1:0","tags":["audio","recording","Linux","Ubuntu"],"title":"Conversation conservation: recording audio input and output simultaneously in Linux","uri":"/posts/conversation-conservation-recording-audio-input-and-output-simultaneously-in-linux/"},{"categories":null,"content":"How The solution if you’re using Linux and PulseAudio is mentioned here but I’ve clarified this below so I can find it quickly: Run pactl load-module module-loopback in a terminal. You should start hearing your microphone input; with headphones on at least, you shouldn’t get feedback. What you do get is a very slight delay in hearing yourself. For me, using earphones and removing one seemed to help remove most of the off-putting distraction this delay causes, while still being able to hear the other participant in the conversation. Start recording in, say, Audacity or some other audio recording software. Open the PulseAudio Volume Control. If you don’t have it installed, apt install pavucontrol should get you it in Ubuntu. You may then have a shortcut to it wherever your programs get listed, e.g. in Ubuntu, you can access it via the Unity dash. Alternatively, run pavucontrol\u0026 in a terminal, open the Recording tab. to Recording tab, select what should be something like “Monitor of Built-in Audio Analogue Stereo” for the selection for Audacity. This is one thing that’s a little counterintuitive; you don’t see this unless you’ve already started recording. When you’ve finished, you can run pactl unload-module module-loopback to stop hearing audio input in the output. You might also want to reset the Audacity recording setting to what it was, so that you don’t forget you changed it. ","date":"2016-12-01","objectID":"/posts/conversation-conservation-recording-audio-input-and-output-simultaneously-in-linux/:2:0","tags":["audio","recording","Linux","Ubuntu"],"title":"Conversation conservation: recording audio input and output simultaneously in Linux","uri":"/posts/conversation-conservation-recording-audio-input-and-output-simultaneously-in-linux/"},{"categories":null,"content":"Previously I wrote about setting up my laptop to dual boot Windows and Ubuntu. As the Ubuntu LTS I had installed was now aging badly1, it really, really, really needed upgrading. I’d been hesitant to upgrade. The last time I installed this way, it took a week of following several guides, many of them out of date or not working, and several failed installs. ","date":"2016-11-28","objectID":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/:0:0","tags":["BitLocker","encryption","LUKS","Ubuntu","Windows"],"title":"A beginner's guide to OS encryption: (almost) 2017 edition","uri":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/"},{"categories":null,"content":"A quick guide Fortunately, thanks to someone really helpful on the internet, I don’t have to write much this time: Follow this guide. That’s pretty much it. ","date":"2016-11-28","objectID":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/:1:0","tags":["BitLocker","encryption","LUKS","Ubuntu","Windows"],"title":"A beginner's guide to OS encryption: (almost) 2017 edition","uri":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/"},{"categories":null,"content":"Installing Windows Seeing as I have my text editor open, I’ll clarify a little more. To start with, I booted Windows 10 with my computer set to boot in UEFI mode and with Secure Boot enabled. Starting from a clean hard disk, when the Windows installer booted, I was using a clean drive, and instructed the installer to create a new partition using a certain proportion of space on my drive, leaving the rest of the disk as unformatted space. You’ll find you don’t get one partition but four: a recovery partition, the system (or EFI partition), a reserved space partition and the primary partition where Windows will actually be installed. Another hour or so later, and following running Windows Update, hopefully you have a working and updated Windows machine. One big plus of the Windows 10 installer versus what I’ve experienced previously is that most computer hardware tends to get detected and installed automatically, without you needing to find drivers buried away on a manufacturer’s website. Another is that, although Windows 10’s cumulative updates can be problematic if there’s an issue with one part of them, they do mean that you have only one big update to install, not several hundred, as is seriously the case with Windows 7. At this point, you can go ahead and encrypt the Windows operating system drive with BitLocker, provided you have a version of Windows that offers it. If you don’t have a Trusted Platform Module in your PC, it’s still possible to use BitLocker, but you’ll need to override this using Group Policy (search for gpedit.msc in the Windows search bar and run it). You can find the options in Group Policy under (breathe in): Computer Configuration\\Administrative Templates\\Windows Components\\BitLocker Drive Encryption\\Operating System Drives. The specific option you need to change is “Require additional authentication at startup” and you can then tick “Allow BitLocker without a compatible TPM”. You should probably consider enabling “Allow enhanced PINs for startup” which is in the same Group Policy section while in there, seeing as this lets you use characters, not just numbers.2 Right clicking on the drive in Windows Explorer and selecting “Turn on BitLocker” should let you then go through the process of encrypting the drive. Although I’m discussing Windows and Ubuntu here, note that the guide I’ve linked to may well work for two Linux installations too. ","date":"2016-11-28","objectID":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/:2:0","tags":["BitLocker","encryption","LUKS","Ubuntu","Windows"],"title":"A beginner's guide to OS encryption: (almost) 2017 edition","uri":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/"},{"categories":null,"content":"Installing Ubuntu The usual advice is to install Linux after Windows, since Windows doesn’t necessarily play as nicely with Linux. I’m not sure if that still holds in the days of UEFI booting, but it seemed sensible to do. Going back to the excellent guide that was posted, it’s not difficult to follow. Pay attention to the comments, as there were three steps that were unnecessary in the process, which were pointed out there. The process has quite a few steps, but even if you don’t understand fully what’s going on, it should Just Work™. Here’s an overview of what’s involved once you’ve loaded Ubuntu from a DVD or USB: Use your unpartitioned disk space to create two partitions. One will be the unencrypted /boot partition and the other will be a LUKS container that contains an LVM physical volume. This is then subdivided into two logical volumes, one for the root partition, /, and the other for /swap. Install Ubuntu to the root partition you created with the installer, as normal, although make sure that you choose “Something else” when asked how to install. Don’t immediately reboot after installing. Access your new installation by using a chroot. This gives a convenient way of modifying a configuration file in the new install (crypttab) to add details of your drive which need to be available at boot, and updating its initramfs with that configuration. After that, booting up into Ubuntu should work and prompt you to unlock the LUKS container. Depending on your UEFI setup and your preference, you may need to go into your PC’s boot menu at startup and select Ubuntu directly (or reorder the default boot order so that it boots by default). ","date":"2016-11-28","objectID":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/:3:0","tags":["BitLocker","encryption","LUKS","Ubuntu","Windows"],"title":"A beginner's guide to OS encryption: (almost) 2017 edition","uri":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/"},{"categories":null,"content":"Some more tips The guide’s pretty good, and my install was successful first time with no real hiccups, which was a pleasant surprise. There are, however, a few points which aren’t mentioned: When you’re using GParted to partition your drive, you may wish to use the “cleared” option instead of “unformatted” to avoid any confusion. Otherwise, if your drive has previous partitions present, you may see GParted inform you that the type of partition is whatever used to be there. It shouldn’t make a difference however, as you later should format the partitions anyway. Make sure you read what the terminal’s telling you. I was really confused when the luksOpen command didn’t work as I’d run the luksFormat command before. Where I’d gone wrong is that luksFormat asks you to confirm you want to format the drive. Instead of typing uppercase “YES” as it instructs, I’d typed “yes”, and the program just exited instead, without warning it hadn’t done anything. When creating the logical volumes where / and /swap will live, you use the lvcreate command. With the -L flag as used in the guide, you directly specify the size it should take. If you want to ensure that the second partition uses the rest of the space, you can use the -l flag as in -l 100%FREE instead of e.g. -L 7.5g. You can also check the status of the volumes you’ve created using pvs and lvs to ensure everything is as you like, before proceeding. When installing the GRUB bootloader, it belongs in the EFI partition. If you select the disk where you’re installling the operating system, the Ubuntu installer should automatically detect the partition and install GRUB there. It’s likely that it’s /dev/sda if the only hard disk that’s plugged in. Alternatively, you can identify the EFI partition directly and choose that, e.g. /dev/sda2. The guide suggests using mount and swapon to check which volumes are being used in the new installation. Running lsblk in the terminal can do this in a single command. It will show you which block devices are mounted and where: you should see the / and /swap inside your encrypted LUKS container. ","date":"2016-11-28","objectID":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/:3:1","tags":["BitLocker","encryption","LUKS","Ubuntu","Windows"],"title":"A beginner's guide to OS encryption: (almost) 2017 edition","uri":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/"},{"categories":null,"content":"Reinstalling How would reinstalling one of the operating systems work? I haven’t tried it yet, so can’t say for certain. Below is a thought experiment, although please heed the warning that it is only a thought experiment. My hunch is that an Ubuntu reinstall should be fairly straightforward: delete the two partitions you created (/boot and the LUKS/LVM partition), create new ones and follow the whole process again. It shouldn’t impact the Windows installation, as the Windows bootloader should still exist in the EFI partition. And reinstalling Ubuntu should just overwrite the existing Ubuntu bootloaders. Reinstalling Windows might be trickier. If you retain the existing EFI partition and install Windows to the same operating system partition (i.e. without wiping all four partitions and telling Windows to install to the unpartitioned space), that shouldn’t break Ubuntu as I’d guess that Windows just uses the existing EFI partition, and therefore would leave GRUB in place. Otherwise, if you do decide to wipe out all four partitions that Windows created, you’ll need to reinstall the GRUB bootloader to the new EFI partition; see Debian’s wiki and this guide. It still shouldn’t otherwise break your Linux installation, apart from this though. Again, I’ve not tested any of this has been tested yet. If I ever try reinstalling one operating system and not both, I’ll no doubt edit this post or add a new one describing that process. A subject for another post. My brief advice if you’re intending to use Ubuntu LTS releases is to upgrade every time, and not skip any, especially if you’re running it as a desktop. ↩︎ Yes, in this case “PIN” does then become a misnomer. ↩︎ ","date":"2016-11-28","objectID":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/:4:0","tags":["BitLocker","encryption","LUKS","Ubuntu","Windows"],"title":"A beginner's guide to OS encryption: (almost) 2017 edition","uri":"/posts/a-beginners-guide-to-os-encryption-almost-2017-edition/"},{"categories":null,"content":"!!! article-edit \"\" Edit 2017-12-29: Justin Smith, who’d read this post, had a discussion by email with me pointing out he’d had some problems with this process: namely, wiping appeared to complete, but then he could still find files present on the disk afterwards. I couldn’t figure it out, but suggested Justin contacted hdparm’s author, Mark Lord, for advice. Mark diagnosed the problem as the drive not being unmounted first: \u003e Yes, you MUST unmount anything that got automounted, because stuff \u003e that is mounted gets periodically written to by the kernel \u003e (filesystem timestamps etc), which could account for some of the \u003e non-zero stuff you saw later. Mark's suggestion for wiping the drive was as follows: \u003e Do this: \u003e \u003e (1) Un-mount all partitions of the drive, including those that got \u003e auto-mounted. Then do \"`sync`\". \u003e \u003e (2) Set an empty password: `hdparm --security-set-pass NULL /dev/sdX` \u003e \u003e (3) Erase it: `hdparm --security-erase NULL /dev/sdX` Yesterday I was trying to erase a hard drive before I used it for a new install. It may well have never been used, but I couldn’t remember and, for the sake of a few minutes, it seemed sensible to do so first. The best way to erase, especially if the drive is a solid state drive, is to use the ATA Secure Erase command which I’ve mentioned before. This command should also work for most magnetic hard drives too, unless they’re ancient. Instead of just mentioning the tool, I’ve summarised the details from the lengthy page where I found them below, and discuss how to deal with “frozen” drives. You can access hdparm by booting into, for example, an Ubuntu installation DVD or USB. Installing Linux isn’t required: you can just “Try Ubuntu” instead which doesn’t install anything at all. ","date":"2016-11-22","objectID":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/:0:0","tags":["hdparm","erase"],"title":"Securely erasing frozen hard disks with hdparm","uri":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/"},{"categories":null,"content":"Safety first First, the most sensible thing to do when erasing a drive is remove any drives other than those you wish to erase. It means even if you accidentally type the wrong thing that you won’t erase a drive that you didn’t intend to. Next, you need to get the drive’s name. Open a terminal by pressing Ctrl+Alt+T. sudo lshw -class disk This will show you details of the disk, including the name. Replace the /dev/sdX in the commands below with that name. ","date":"2016-11-22","objectID":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/:1:0","tags":["hdparm","erase"],"title":"Securely erasing frozen hard disks with hdparm","uri":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/"},{"categories":null,"content":"Setting a password The next task is to set a password. I’m not sure that you really need to do this, but the kernel.org guide advises it due to problems with certain PCs, and it only takes one extra command. Note that the “foo” password below can be replaced by something of your choice, but it doesn’t really matter; when the drive is erased, the password should be removed. $ sudo hdparm --user-master u --security-set-pass foo /dev/sdX security_password: \"foo\" /dev/sdX: Issuing SECURITY_SET_PASS command, password=\"foo\", user=user, mode=high SECURITY_SET_PASS: Input/output error Well, that error wasn’t expected. When I’ve used hdparm in this way previously, I haven’t had a problem. You might expect it if the drive has a password set already, but in this case I couldn’t recall doing that. If that’s the same for you, it could be a simple fix. First, check the drive’s current status via: $ sudo hdparm -I /dev/sdX and you may see “frozen”, as opposed to “not frozen”, in the “Security” section of the hdparm output. This means you can’t change security settings, so attempting to set a password fails. ","date":"2016-11-22","objectID":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/:2:0","tags":["hdparm","erase"],"title":"Securely erasing frozen hard disks with hdparm","uri":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/"},{"categories":null,"content":"Unfreezing In this case, the actual fix is simple. Suspending the PC, then powering it back on should then give you an unfrozen drive: you can check by running the hdparm -I command above again. Now, if you retry setting a password, you should be able to run the SECURITY_SET_PASS command without error. Also, rerunning the hdparm -I command yet another time should show that a password is “enabled” as opposed to “not enabled”. ","date":"2016-11-22","objectID":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/:3:0","tags":["hdparm","erase"],"title":"Securely erasing frozen hard disks with hdparm","uri":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/"},{"categories":null,"content":"Erasing You can now proceed with the erase. If your drive’s hdparm output states supported: enhanced erase, you could replace the --security-erase below with --security-erase-enhanced: $ sudo time hdparm --user-master u --security-erase foo /dev/sdX After the erase, the password should be removed, and running the hdparm -I command one final time should show that the password is “not enabled”. ","date":"2016-11-22","objectID":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/:4:0","tags":["hdparm","erase"],"title":"Securely erasing frozen hard disks with hdparm","uri":"/posts/securely-erasing-frozen-hard-disks-with-hdparm/"},{"categories":null,"content":"go-mailin8 is a command line utility I’ve created to get the most recent message from a Mailinator email inbox. What’s Mailinator? Mailinator is one of a number of free services that allows you to use a disposable email address. This is handy when you’re signing up for something but when you know that the account or email is only needed ephemerally, as a one-off. My frequent use case is for free Bandcamp downloads that require an email address. It can also be useful for testing on the web, if you’re developing something that sends email. What using a disposable email address offers is not having to hand out your real email address when that’s not needed, with the advantage of reducing the number of unwanted messages you receive. Yes, of course, you can usually unsubscribe from mailing lists, but that’s extra work. Better to just not let the unwanted messages anywhere near your real inbox, where possible. Can’t you just visit the site? Yes, you can. But I’m lazy, and this means switching to a new tab, visiting the URL and loading their home page, then entering the inbox address, loading the inbox page, and clicking the mail to display it and then finally copying the activation or download URL I’m looking for. Much easier if I can just run a command line program that just takes the local-part of the mailbox name (the part before the @): ./go-mailin8 \u003clocal-part\u003e and gives me back the most recent message. Copy-pasting the URL directly from the terminal is then simple. Why Go and not Python? It would be probably have been easier to write in Python as I know that better than I do Go. But I’m conscious that I spent some time over the summer reading up on Go; the more chance to practise, the better. In this case, features Go offers like being faster, and easier to write concurrent code aren’t really important. You could equally well write this code in Python and it would be just as useful. Where Go excels is that the code has no dependencies to install once it is compiled. Much simpler than telling someone to first install Python and then the packages that they need. (I realise that the target audience of such a tool are probably more than capable of doing so, but this reduces the barrier for someone to get the program running on their computer nonetheless.) ","date":"2016-11-12","objectID":"/posts/collecting-disposable-email-with-go-mailin8/:0:0","tags":["golang","Mailinator","email"],"title":"Collecting disposable email with go-mailin8","uri":"/posts/collecting-disposable-email-with-go-mailin8/"},{"categories":null,"content":"After what seems quite a long time from when the idea was first mentioned to change the name of my employer, it’s finally happened, and they’ve got a new website and everything. We still have the old company mugs, sadly. I’d been helping out in renaming things. It’s surprising just how deep the hooks of a name go: email addresses, documentation, licences, even mentions in code too. A more startlingly obvious case is that of a website. And as there’s now a new website, we don’t really need to maintain the old one anymore. Just taking everything down would be a shame as there’s lots of content on there that people have found useful in the past. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:0:0","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"WordPress migration Like many sites, it used WordPress. One option for migration away from WordPress is to export the content as an XML file, which either various other hosts may be able to import for you directly, or you can try converting into a useful format yourself, e.g. if you’re using a static site generator to build a new site. We just wanted to archive the site, rather than move to a new site and reuse the old content, which is a different task. There are several WordPress plugins that are supposed to convert your site into a static one. However, none of the two I tried were successful; one just froze without seemingly doing anything, while the other only pulled a couple of pages from the site. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:1:0","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Gathering the site content with wget In the end, I resorted to wget which did an admirably good job after a few attempts. wget is more often known as a command-line download tool, but is much more powerful than a simple downloader. The downside with this is that there are options present that you might not even know about until you try this kind of site archiving, hit a problem and then discover which wget option you should have used to fix it. The upside is that wget probably does feature the option you require. After lots of trial and error, what I ended up with was (deep breath): wget --page-requisites --convert-links --adjust-extension --mirror --span-hosts --domains=blog.scraperwiki.com,scraperwiki.com --exclude-domains beta.scraperwiki.com,classic.scraperwiki.com,media.scraperwiki.com,mot.scraperwiki.com,newsreader.scraperwiki.com,premium.scraperwiki.com,status.scraperwiki.com,x.scraperwiki.com scraperwiki.com Let’s look at what each option does: --page-requisites collects other files need to render a HTML page, e.g. images and CSS. --convert-links converts links in the retrieved document to one that will correctly display in the local mirror; this all happens at the end of a collection; if you check files while the collection is in progress, the links won’t yet have changed. --adjust-extension gives HTML files an .html file extension if they don’t have one (--convert-links adheres to these modified filenames too, so if a HTML file has a modified name, then the link to will be correct). --mirror is an alias for a series of options that facilitate mirroring a site. --span-hosts enables wget to move across different hostnames. --domains and --exclude-domains list domains that wget should or shouldn’t retrieve content from. Later the files from our multiple domains got moved into one directory for simplicity. I think you could have done this using --no-host-directories. Running this command will then proceed to collect the content from the sites you’ve allowed it to crawl. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:2:0","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Hosting for free If you have a free GitHub account and are competent enough with the basics of git, note that you can host static sites, also for free, on GitHub Pages. Introducing git version control is an entirely other post, but there are plenty of online tutorials. To do something like this, don’t feel overwhelmed by the different commands. You only really need the basics, i.e. knowing how to create a new repository, add files to it, make commits and push them to a remote repository. Another nice feature of preparing your static site in a version controlled repository is that when you encounter things that need fixing, you can try them and always be able to restore to an earlier version, should you need to. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:3:0","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Problems I had to solve ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:0","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Broken links Initially, the links to images or other pages were broken, the --convert-links option fixed that. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:1","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Unrestricted crawling wget was crawling other scraperwiki.com domains than the ones we wanted and collecting a lot of unnecessary content. All I needed was the main site, so I specified the domains to exclude. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:2","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Removing dynamic features in the static site version Some features of the site, as expected, no longer functioned after collecting the pages. For example, the comment and contact forms we had involve some processing on the WordPress server, and therefore were displayed on the static site, but didn’t work. Removing this content is possible in a couple of ways. Either you could process the HTML, using blunt instruments of find/replace, or the finer tools of reprocessing the HTML with a proper parser, and remove the unwanted elements, for instance using Python and lxml. Easier still is just hiding the broken elements. Finding an appropriate selector for those elements, and then adding a CSS rule for that selector containing display: none; (perhaps with !important if required) will hide them. This also preserves the HTML as it was without mangling it. It probably won’t affect you, but it’s worth noting that automated bots may well still be trying to trigger elements “hidden” in this way. They’re still on the page, just not displayed to normal users of the site (unless they tweak the CSS). We had an odd issue possibly caused by a redirect from the old site: bots were presumably attempting to submit to a form on our new site, even though they were visiting the archive of the old site. (It was flagged by the form provider asking us to approve submissions from that URL.) ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:3","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Query strings in filenames This was caused by WordPress hosting resources with version query strings in the URL, e.g. CSS and JS. When wget retrieved the files initially, it retained the ? and the trailing part of the name, e.g. jetpack.css?ver=4.1.1. The problem is that the links also look like this too. The “?” gets interpreted by clients as a query string, and not part of a static filename. In a static version of the site, the query string won’t work: the browser requests the file without the ? and passes a (now) useless query string. Our copy of the file actually has a filename of jetpack.css?ver=4.1.1, not jetpack.css. We don’t want the client to request jetpack.css with a query string of ?ver=4.1.1, but to actually request a static file with the question mark in its name. I’m not entirely sure what fixed this; I didn’t notice at first. From some point onwards, the ? ended up getting encoded as %3F which then gets correctly requested as a filename with a question mark in it. Not particularly clean, but it worked. It may have just been using a recent version of wget that solved this. This was nice as the alternative was a horrendous find/replace task using commands like: find . -type f -exec grep -Iq . {} \\; -and -exec sed -i 's!wp-content/plugins/jetpack/css/jetpack.css?ver=4.1.1!wp-content/plugins/jetpack/css/jetpack.css!g' {} \\; to fix up the content by hand for every CSS and JS file. (You could optimise by having sed perform all the replacements in one command, rather than executing the same find repeatedly, but I was verifying the changes by eye after processing each filename; git diff is useful here.) That equally worked, but was much more work to check. Searching now, by far the simplest fix would be use a Wordpress plugin to remove the query strings before archiving. There are a couple out there. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:4","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"srcset images Some of our images were responsive srcset images listed in img attributes. Until recently, wget didn’t handle this, but from wget 1.18, it supports srcset images just as it does images in src attributes of img elements. To clarify, it both correctly retrieves the images and updates the links. (I assume only when using --convert-links.) ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:5","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Links in option values As part of the site archive navigation, there were links to option values. These weren’t updated. I just hid those elements as it didn’t seem a critical part of the site. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:6","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"WordPress emoji code Recent versions of WordPress feature code to add emoji support for certain browsers. This embedded JavaScript is in the HTML and was still pulling from the old WordPress site because it contains an absolute URL within it. Saving the file locally and correcting the URL is non-trivial as a relative URL depends on how deep in the local mirror you are. The simplest solution was disabling this feature, by adding the “Disable Emojis” plugin to the site, then recrawling. It’s not essential to fix this, but means the static site is not making requests for files that don’t exist when you move from WordPress. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:7","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Fixing up incorrect index.html URLs Lots of URLs on the original site ended with /, rather than /some_page.html. wget instead saved these pages with a filename of index.html. Fixing these up was a pain. In terms of making the site work, this strictly wasn’t necessary. GitHub Pages will respond with a request to foo/ as the index.html in the foo directory, so any existing links to the site would still work. However, fixing this does ensure anyone sharing links to our pages uses the same URL as the pages always had. I’m not even completely sure that this was entirely fixed correctly; there are just too many URLs to verify, but it appeared to have the desired behaviour. You can see the commit message for the code I used. Essentially, it used lots of regular expressions to substitute: foo/index.html to foo/; foo/index.html#bar to foo#bar; foo/baz.html to foo/baz; foo/baz.html#foobar to foo/baz#foobar. There are a few other tweaks I did to clean up URL links too. They are detailed in the other commit messages in the repository. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:8","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Some other redirect problems For us, some redirects we had set up on the old site caused a couple of cases where files ended up in the wrong place or with the wrong name compared with the original site. We had to be careful to fix up relative URLs here. We also had multiple copies of certain files. These weren’t difficult to fix by hand as there were only a few to modify. Finally, we had to correctly set up redirects to the new site in our server configuration, and everything was finally done. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:4:9","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Moving on After much work, the static version’s now working well. Everything’s pretty much the same on the site aside from the interactive forms. It was much more of a task to get done than I’d imagined. I think it was around half a day spent trying out wget options and another couple of days spent on resolving all the other issues. The advantages are that we don’t need to maintain a WordPress installation to run the site and the site’s getting hosted for free. Good luck if you’re tackling the same problem! Hopefully some of the tips here might help you with your migration. ","date":"2016-09-21","objectID":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/:5:0","tags":["WordPress"],"title":"Archiving a WordPress site with \u003ccode\u003ewget\u003c/code\u003e and hosting for free","uri":"/posts/archiving-a-wordpress-site-with-codewget/code-and-hosting-for-free/"},{"categories":null,"content":"Short post, but more noting this for myself more than anything. There are several times when I’ve blogged little fixes and they’ve proven handy to have in the future, e.g. when I’m setting up a new PC. Partly because I know they’re on my blog, and partly because the posts are titled in the way I would search for a solution. If you have a Windows PC (I think 8 upwards; but definitely 10, and 8.1) with BitLocker enabled, you may find that it reboots shortly without you even doing anything. If you’ve a long password and aren’t a quick typist, you may even find that this means you struggle to enter your password in time. This is apparently a problem if you have a UEFI install, which will be a common configuration these days. Having just tested this simple fix that I saw on Microsoft’s forums: bcdedit /set {bootmgr} bootshutdowndisabled 1 it seems to have resolved the problem; the password prompt will remain on indefinitely. ","date":"2016-09-17","objectID":"/posts/stopping-windows-from-rebooting-at-the-bitlocker-boot-password-prompt/:0:0","tags":["BitLocker"],"title":"Stopping Windows from rebooting at the BitLocker boot password prompt","uri":"/posts/stopping-windows-from-rebooting-at-the-bitlocker-boot-password-prompt/"},{"categories":null,"content":" Human, human, human, human, Human, human, human, human, Human, human, human, human, Human, human, human after all. Daft Punk — Human After All I logged into GitHub this week and saw an unusual message: One of our mostly harmless robots seems to think you are not a human. Because of that, it’s hidden your profile from the public. If you really are human, please contact support to have your profile reinstated. We promise we won’t require DNA proof of your humanity. Putting aside the irony of GitHub detecting bots with their own automated systems, one short email later, it was politely resolved in ten minutes by GitHub’s support team, who decided I was human after all.1 ","date":"2016-09-11","objectID":"/posts/what-happens-when-github-decides-youre-not-a-human/:0:0","tags":["GitHub"],"title":"What happens when GitHub decides you're not a human","uri":"/posts/what-happens-when-github-decides-youre-not-a-human/"},{"categories":null,"content":"What might have caused it? Perhaps creating several similar looking repositories (albeit with different code) in a relatively short space of time. These contain example code to access the API that we offer at work. Maybe it’s because the commits looked similar, maybe because the commit messages or the commit content contained a URL. Who can say? ","date":"2016-09-11","objectID":"/posts/what-happens-when-github-decides-youre-not-a-human/:1:0","tags":["GitHub"],"title":"What happens when GitHub decides you're not a human","uri":"/posts/what-happens-when-github-decides-youre-not-a-human/"},{"categories":null,"content":"Why is this not ideal? Despite the quick resolution, the way this situation is handled might be flawed. First, though the warning was omnipresent, large and red, it didn’t describe the full nature of the problem. The phrase “hidden your profile from the public” to me just implied that GitHub had hidden my profile page. Seemingly, I could still do and access everything as normal, albeit with a warning on every page, so I didn’t fully appreciate the consequences. From reading around, what apparently happens is that any repository that’s under your profile becomes a nice 404 error to anyone who’s not you. I didn’t check this firsthand (because I wasn’t under any impression that anything had really changed with my account). If this is true, that might create some issues which I’ll discuss later in this post.2 Second, they didn’t send me any email warning at all, unless it went into my junk mail. I only noticed because I logged into my account and there was a big red banner at the top of every screen. What if I didn’t log into GitHub at all for a while? Finally, I assume that the process happens instantly when their system considers you a bot. There’s no grace period where you can confirm your identity, before they hide your profile publically. It just gets hidden and, what I suspect happens is, the first you know of it is when it has been hidden. ","date":"2016-09-11","objectID":"/posts/what-happens-when-github-decides-youre-not-a-human/:2:0","tags":["GitHub"],"title":"What happens when GitHub decides you're not a human","uri":"/posts/what-happens-when-github-decides-youre-not-a-human/"},{"categories":null,"content":"How might this cause problems? Let’s say that you have a very high profile project under your GitHub account that lots of people rely on. Let’s also suppose that this is the definitive installation source. At least for Go projects, this may be true: Go developers will often host projects on GitHub and that’s the official source. Now, for whatever reason, your account gets labelled as a bot, and perhaps all of a sudden, your software is unavailable to anyone else, potentially breaking their software. If this was a major problem, I’d imagine this would get fixed pretty promptly, but still, it would be an inconvenience, and a baffling one for anyone encountering it. Again, I’m prepared to be corrected on this. If this isn’t how things work, that’s great. Maybe popular accounts, e.g. those with lots of followers, or owning projects with lots of stars, have some protection against this before their accounts are tagged in this way? If there is a possibility that this could actually happen though, then if there was some kind of short delay and an email warning before labelling your account as “not human”, at least you’d have a chance to fix up your account first, without your account temporarily becoming a Ghost of GitHub Past. ","date":"2016-09-11","objectID":"/posts/what-happens-when-github-decides-youre-not-a-human/:3:0","tags":["GitHub"],"title":"What happens when GitHub decides you're not a human","uri":"/posts/what-happens-when-github-decides-youre-not-a-human/"},{"categories":null,"content":"What’s the relevance to other online services? This kind of situation isn’t restricted specifically to GitHub. It does illustrate features of online services that are ever present, but remain latent until you encounter them. These are likely some variant of: the site owners can modify your account’s privileges, disable your account, or even delete your account and all its content without warning. this might happen without any way of you appealing the decision; that’s entirely down to the policies of the site. even if you behave in line with the site’s terms and conditions, you may inadvertently break “rules” that the site owners don’t even realise they have in place, but are instead derived by some automated machine learning system. In very much the same way that you might have experienced: has your bank ever suddenly prevented you from making a payment until you confirm the transaction? All of a sudden, you find yourself in the midst of some real life version of Mao where you lose out because you don’t even know what you did wrong. ","date":"2016-09-11","objectID":"/posts/what-happens-when-github-decides-youre-not-a-human/:4:0","tags":["GitHub"],"title":"What happens when GitHub decides you're not a human","uri":"/posts/what-happens-when-github-decides-youre-not-a-human/"},{"categories":null,"content":"More on GitHub OK, diversion aside, let’s go back to speculating about GitHub accounts. Let’s go further than hiding my account for a short time. What if my GitHub account was disabled entirely either for some period of time, or even deleted permanently? A considerable amount of my work is done via GitHub. Using an alternative would be a major inconvenience. With my employer, I’d have to figure out some solution to resolving it, enabling me to contribute again. That might mean moving to a competitor, or some self-hosted solution, or that I submit patches instead. But who’d want to deal with the overhead of working with patches from one developer when everyone else is capable of just pushing straight to GitHub? Even if you’re just coding in your free time, this is also a problem if you want to assist with any of the numerous open source projects that take contributions directly via GitHub. Another side issue if you’re not rigorous with backups, you won’t have access to the repositories that you didn’t have locally. However, GitHub do state in their terms that they will attempt to “make a reasonable effort to provide the affected account owner with a copy of their account contents upon request”. GitHub’s terms do state you can only have one account. It’s less clear whether having an account disabled precludes you from signing up again. Nonetheless, they do say that they can “refuse any and all current or future use of the Service, or any other GitHub service, for any reason at any time”. So it may be that any future accounts you create may well be subject to being disabled due to your prior history. The major online services are ubiquitous and have transformed the way many people worldwide interact and work. It’s worth remembering that your continued ability to access them is never guaranteed. If you lose access to services you use for leisure, it may well be frustrating. If you’re relying on them for work, you may find yourself dealing with big problems that you very quickly need to solve. With GitHub, I’m still not sure what my solution would be for work if GitHub permanently kicked me out from their site. As much as GitHub, historically at least, do seem to be on the side of developers, and I’ve very few complaints about the site,3 it’s still slightly concerning that this gives them a position of power. Incidentally, it’s weird thinking that it’s more than possible that to some listeners these days, Daft Punk may well only be known as “that robot band that did some songs with Pharrell”, despite having been around for more than twenty years. If, for whatever reason, you haven’t heard the Human After All album, I’d go straight to Alive 2007 where those tracks work a lot better in context, rather than the isolation of individually being played out; they’re closer to the realm of DJ tools that work magnificently live. ↩︎ Some other thoughts: I have no idea what happens to open pull requests, are they still visible? Can you create new pull requests? What happens to organisations where you’re the sole member? What happens to GitHub pages under your profile? Again, no idea. ↩︎ Out of the main competitors, I’ve used Bitbucket considerably, and prefer GitHub, although Bitbucket is better in that personal private repositories don’t require you to pay. The little use I’ve had of GitLab I’ve found very confusing. (Admittedly, that confusion may not have been helped by not being in charge of the GitLab group I was contributing to, so it was awkward trying to get permissions configured.) ↩︎ ","date":"2016-09-11","objectID":"/posts/what-happens-when-github-decides-youre-not-a-human/:5:0","tags":["GitHub"],"title":"What happens when GitHub decides you're not a human","uri":"/posts/what-happens-when-github-decides-youre-not-a-human/"},{"categories":null,"content":"Windows 10 is only a free upgrade from Windows 7 or 8.1 until the end of this month1. If you’ve hesitated so far, you might be a bit unsure as to whether you should switch a system you’re satisfied with for a potentially less satisfactory one. But, if you want a free upgrade, you haven’t got long left, so you need to decide whether to take it up very soon. Earlier this year, I installed it on one PC, partly to test Traktor out on a clean install of Windows 10, and partly to claim the free upgrade on that PC. That went OK. And, as I’m claiming free upgrades on other PCs that I’m caring for, you might think that, at least, I don’t loathe the new version enough to not switch. From what I’ve seen, that’s true, though it has a few residual problems, even almost a year after release. What I do know reasonably well is the installation process as I’ve been through that a few times now, so I’ll focus on how that compares with previous versions. Before you start, if you have an existing installation, it’s worth checking in the “Get Windows 10” app whether you have any compatibility issues. As far as I can tell, there’s no separate checker you can run. If, like me, you disabled that annoying application, you’ll want to re-enable it and see if there are any potential issues from any applications or hardware you have. ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:0:0","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"Clean installation Clean installation is a lot simpler than when Windows 10 was first launched, if you’re claiming a free upgrade. Since the Threshold release last year, you can simply enter your existing Windows 7/8 product key, and that will be activated. No need to do a upgrade over an existing installation solely to claim a digital entitlement, and then reinstall. The installation process is very similar to that of Windows 7 and 8, and simple enough to navigate. Boot up the installer, select language options, choose where you want to install the operating system, wait a few minutes and you should soon have a working Windows install. ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:1:0","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"Drivers In the past, even as recently as Windows 7, installing hardware was a mess in that you’d often have to trawl badly designed hardware manufacturer websites for several drivers you’d need to make everything work. Often, you’d be prompted to check online for drivers, only for Windows to unhelpfully say “sorry, go and find them yourself” (polite paraphrasing mine). This seems particularly backwards compared to modern Linux installs where, at least in my limited experience, most hardware works without much user intervention. In this respect, Windows 10 has caught up2. For the PCs I’ve tried Windows 10 on, almost all the hardware has been correctly detected and the drivers installed. Even on a PC near a decade old, long since abandoned by the manufacturer, it was a particularly pleasant surprise to find that almost everything had been correctly installed. The main exception I found was a Native Instruments audio interface, but I realise that’s not common hardware. ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:1:1","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"Updates One of the major drawbacks of Windows past is the update system. A fresh install of Windows 7 now requires in excess of two hundred updates, which greatly extends the time to complete the install. You’ll endure reboot on reboot, until it’s done. If it’s done. Unfortunately, it’s also possible to run into failures while installing such an inordinate number of updates; trying to install as many updates in one go to minimise rebooting sees some phenomenal memory usage. That wasn’t helped by there only being a single service pack for Windows 7, released several years ago, so you still need all updates released since then. (While you can slipstream updates into Windows installers, that’s not going to be something that many users bother with.) Here, Windows 10 seems a big improvement. Installers available from Microsoft’s site are regularly updated: the one I’ve used recently is from April of this year. This means that there shouldn’t be many more updates required for your newly installed system. On top of that, they’ve made the updates cumulative, so you only need the latest to get all fixes. Furthermore, they’re all rolled into a single update, which means there’s very little that a new system needs. The downside of this, of course, is that you can’t pick and choose what applies. The past has seen cases where rogue Windows updates can critically break systems: recently, in my experience. With Windows 10, all you can do is not apply any subsequent updates whatsoever until your issue with the updates gets resolved. Relying on the stability of a house of cards doesn’t seem that wise. More positively, there is an upside of the cumulative update system. Previously — and I’ve encountered this firsthand for XP and Windows 7 — as a particular Windows version grows older, Windows Update becomes far creakier. In recent months, Windows Update has taken the best part of an hour to check for updates on relatively modern Core i5 machines, and hours on my ancient Core2Duo PC. If this new method of rolling out updates avoids this, it will prevent much user frustration as the operating system gets older, particularly if, as Microsoft claim, this is the “last version” of Windows. ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:2:0","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"Upgrade install I’m not a huge fan of upgrade installs with Windows. Usually, if a full installation is warranted, enough time has passed that it’s often worthwhile to do a clean install. In one case I dealt with, I felt it was worth a try first as reinstalling everything would be a lot of work3. If all else failed, a clean install was still an available alternative. In fact, the upgrade install went without a problem. Downloading the installer for reuse is a smart idea if you have several PCs to upgrade. You can download a USB creator or ISO from Microsoft’s site, and run setup.exe within your current install to upgrade. The whole process didn’t take too long either and didn’t seem to cause any problems following it. Microsoft do blatantly ignore your existing choice of browser and select Edge as the default for you though… thanks for that. ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:3:0","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"And after installation? Despite it being out almost a year, I’m only really now starting to use Windows 10 on a regular basis. Where possible, I’d rather hold off if I have systems that are working well and let others find and report bugs. Even from the outset of installation, one noticeable difference to previous versions of Windows is the number of privacy options there are. Microsoft are intent on collecting much more data than in previous versions of Windows. What you should be reading is this page on Microsoft’s site that details telemetry to tell you exactly what can be disabled. ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:4:0","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"Other problems Several other things I don’t like have emerged over the few hours I’ve used Windows 10. Windows 10 sees the return of the Start menu on desktops, which was one of the big pain points of Windows 8. What still remains is a slightly uneasy mix of normal applications, and universal ones. This particularly affects settings. For example, there are two separate applications you can use to remove software: that as seen in the traditional Control Panel and another that’s a universal looking app. That just feels indecisive. Likewise, prompts are a mixture of old and new. It’s not enough to be too distracting, but does feel unpolished. Speaking of the Start menu, there are lots of things that clutter it by default. There are more bundled apps (for instance, Twitter, Skype) too which some may find useful, but I’d prefer to not have them installed by default. OneDrive integration might be desirable for some people, but not for me, so it’s disabled; the simplest way is to go to Group Policy and disable it there. The default theme is ridiculously bright. When I used it earlier this year, I found it felt tiring to work with over long periods. The Anniversary Update to be released soon finally promises to reintroduce an Aero-like theme, which should address that complaint. Another problem is that using BitLocker still requires a premium version of Windows. I have that, so it doesn’t affect me, but I don’t understand why Microsoft count what is nowadays a fundamental security feature as optional. Lots of consumer laptops will only have the Home version of Windows which doesn’t feature BitLocker. Since Windows 8.1, there's another encryption option called \"Device Encryption\". It's also really difficult to find useful documentation about this option on Microsoft's site. I did find a couple of more informative links on Microsoft's site: [\"What's New in BitLocker\"](https://technet.microsoft.com/en-us/library/dn306081(v=ws.11).aspx#BKMK_Encryption) and the [Windows 10 Security Guide](https://technet.microsoft.com/itpro/windows/keep-secure/windows-10-security-guide#information). The requirements are declared in the [Windows 10 Specifications](https://www.microsoft.com/en-us/windows/windows-10-specifications#sysreqs). Spoiler alert: \u003e Device encryption requires a PC with InstantGo and TPM 2.0. But lots of budget hardware skips on the Trusted Platform Module. And, I might be mistaken, that still doesn't cover securing removable drives for non-Pro/Enterprise users. The frequent notification to “install a language to enable typing features” gets tiring fast. As far as I can tell, the only way to disable it is bowing to its demands. If you just ignore it, it decides to reappear at regular intervals. That’s a sloppy bit of user interface design. The issue where you need to use the systempropertiesperformance fix to disable animations and other visual enhancements for non-admin users still remains. Otherwise, changes made disappear once you log out. ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:4:1","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"But Windows 10 is worth a try That’s a long list of complaints, but actually I’m more positive about Windows 10 than that might suggest. My near-decade old PC with an SSD boots Windows 10 in around twenty seconds from power on, which feels brisk. And there’s nothing I’ve seen yet that seems particularly detrimental in terms of major problems or compatibility issues. Lack of BitLocker for home users aside, there’s not much in the way of major issues. If you’ve been hesitating, it’s worth taking a look. Especially given that my understanding from reading elsewhere is that the digital entitlement may well work as follows: once you’ve claimed the Windows 10 free upgrade entitlement for a PC before the deadline, even if you revert to your previous version of Windows, the upgrade remains valid even after the deadline. That month is July 2016. And it holds unless Microsoft decide that they need a greater user uptake than they have now, and announce a deadline extension. ↩︎ I never used Windows 8 enough to know if the improvements in driver detection were there or not. ↩︎ There was a substantial customised game installation which would have taken a long time to install the addons all over again. The previous upgrade strategy from XP to 7 was to copy everything over, then reinstall whichever extras for that game required it. ↩︎ ","date":"2016-07-02","objectID":"/posts/windows-10-installation-impressions/:5:0","tags":["Windows","installation"],"title":"Windows 10 installation impressions","uri":"/posts/windows-10-installation-impressions/"},{"categories":null,"content":"Last week, I went to Sónar for a second time, and the sunshine and music there feels a hazy and distant dream today. Nonetheless, it reminded me that I had the bulk of a write-up of one of last year’s Sónar+D talks sat around for a while. It’s still relevant, because of the subject rather than the quality of my writing. So, here it is. At Sónar 2015, I sat in on part of a session discussing how to push past creative blocks while making electronic music. Dennis DeSantis, who has written a book published by Ableton on this topic, was involved in a Q\u0026A-style event to discuss it. What motivated the book? It was explained that the book came from documenting his own strategies for solving specific problems in the process of making electronic music. Furthermore, he was insistent that these were his own strategies for dealing with them, not a definite prescription for what you should do. Maybe they work, but they might not: you might need to vary them or try a completely different approach. The idea as much as anything is that, if you’re at a creative dead end, the suggestions may inspire you to mix up what you’re doing, which could reveal a new path. Even in the time I attended, the session tackled several questions. Here’s what I took away from the insightful discussions then, much of which the book covers too. (For whatever reason, despite enjoying the talk, it took me a year to get around to reading the book, and you can find my view on it below the discussion of the session.) ","date":"2016-06-26","objectID":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./:0:0","tags":["book review","music","production "],"title":"Book (and talk) review: Making Music. 74 Creative Strategies for Electronic Music Producers.","uri":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./"},{"categories":null,"content":"How to approach that fresh, empty project file? When all you have is nothing, making something from that can be difficult. It was mentioned that, if like Mozart, inspiration strikes when you go for a walk, then following that and seeing where it takes you is a good start. For those of us not blessed with the talent to both be able to conjure up wonderful music ideas away from an instrument and transcribe them into software, we perhaps need to try something else. In that case, just doing something — anything — is better than nothing, even, as was cited, just mashing the keyboard. Beginning with the elements that electronic music is frequently built on, for example, drums or bass, can help too. Building some kind of hook with one element may spark ideas to fill in the blanks with other elements. Developing and refining ideas by splitting production time into distinct creative phases is another approach. First, keep everything you make, then spend some time editing that ruthlessly. Remove anything that isn’t great. You can repeat that until you have enough material to start building up a song. ","date":"2016-06-26","objectID":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./:1:0","tags":["book review","music","production "],"title":"Book (and talk) review: Making Music. 74 Creative Strategies for Electronic Music Producers.","uri":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./"},{"categories":null,"content":"Having too many tools to choose from The problem of choice is one that surely almost everyone making music with computers has faced at some point. With a plethora of cheap or free VST plugins available, and settings for each one, it’s possible to get lost shifting faders and turning dials without getting anywhere. The advice there then is to deliberately restrict yourself. Just work with a small number of tools you already have. That said, when your VST list is large, it can actually be difficult to adhere to this. Initially sketching out parts with very basic presets only, even as simple as General MIDI instruments, was another insightful strategy. That way, you’re solely working on the composition side first and foremost, then can expand on those sketches. The idea that it’s only the lack of a perfect plugin to do X, Y or Z that’s blocking your work was dismissed as well. People were making great electronic music decades ago with much more restrictive tools than those available to us today. DeSantis pointed out that you can go on looking for the perfect compression plugin, but no need to look further: you probably already have it in your collection. ","date":"2016-06-26","objectID":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./:2:0","tags":["book review","music","production "],"title":"Book (and talk) review: Making Music. 74 Creative Strategies for Electronic Music Producers.","uri":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./"},{"categories":null,"content":"Switching software to get out of creative dead ends By contrast to focusing on certain tools you already have, switching digital audio workstation (DAW) completely was proposed — he candidly admitted that Ableton may not want him to say that. This sounds like a big leap to make, but enables you to break free of the workflow that’s dictated by the current software you’re using. By changing your DAW to one which doesn’t let you quite work as you used to, it can help break the patterns you may usually fall into, giving you fresh inspiration. Taking this idea, it was mentioned that some producers even move away from software entirely and use hardware instead. This conflicting advice reflected the idea that there are no magic solutions for sparking creativity, only suggestions. If an approach doesn’t work, you need to try something else until you find something that works. ","date":"2016-06-26","objectID":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./:3:0","tags":["book review","music","production "],"title":"Book (and talk) review: Making Music. 74 Creative Strategies for Electronic Music Producers.","uri":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./"},{"categories":null,"content":"Actively listening to existing pieces for ideas If you’re a DJ, you’re probably familiar with the idea that it can sometimes be more effective to align tracks at a higher level than just matching beats or, more commonly, bars (where the first beat of a measure for multiple tracks play together). To do this, you can listen to a piece of music and count the number of bars that constituent each part of its structure. That is, in terms of verse/chorus, or intro/build-up/drop/breakdown/outro. With this knowledge, you could organise playback so that, for example, build-ups of different tracks play at once, or a build-up coincides with a breakdown. This also gives you a structure as a framework for making a new piece in that style, too. You might opt for, say, a 16 bar intro and 32 bar build up because other music in the genre uses that. And this makes the creative process a little simpler because you already have constraints on what you need to make a track. What DeSantis proposed during the session was going even further: careful listening to build up a catalogue of attributes. By listening to a piece of music in depth and breaking it down into its key components — elements like rhythm, melody, and the sounds used — you have a instant starting point of well-defined restrictions which you can take away and use as the basis for your own work. When taking those elements outside of the context of that piece as inspiration, you’re likely to end up with something that sounds distinct from the original work, rather than a poor imitation. ","date":"2016-06-26","objectID":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./:4:0","tags":["book review","music","production "],"title":"Book (and talk) review: Making Music. 74 Creative Strategies for Electronic Music Producers.","uri":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./"},{"categories":null,"content":"Knowing when a piece of music is complete On answering this, DeSantis cited an aphorism paraphrased as “art is never finished, only abandoned” (an internet search tells me this is often attributed to Paul Valéry). It’s easy to be entangled in tweaking things forever, constantly returning to a project and slightly adjusting it. At some point, you should declare it as being “resolved”. Resolved might mean declaring it complete, or something that you’re decidedly not going to pursue further. In line with the above quote, it was noted that a work is never going to achieve perfection, but it may still be good enough. It might be that you feel instinctively that it’s done. Otherwise, if you’re at the point of diminishing returns — spending lots of time on it without making any real progress — then that’s an indicator that your time may be better spent on the next project. If the current project’s really going nowhere, then best to look ahead to the next. Deadlines were also mentioned as a great incentive for getting things finished: if you don’t finish on time, you don’t get the job. Producers who aren’t creating as a career don’t have that pressure. But you can still apply an informal deadline: perhaps aiming to finish a piece by a certain date or abandoning it otherwise could help you work with some time restriction in mind. ","date":"2016-06-26","objectID":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./:5:0","tags":["book review","music","production "],"title":"Book (and talk) review: Making Music. 74 Creative Strategies for Electronic Music Producers.","uri":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./"},{"categories":null,"content":"And how’s the book? The sample on Ableton’s site is a sizeable and representative chunk of the entire book. That alone should an idea of whether you’d find it worthwhile, without me needing to tell you. There’s much to like about the book. The chapters are concise and breezy, and cover various aspects of the music creation process from start to finish. Each chapter describes a problem and then explains a suggested way of solving it: a style that books in other disciplines could do well to consider. Learning how to accomplish an end goal is highly motivating, especially when you learn something new in a five minute read. As the book deals with a creative process, I also liked that, as mentioned above, the ideas are labelled just that, as suggestions only. There might be conventions that creators use, but there aren’t usually concrete rules in creating something new. Indeed, some of the guidance offered contradicts that offered in other chapters, representing different approaches from which you can work. It’s short enough to read through the entire book from start to end quickly, picking up a flavour of any ideas that are new to you, and then placing on a shelf to refer back to as needed. The book doesn’t always delve into those ideas in great detail. However, there’s enough there to put the idea into practice and, importantly, making you aware of the concept so that you can investigate further elsewhere. For a wide ranging audience, this is an easy read. The assumed knowledge of the reader is minimal, outside of some experience of working with digital audio workstations. For my limited background in music, this is a good thing. That said, I think some of the exercises and ideas suggested in starting to create something when inspiration is lacking may well benefit much more competent readers too. Having Ableton as book publisher is a boon for marketing the book to one producer community, but could deter users of other software from reading. It should be stressed that the ideas covered are software, hardware and even genre agnostic. There’s nothing presented that’s particularly Ableton specific. It would be a pity if fewer people read it because of that; it’s an enjoyable and enlightening read for learners. ","date":"2016-06-26","objectID":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./:6:0","tags":["book review","music","production "],"title":"Book (and talk) review: Making Music. 74 Creative Strategies for Electronic Music Producers.","uri":"/posts/book-and-talk-review-making-music.-74-creative-strategies-for-electronic-music-producers./"},{"categories":null,"content":"A while back I wrote an automatic tracklisting downloader for BBC radio shows in Python. It’s simple to use and it also hooks well into get_iplayer to retrieve or tag tracklistings when you download a radio show. One of the outstanding issues was to actually release it on PyPI — the official source for third party Python packages — to make it a bit simpler to install, and to make it easier for Python users to find. It was already packaged such that it could be installed using pip, but this was via GitHub rather than PyPI. I remembered I never got round to releasing my package on PyPI and, for some reason yesterday, thought it would be a good time to see what the process was like. Maybe it’ll only take half an hour, I thought, naively. It actually took me more like double to triple that time to complete the task. Anyway, perhaps documenting this process, along with the numerous errors I encountered, might help save anyone else running up against the same painful brick walls that I did, and allow them to share their work with the Python community. ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:0:0","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Starting out A good starting point is a guide on the official Python site that’s fairly comprehensive. The first thing you notice when reading it is that there are lots of options that are labelled “not recommended” and instead suggest using the twine package instead. This is because older versions of Python will happily submit your PyPI credentials in plaintext when uploading packages. More recent versions are OK, but you’re best to check. It’s clearly a good thing to warn people about this. But it would be much nicer if they didn’t even need such warnings in the first place. (I’m also willing to bet there’s still a non-zero number of people on affected versions of Python who follow instructions elsewhere without even being aware of sending their passwords in the clear.) ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:1:0","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Testing times As the Python packaging guide mentions, there’s a PyPI test site that you can use to test out uploading your package before you do the same on the live site. Why’s this useful? If you make mistakes in your setup and your package doesn’t install, the only real recourse is to release a new version. Rather than having to create new versions, you’re really best using the test PyPI site first to make sure everything’s OK before you begin. The test site looks to be an exact duplicate of the main site albeit with a different selection of packages. This also means that you have the same restriction as on the live site: that is, you can’t replace an existing version. You have to bump the version in setup.py and upload it as a new version. I understand why this is in place for the live site. But, it’s a little frustrating when testing: every change requires you to also increase the version number. Or, as I more often did, forget to do so, then see the error message telling you that the version already exists and only then increase the version number. The details in the guide show the structure of the .pypirc configuration. However, they don’t point out that you can also add the PyPI test site too: [distutils] index-servers= pypi testpypi [testpypi] repository = https://testpypi.python.org/pypi [pypi] repository = https://pypi.python.org/pypi You can also add your username and password to each entry too, although you may not want it lying around in plain text in a configuration file. If you don’t already have accounts on the live and test PyPI sites, this point is a good time to create them. You won’t be able to progress much further otherwise. ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:2:0","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Building the project distribution Provided you’ve got setup.py configured correctly, it’s not too difficult to build for distribution. I used the example from the PyPA sample project as a starting point. It’s also sensible to have tagged the commit in your repository that marks the version to be released. The Python packaging guide linked above is also really helpful in determining which arguments you should use with setup.py to build your package. If you’re trying to build Python wheels, one hiccup is that you likely need the wheel package installed. Otherwise, you may find with a command like: python setup.py sdist bdist_wheel you get the error: error: invalid command 'bdist_wheel' ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:2:1","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Registering the project Since twine is recommended, it seemed sensible to use that. As my package is for Python 2 only — since a dependency doesn’t support Python 3 yet — it also seemed sensible to do the distribution upload using Python 2.7. The first problem I hit was running twine register dist/* -r testpypi to register the project. Note that you need to specify the repository using -r if you want to use something other than the default, and the name is taken from your .pypirc. Registering bbc-radio-tracklisting-downloader-0.0.1.tar.gz UnicodeDecodeError: 'ascii' codec can't decode byte 0xc2 in position 27: ordinal not in range(128) That’s the entirety of the error message from the register command. There was nothing to indicate where the problem lay at all, making it very difficult to start fixing it. Switching to Python 3.4 for the upload avoided the Unicode error, but instead gave me: HTTPError: 401 Client Error: You must login to access this feature for url: https://pypi.python.org/pypi It still could have been a problem with the way I was inputting my password. I’d tried both typing it at the command line -p PASSWORD and pasting it into the prompt which appears if no password is entered. I’d even tried entering it directly into .pypirc. It’s possible I was just doing something wrong, but, to me, it implies that twine wasn’t correctly handling one of the characters in my password. At this stage, I gave up trying at this point and went directly to the test site to register the package so I could at least make some progress and feel slightly less incompetent. That worked without a hitch. ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:2:2","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Adding a description On to the next problem. Unfortunately, the PyPI site doesn’t support the display of Markdown READMEs for package descriptions. So, before you actually upload a package, you should provide the package description in a form that PyPI can use. To me, Markdown always seemed much simpler than reStructuredText (reST), so I naturally gravitate to writing documentation in it. My README for this project was no different. The main options I was aware of for solving this were: specifying a long_description in your setup.py which uses reST markup, whether read in from a file, or included in full in the setup.py itself, or converting README.md to a reStructuredText README.rst. I went for the latter for simplicity. Using pandoc, it’s pretty simple to convert an existing Markdown file: pandoc --from=markdown --to=rst --output=README.rst README.md On checking the output from pandoc in an online viewer, everything looked the same as it did when using Markdown. I also edited the README slightly to use nicer looking markup for headings than the source Pandoc generated. ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:2:3","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Uploading Whew! Back to twine, and now the project’s registered, let’s try uploading it via: twine upload dist/* -r testpypi I was still having the same problem as before: the 401 error. Systematically removing unusual characters from my password until the upload worked fixed this. (Note that the PyPI site itself was OK with me logging in with the same password, this seemed a twine problem, or something I was just doing wrong.) OK. Now it’s uploaded, let’s check what everything looks like on the test PyPI site. What? Where’s the description taken from the README? There’s nothing there at all. The README’s not even present as plain text instead of formatted HTML! What I then did first was to replace my README.rst with one borrowed from someone else’s package that I knew displayed correctly on PyPI. Then, I uploaded a new version of my package to the test site to make sure that I had everything configured and uploaded correctly. Once I’d proved that the problem was with the content of my README, I somehow instinctively figured that it was the transition markers that I’d added that were to blame. Again, this was the second time in this entire process that I had little or no indication of where the problem actually was. Anyway, removing these transition markers, running setup.py and uploading yet again finally enabled my README to be correctly displayed as the package description. ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:2:4","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Releasing Once that worked, all I had to do was revert the change to the version number I’d made in setup.py during testing, and then repeat the whole process on the real PyPI site. Having negotiated all the obstacles the first time, I was relieved to find this just worked. The end result of all that is that you can finally do pip install bbc-radio-tracklisting-downloader instead of having to install it from GitHub. Obviously in a process like this, there’s going to be a couple of configuration hoops to jump through, especially the first time you attempt the process. Now I’ve been through it, I could actually probably release a new project in the thirty minutes I originally estimated. On the other hand, the number of barriers I faced, along with the cases of little or no helpful error information was frustrating. Every stumbling block encountered is a point where someone could decide that they’ve had finally enough and just give up. In those cases, it’s the community’s loss. Developers who may well have useful packages to contribute may ultimately decide that it’s not worth the trouble to release on PyPI. ","date":"2016-05-15","objectID":"/posts/pypi-releasing-frustration/:2:5","tags":["Python","PyPI"],"title":"PyPI: releasing frustration ","uri":"/posts/pypi-releasing-frustration/"},{"categories":null,"content":"Snap happy Firefox has a buried feature in its Developer Tools, which is potentially useful even if you’re not a developer. If you’ve never used the Developer Tools, just press F12 and you’ll see the tools. You should see a cog icon towards the upper right of the pane that’s just displayed. The cog is on the same row as the Inspector, Console etc. buttons. Clicking this displays a page of options that you can use to customise the Developer Tools. By ticking “Take a fullpage screenshot” under “Available Toolbox Buttons”, you should see a camera icon appear near the cog icon you previously clicked. Selecting this gives you a screenshot of the page where you opened the Developer Tools. The resulting PNG file can be found in your Firefox Downloads. This is a really handy feature. It means that you can take an image of a scrollable page that’s larger than your current window. No need for screenshotting parts of the page then joining them together by hand. ","date":"2016-04-20","objectID":"/posts/taking-web-page-screenshots-smartly-with-firefoxs-developer-tools/:1:0","tags":["Firefox","screenshot "],"title":"Taking web page screenshots smartly with Firefox's Developer Tools","uri":"/posts/taking-web-page-screenshots-smartly-with-firefoxs-developer-tools/"},{"categories":null,"content":"Solving bigger problems But what if you want to enlarge the visible area beyond the size of your current window, and then take a screenshot? Why would this be even useful? Let’s say that you want a screenshot of a web page displaying a map, where the map expands to fill your browser window. And let’s say that you want the screenshot to cover a larger area than fits into the largest browser window you can have on your desktop. Just expanding the window isn’t going to give you the size that you need. How can you do this? If you hover over the toolbox icons in the Developer Tools panel, near to the cog, you should be able to find the Responsive Design mode. (It’s a very small, dotted rectangle inside a larger, thicker rectangle.) Click this. Your display will change to have a border and extra options appear. The selected resolution is shown above the display. Clicking on the resolution dropdown menu, you can directly type the resolution you want this preview display to have and press Enter to have it take effect. All that’s left is to take a screenshot again using the “Take a fullpage screenshot” option in the Developer Tools toolbox. (Note that Responsive Design mode has its own screenshot tool, near to the resolution menu, but this only takes an image of the page visible within the display window you’ve just set, i.e. it’s not necessarily the full page if there would still be scroll bars at that actual display resolution you’ve selected.) ","date":"2016-04-20","objectID":"/posts/taking-web-page-screenshots-smartly-with-firefoxs-developer-tools/:2:0","tags":["Firefox","screenshot "],"title":"Taking web page screenshots smartly with Firefox's Developer Tools","uri":"/posts/taking-web-page-screenshots-smartly-with-firefoxs-developer-tools/"},{"categories":null,"content":"Kicking the Bitbucket What I noticed this week is that users who’d left a Bitbucket team1 that I’m currently a member of were still billed as having admin access to several repositories. But, this wasn’t by virtue of being a team member, as we’d removed them. Instead, this was via their own user accounts. This seems like the intended behaviour. It’s documented here: a user gets admin access to the repository when they create it. I tested this by just creating a new repository that was owned by the team. Checking the access management settings showed that the team was the owner of the repository, but I had separate admin permissions for the repository. If you’ve got the required permissions yourself, you can go and clean up user access on a repository basis, via that same repository access management settings. A short look around — though short does mean very short, admittedly — for suitable bits of the API that would let me purge old users without having to manually click through each repository didn’t reveal anything. Click, click, click seemed the only way to fix it. And if you have a lot of repositories, that’s a lot of clicking. ","date":"2016-04-16","objectID":"/posts/managing-a-bitbucket-users-permissions-when-theyve-left-your-team/:1:0","tags":["Bitbucket"],"title":"Managing a Bitbucket user's permissions when they've left your team","uri":"/posts/managing-a-bitbucket-users-permissions-when-theyve-left-your-team/"},{"categories":null,"content":"Stop it (and tidy up) Looking now at this late hour, it seems that I was looking in entirely the wrong place previously. A much, much quicker route to cleaning up is to browse to your team’s page and select “Manage team”. From there, look at “Plan details”. There you can see exactly who has access to your team’s repositories and what access type they have. This might be “group”. That is, because they’re part of a user group in your team — groups are another aspect of access management, you can be in the team, but maybe you’re in a group that maybe doesn’t have access to any team repositories at all. A user’s access type can also be “repository”, i.e. they have access because their user account has explicitly set permissions on that repository. Clicking “View access” shows you which team repositories they can access. Next to “View access”, there’s a cross icon that you can click to remove all the user’s permissions from your team. In just one click. What happens if you do that for a user that’s still on the team? I can’t answer that; maybe it boots them out of the team too? (I wasn’t about to try it.) ","date":"2016-04-16","objectID":"/posts/managing-a-bitbucket-users-permissions-when-theyve-left-your-team/:2:0","tags":["Bitbucket"],"title":"Managing a Bitbucket user's permissions when they've left your team","uri":"/posts/managing-a-bitbucket-users-permissions-when-theyve-left-your-team/"},{"categories":null,"content":"A potentially money saving tip It’s sensible to keep access permissions to your repositories to a minimum to ensure that only users who need access have it. Of course, it may be that you have equally sensible reasons to retain access for ex-team members. However, it’s also worth considering how Bitbucket counts users in your team’s plan. The definition of users on the Bitbucket plan details page is: The number of users (including you) with any level of access to one or more of your private repos. This is whether they are a formal team member or not. (Team members without repository access don’t count here.) So, this spring cleaning might allow you to switch to a cheaper Bitbucket plan for your team too. Organisation in GitHub parlance. ↩︎ ","date":"2016-04-16","objectID":"/posts/managing-a-bitbucket-users-permissions-when-theyve-left-your-team/:3:0","tags":["Bitbucket"],"title":"Managing a Bitbucket user's permissions when they've left your team","uri":"/posts/managing-a-bitbucket-users-permissions-when-theyve-left-your-team/"},{"categories":null,"content":"Because, obviously, I have nothing I’d rather do than try to fathom how Secure Boot works on a Monday evening, I’m writing this up. Maybe you’re experiencing this problem right now. Maybe you’re not, but you’re intrigued as to how I go about diagnosing computer problems. ","date":"2016-04-12","objectID":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/:0:0","tags":["Windows","Secure Boot"],"title":"The case of a Windows 7 update, Secure Boot and a suspect motherboard ","uri":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/"},{"categories":null,"content":"A stubborn PC I maintain a PC that has Windows 7 installed (due to be upgraded to Windows 10) and, yesterday evening, installed several optional updates.1 Nothing unusual there. A day later, an owner of the machine restarted it. Except, well, the PC refused. Instead it said this: The system found unauthorized changes on the firmware, operating system or UEFI drivers. Hmm. That’s certainly not good since it’s related to Secure Boot’s check of boot integrity. Warily, I disabled Secure Boot in the BIOS to get the machine to boot Windows, and checked the System Event Log. Looking around the log, it turns out that the last time that the machine had previously successfully started was just before I installed the updates, which is consistent with what had happened. As the PC was fine until that point, it ruled out anything malicious, and pointed to the update as being a likely candidate. Having looked through the list of recently installed updates, most of them seemed fairly routine, except the KB3133977 update for a Bitlocker issue. What stood out were the changes to Windows Boot Manager files Bootmgr.efi and Bootmgfw.efi, which also suggested the update was at fault. ","date":"2016-04-12","objectID":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/:1:0","tags":["Windows","Secure Boot"],"title":"The case of a Windows 7 update, Secure Boot and a suspect motherboard ","uri":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/"},{"categories":null,"content":"ASUSpect? Searching around led me to several recent links, as I mentioned in comments on this SuperUser question. In those posts, Windows users were reporting the same issue: the BIOS complaining that there was an unauthorised change. This being a relatively new issue isn’t surprising as the update was only released on March 14th, 2016. ASUS being the motherboard manufacturer seems to be common to those experiencing the problem. It seems to affect different models; here it’s a H87-PRO. Even then it’s unclear what the root cause is: maybe ASUS motherboards are the only ones that allow you to enable Secure Boot with Windows 7? maybe there’s something unusual with the signing process of this update that only affected ASUS motherboards? or maybe it’s just a coincidence? (But, it’s odd that it’s a common link in several cases. You’d think if users with other motherboards would have been affected, they’d have mentioned it too.) As linked above, Microsoft’s site suggests Secure Boot is supported on Windows 8 and up, and I can’t find any documentation to suggest Secure Boot should work on Windows 7. It’s possible that this is just behaviour out of specification that didn’t cause any problem until now. !!! article-edit \"\" Edit 2016-05-15: It turns out that it was a bad configuration on the part of ASUS as documented by their FAQ. Judging from the news stories published around 4th May 2016 and onwards, that note has only been released by ASUS this month. Pretty slow response, really, seeing as this problem’s been known for several weeks. Their fix is one of those already suggested here: disable Secure Boot. (It’s a while ago since I installed the PC, but I guess that I originally must have tried turning on Secure Boot during installation and, since the PC booted without a problem, left it enabled.) ","date":"2016-04-12","objectID":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/:2:0","tags":["Windows","Secure Boot"],"title":"The case of a Windows 7 update, Secure Boot and a suspect motherboard ","uri":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/"},{"categories":null,"content":"What to do? Other posts I’d read indicated that removing KB3133977 didn’t solve this. Perhaps there are EFI changes that don’t get reverted when you uninstall the update? Who knows? However, restoring a previous system backup fixes the issue. You could then leave everything as is, but have to make sure that you don’t install this update (hiding it would work). This is inconvenient then, but fine provided you have regular backups. Oh, but the update might actually fix a problem you’re experiencing with Bitlocker. In which case, you’re going to have to install it regardless then, aren’t you? Another solution is to disable Secure Boot in the BIOS. That’s not a perfect solution though, especially if you’re booting other operating systems that work with Secure Boot. It’s always possible, but unlikely, that Microsoft could reissue the update and enabling the Secure Boot option might be possible again. None of that explains completely what happened, but that’s at least enough to hopefully stop you panicking and make your Windows install accessible, if you’ve been affected. What fun will today’s later batch of updates brings, I wonder? !!! article-edit \"\" Edit 2016-05-20: Marc-Andre Renaud kindly emailed me to share his experience: \u003e Your hunch about deleting keys while booting in UEFI mode possibly \u003e fixing this was correct. I deleted the keys on my ASUS Sabertooth \u003e z97 motherboard and was able to boot. ","date":"2016-04-12","objectID":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/:3:0","tags":["Windows","Secure Boot"],"title":"The case of a Windows 7 update, Secure Boot and a suspect motherboard ","uri":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/"},{"categories":null,"content":"One more thing Incidentally, I wondered what happens if you disable Secure Boot, then do an in-place upgrade of Windows in this situation, where we can’t enable Secure Boot? Can you later enable it? My hunch is that you should be able to, provided your operating system was installed when your BIOS was in UEFI mode. With the ASUS motherboard I was looking at, if you were doing an in-place upgrade in this particularly strange case, I think you’d need to clear the Secure Boot keys to disable Secure Boot while allowing UEFI (otherwise you wouldn’t be able to boot Windows 7). Once the upgrade’s complete, I’d then expect I could go back to the BIOS and install the default Secure Boot keys, which should let you set Secure Boot again. !!! article-edit \"\" Edit September 2016: Having tested this on a PC with this exact problem, the answer is yes, this works fine. This Windows 7 was installed in UEFI mode, had Secure Boot enabled until this ASUS problem occurred earlier this year, and was then upgraded to Windows 10 with Secure Boot disabled. Enabling Secure Boot after the upgrade was successful. Previously, I had recommended updates set to automatically install, but unfortunately it’s now necessary to make sure you don’t inadvertently install Windows 10. ↩︎ ","date":"2016-04-12","objectID":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/:4:0","tags":["Windows","Secure Boot"],"title":"The case of a Windows 7 update, Secure Boot and a suspect motherboard ","uri":"/posts/the-case-of-a-windows-7-update-secure-boot-and-a-suspect-motherboard/"},{"categories":null,"content":"What are keyword searches? Keyword searches, or quick searches, are really handy when browsing. If you think of something that you want to search for, they remove the extra step of first navigating to the site and selecting the search box. Instead, you can just directly enter your query into your address bar, preceded by some keyword. For instance, if I look up “Ubuntu” on DuckDuckGo, I can just hit Ctrl+L to access the address bar, type d Ubuntu and hit enter, to directly access the page of search results. ","date":"2016-04-03","objectID":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/:1:0","tags":["Firefox","keyword search","quick search"],"title":"Firefox keyword searches, and how to fix adding them","uri":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/"},{"categories":null,"content":"How keyword searches save time This might not seem like a big time saving, but when you consider that you have to repeat the same process for every site you search on, it soon adds up. It’s a feature you can use not only with traditional search engines, but with most sites that have a search box. One example is YouTube, I’ll often — maybe too often — type yt Yet another timewasting distraction. ","date":"2016-04-03","objectID":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/:2:0","tags":["Firefox","keyword search","quick search"],"title":"Firefox keyword searches, and how to fix adding them","uri":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/"},{"categories":null,"content":"Creating and using custom keyword searches in Firefox This is really simple to do. You’re really just adding a bookmark, but one that refers to whatever URL the search box will send you to, not the page itself. Right click a search box on a webpage. Select “Add a Keyword for this Search…” In the dialogue that opens, enter a keyword and the search URL is then added to your bookmarks with a placeholder for a query. When you next type that keyword followed by a search query into the address bar, the bookmark gets loaded with your query automatically inserted into the URL. These keyword searches are normal bookmarks; you can edit them like any other bookmark. The only difference is that they contain the %s placeholder in the URL. When you carry out a keyword search, the term following the keyword replaces the %s in the bookmark. If you’re browsing through your bookmarks in the Library, you’ll only see “Name”, “Location” and “Tags”, but if you click “More”, you’ll see (and can change) the keyword you’ve set too. (This all works because, as you’ve probably noticed, using a search option on a site usually sends you to a URL that contains the search term. Hopefully you can see that if you construct the URL without even visiting the site first, that this should still work. That said, this assumes that the site uses GET requests where the query is contained in the URL itself.) ","date":"2016-04-03","objectID":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/:3:0","tags":["Firefox","keyword search","quick search"],"title":"Firefox keyword searches, and how to fix adding them","uri":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/"},{"categories":null,"content":"But my keyword searches aren’t being added? As I find them so useful, I sorely missed it when recently, for whatever reason, Firefox decided that it wouldn’t add them any more. The bookmark would get created, but the keyword I added didn’t get saved. Even if I manually edited the bookmark, I could enter a keyword but on navigating away to another bookmark and back again, the keyword would be lost again. The simple fix was just to exit Firefox, locate places.sqlite inside the Firefox profile directory,1 rename places.sqlite, and let the file be rebuilt on the next restart (as documented in Mozilla’s support pages). The documentation states that you lose history, but any bookmarks are rebuilt from a backup. Once you’re happy everything’s OK, you can delete the old places.sqlite. After that, new keyword searches were added without a problem. Its location depends on your operating system. For me, on Linux, it was in the hidden ~/.mozilla/firefox. On Windows, it’ll be in %APPDATA%\\Mozilla\\Firefox\\Profiles\\. Make sure that you’re in the correct profile directory, if you have multiple Firefox profiles. ↩︎ ","date":"2016-04-03","objectID":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/:4:0","tags":["Firefox","keyword search","quick search"],"title":"Firefox keyword searches, and how to fix adding them","uri":"/posts/firefox-keyword-searches-and-how-to-fix-adding-them/"},{"categories":null,"content":"With a Windows 10 1511 Threshold 2 ISO that I downloaded (using the source I’ve previously described), I was recently testing out Windows 10. The Threshold 2 release was the first major release of Windows 10 since launch and was made available in November 2015. Previously, the route to claiming the free Windows 10 upgrade from an existing version of Windows was to do an in-place upgrade from a current install. The Threshold 2 release changes this so that you can enter a Windows 7 or 8 product key on a clean install of Windows 10 and successfully activate it. But I still had a lot of questions about how the activation process worked and, probably because Threshold 2 is still fairly new, I couldn’t find much to tell me how this was actually going to work out. Here’s what I learned. ","date":"2016-01-12","objectID":"/posts/answers-to-some-windows-10-activation-questions/:0:0","tags":["Windows","activation"],"title":"Answers to some Windows 10 activation questions","uri":"/posts/answers-to-some-windows-10-activation-questions/"},{"categories":null,"content":"What happens if you clean install Windows 10 on a PC with an embedded Windows licence? These days, Windows licences for mass produced consumer systems are usually included in the BIOS itself, rather than printed on a sticker. What the Windows 10 install does is automatically installs and activates the corresponding Windows 10 version if your hardware has an embedded licence. (Pro if your laptop had a Win 7 Pro/Ultimate or Win 8 Pro licence, or Home if you had a Win 7/8 Home licence.) I suspect that this is still counted as part of the free upgrade entitlement rather than offered in perpetuity, so won’t be an option once Microsoft’s free upgrade offer expires in July 2016. You’ll probably have to pay for an upgrade then. ","date":"2016-01-12","objectID":"/posts/answers-to-some-windows-10-activation-questions/:1:0","tags":["Windows","activation"],"title":"Answers to some Windows 10 activation questions","uri":"/posts/answers-to-some-windows-10-activation-questions/"},{"categories":null,"content":"Can you use a Windows 8/8.1 Pro Pack key to claim a Windows 10 Pro upgrade? Yes. You can go to “change product key” and try entering your key, but you’ll probably see the error 0xc004f210. This is because you’re on Home, not Pro, so the key’s not valid for that version of Windows. What works instead is to use the generic Pro key which will then install the Pro upgrade. Your Windows install will then show as not being activated as it only had a Home licence. Now, entering that Pro Pack key should activate without any problem, I think, provided that it was previously used on the same hardware. Clarifying this with how this worked for me might help explain this process if it’s still somewhat unclear. My laptop had a Windows 8 Home licence which got upgraded to Windows 10 Home automatically on install. Then, I changed the product key to the generic Pro key. Following the Pro upgrade, the Windows 8 Pro Pack key that I’d previously used on the same laptop activated Windows 10 Pro. ","date":"2016-01-12","objectID":"/posts/answers-to-some-windows-10-activation-questions/:2:0","tags":["Windows","activation"],"title":"Answers to some Windows 10 activation questions","uri":"/posts/answers-to-some-windows-10-activation-questions/"},{"categories":null,"content":"Can you clean install Windows 10 Pro on a laptop which has an embedded Home licence? Once you’ve activated Windows 10, you get a digital entitlement. Any product key you first used to activate is no longer needed: in future, the Windows activation servers just check your hardware and see that you have a valid licence. The issue here is that — even if your hardware has a entitlement for the Pro version — if the hardware has a key for a Home version, that will be recognised on install and you’ll end up with a Home version install. You’d then have to enter the generic Pro key and hopefully your PC should activate without a problem then, since it should have the digital entitlement. This is inconvenient: it does mean you don’t get a clean Pro install, but one that’s upgraded from Home to Pro (though in practice there’s probably little difference between these two routes). I haven’t tested this, but as far as I can tell, the solution is to modify the installer disk image with a text file that contains a product key. If you use the generic Pro key, you should get the Pro version installed. From there, you could enter your Windows 7/8 product key as described above or, if reinstalling, the digital entitlement should allow you to activate without entering any product key. ","date":"2016-01-12","objectID":"/posts/answers-to-some-windows-10-activation-questions/:3:0","tags":["Windows","activation"],"title":"Answers to some Windows 10 activation questions","uri":"/posts/answers-to-some-windows-10-activation-questions/"},{"categories":null,"content":"First impressions of Windows 10 So far, I’ve noticed that the start menu is an improvement on Windows 8, but not as good as the one in Windows 7, the default theme is awful to look at compared with Windows 7’s Aero — it’s far too white and bright, with very little contrast in Explorer and Control Panel windows — and that there are lots of telemetry tracking and unwanted bundled software (e.g. OneDrive) to remove. There is a Microsoft guide on how to do so. Set aside some time. It’s a lengthy read. Activation confusion aside, Windows 10 installed without a problem. The installation process has been streamlined to require very little interaction beyond picking install language. Another pleasant surprise was that every piece of built-in laptop hardware had drivers installed automatically which saves a lot of delving into badly designed manufacturer websites to retrieve them. Even on Windows 7, a clean install would usually result in a lot of small pieces of hardware having missing drivers. And, in the brief testing I’ve done running Traktor, Windows 10 seemed stable enough. ","date":"2016-01-12","objectID":"/posts/answers-to-some-windows-10-activation-questions/:4:0","tags":["Windows","activation"],"title":"Answers to some Windows 10 activation questions","uri":"/posts/answers-to-some-windows-10-activation-questions/"},{"categories":null,"content":"!!! article-edit \"\" Edit September 2016: What I didn’t mention here was how you could get EPO data when on Linux. This can help reduce the time it takes for your watch to get a GPS signal lock. There’s a helpful blog post that gives a lot of the details. There’s a Ruby utility that manages FIT files and features an option to collect the EPO data from Garmin’s servers. Alternatively, I also wrote a Go program (which just compiles to a single binary rather than needing lots of dependencies) to collect this data which I’ve been using on Linux; you should be able to compile it easily to work on OS X or Windows. I’m now the owner of a Garmin GPS watch. When I was looking at GPS watches, I couldn’t find a huge amount of information on using them with Linux. This post concerns the Forerunner 15 (FR15) particularly, but think may well apply to several others in that range that function as USB storage devices, with the usual “your mileage may vary” caveats. ","date":"2015-12-30","objectID":"/posts/using-garmin-forerunner-watches-with-linux/:0:0","tags":["Garmin","Forerunner","Linux","GPS","running"],"title":"Using Garmin Forerunner watches with Linux","uri":"/posts/using-garmin-forerunner-watches-with-linux/"},{"categories":null,"content":"Can you use it with Linux? Yes, to some extent. Though you won’t be able to use all the fancy display features on Garmin’s site. That said, I’m not bothered about that and would actually prefer to keep my data offline, especially with the number of website data breaches there have been recently. The FR15 shows up as a normal USB storage device so you can copy over .FIT files from the watch directly and work with them on your computer. My run data was stored in GARMIN/ACTIVITY. Note that, when researching this, you may read older posts about the unofficial Garmin Communication web browser plugin that was capable of uploading data to Garmin’s servers. This no longer functions and this GitHub issue suggests it won’t be fixed anytime soon. ","date":"2015-12-30","objectID":"/posts/using-garmin-forerunner-watches-with-linux/:1:0","tags":["Garmin","Forerunner","Linux","GPS","running"],"title":"Using Garmin Forerunner watches with Linux","uri":"/posts/using-garmin-forerunner-watches-with-linux/"},{"categories":null,"content":"Installing GPSBabel The next job is to get the FIT files into a format that’s more widely usable. GPSBabel is a application for Windows, OS X and Linux that lets you convert between GPS data formats, including converting Garmin’s FIT (Flexible and Interoperable Data Transfer) format into .GPX (GPS Exchange Format) which is well supported by various free, open source applications. Support for FIT was added to GPSBabel in versions 1.4.3 and later, judging from the GPSBabel changelog. If your distribution’s repositories have an older version, you’ll need to compile it from source. In Ubuntu, it was reasonably straightforward to clone it from GitHub and then do the following: ./configure --prefix=$HOME/.local make make install Using prefix in configure allows you to install to a local directory and avoids the need for sudo in make install. The only hiccup was that the configure step gave the error message: configure: error: Qt4 or Qt5 is required, but neither was found The fix was to do the following: sudo apt-get install qtcreator which was probably overkill as it installed a lot of packages, but compiling then worked. (I then removed qtcreator via apt-get remove.) ","date":"2015-12-30","objectID":"/posts/using-garmin-forerunner-watches-with-linux/:1:1","tags":["Garmin","Forerunner","Linux","GPS","running"],"title":"Using Garmin Forerunner watches with Linux","uri":"/posts/using-garmin-forerunner-watches-with-linux/"},{"categories":null,"content":"Converting Garmin’s FIT data to GPX An example use of GPSBabel is: gpsbabel -i garmin_fit -f input.FIT -o gpx -F output.gpx This gives you a GPX file which can be loaded into far more software. As a quick test, I loaded the output GPX file into Viking which seems a little bit limited, but at least let me visualise speed against time, and see where I’ve been. There are web apps out there that will save you this hassle and do the conversion for you, but, again, I’d rather keep my location data to myself. (Some of the high-end watches also have the ability to send the data to Garmin via Bluetooth to mobile apps that collect the data, so that’s another way you could avoid the requirement for Windows or OS X.) ","date":"2015-12-30","objectID":"/posts/using-garmin-forerunner-watches-with-linux/:1:2","tags":["Garmin","Forerunner","Linux","GPS","running"],"title":"Using Garmin Forerunner watches with Linux","uri":"/posts/using-garmin-forerunner-watches-with-linux/"},{"categories":null,"content":"Upgrading the watch’s firmware My watch came with the latest firmware installed and the watch has been out a while, so I doubt there are any major issues remaining. But, there may still be further updates as it’s still a supported product. The official update method is via Garmin’s Windows or OS X software. If you search their site, you can find that updates exist, but you’re told to use Garmin Express to install them. However, it is possible to find download URLs for firmware updates, if you look in the HTML source. For instance, the current FR15 firmware is available via direct download using any client you like, albeit via insecure http. (I haven’t tested the official software, it may well use the same links.) I haven’t needed this information yet, but there are more details regarding a different model on the Garmin forums, but may well be relevant here. As a post there states, you may need to remove 60 bytes of an extraneous header using tail: To verify that an original file needs this header-removing, open it with hexdump -C [Download FileName] | head. You should see it starting with the string KpGrd The main watch firmware has two of those KpGrd headers, but we only remove the first one. Also bear in mind that there may be multiple firmware files to update, e.g. the watch firmware itself, as well as language files and the ANT firmware. And apparently all these need the header removing. Anyway, you can search Garmin’s site for e.g. “Forerunner 15 update” and see what you find. ","date":"2015-12-30","objectID":"/posts/using-garmin-forerunner-watches-with-linux/:1:3","tags":["Garmin","Forerunner","Linux","GPS","running"],"title":"Using Garmin Forerunner watches with Linux","uri":"/posts/using-garmin-forerunner-watches-with-linux/"},{"categories":null,"content":"What’s the watch actually like? I’ve only started using it today, but it seems good at what it does. I tracked a run and, at a glance, the data looked consistent with where I’d been and how I’d run. It’s a basic model, but capable. It’s got what seems like a decent battery, apparently eight hours in active GPS use. Anecdotally, I headed out taking it straight from the box without charging and it was flashing an empty battery warning for most of the run (an hour or so), but still managed to record the entire distance. It’s also waterproof which was handy since I was caught up in a heavy downpour while out. And it didn’t feel noticeably less comfortable or more cumbersome than the (near-weightless) Casio digital watch that I’m usually wearing. There are also activity tracker features available to count steps and calories, though I’m less concerned about those. Recording a run for later inspection, tracking my heart rate, distance travelled and time elapsed while running, and keeping my best times are the features I really want. And the FR15 does all of these things. The FR15 was on offer when I bought it. Would I have paid the full retail price? I don’t know. At that price and adding on an optional heart rate monitor bundled, it’s very similar in price to higher-end watches like Garmin’s 225. On the other hand, I did look into more expensive watches and all of them, at least the Garmin watches, seemed to have drawbacks. There wasn’t an obvious candidate that stood out as the one to get. As a starter watch, the FR15 seems like a good deal, especially as it seems we’re in a time where the technology’s still improving. (Perhaps indicative of that, Garmin, much to the chagrin of unlucky purchasers, released a newer version of their FR225 only a few months after they launched it.) There’s also competition from the likes of Fitbit and Jawbone which will probably push improvements to these devices in future. Newer, more expensive watches are already switching over to wrist monitoring of heart rate instead of using more traditional heart rate bands. So, if this is technology that I use long term, then the next step would be to a watch featuring wrist heart rate monitoring and cadence measurement. Nonetheless, the expandability of the FR15 is a bonus: you can use a separate ANT heart rate monitor (it optionally is bundled with one), as well as a footpod. Excluding these extras keeps the price down, but it’s still possible to augment the device with these features in future; I’ve already ordered a heart rate monitor. The one downside of the design I’ve seen so far is that I think the battery will be a chore to replace when it expires. It’s not officially user-replaceable. That drawback aside, it seems a decent choice. ","date":"2015-12-30","objectID":"/posts/using-garmin-forerunner-watches-with-linux/:2:0","tags":["Garmin","Forerunner","Linux","GPS","running"],"title":"Using Garmin Forerunner watches with Linux","uri":"/posts/using-garmin-forerunner-watches-with-linux/"},{"categories":null,"content":"Tracks that beat Traktor’s beat detection Traktor’s not always perfect in automatic beat detection. I was trying to set the correct beats per minute (bpm) value for a particular track and not having a lot of luck with the tap function that Traktor provides. The estimated bpm kept jumping around on each click. If you’re not familiar, a tap feature is common in music software for beat detection: tap a button along to the beat of the music, and after a few clicks, you should have a fairly good estimation of what the bpm is. Another option you have is to manually adjust the beat grid to set the bpm; the beat grid is a series of line markers that you want to mark the track’s beats. It acts as a visual guide for the user that shows whether the bpm is set correctly. This is a chore as it requires fine adjustment. You’ll end up clicking a lot to stretch or compress the grid and correct it. Also, if you’re zoomed in to precisely align this, you can’t see if everything’s still OK further along the track without going back and forth, like Cameo. ","date":"2015-12-21","objectID":"/posts/beatmatching-to-beat-grid-in-traktor/:1:0","tags":["Traktor","beatmatching","DJ","beat grid"],"title":"Beatmatching to beat grid in Traktor ","uri":"/posts/beatmatching-to-beat-grid-in-traktor/"},{"categories":null,"content":"Beatmatching to find the bpm The last time I was contemplating doing this I realised that if you can beatmatch by ear, then to find an unknown track’s bpm, you can simply beatmatch to a track with a known good bpm. It’s more fun too as you’re actually listening to the music and DJing, rather than click-click-clicking like you’re gold farming in some video game. Once you’re happy that the tracks are beatmatched, if you have the current bpm for the known good track displayed — you can change what’s displayed in each deck’s heading in Preferences — then that should give you the correct bpm for the track with unknown bpm. You can then type this in where the bpm is displayed on the “Grid” panel for the deck playing the track. All that’s left to do is seek to the first downbeat and add that as a beat grid cue point (use the “Cue” panel for the deck) and the track should be ready for beatmatching. Traktor usually does a decent job on finding that downbeat though. That would be all, except you might only have a correct bpm for the particular section you’ve been listening to: it’s possible that the bpm fluctuates. If the beat detection fails, this is quite a common problem, particularly for older pieces of music which weren’t created digitally and may drift slightly due to human timing. A fix for this is to just work with a loop in the track which does have a sufficiently steady bpm to mix. You can add loops in the “Cue” panel. While I’m here, I should wistfully wish that Native Instruments would improve the beat matching aspect. Using multiple beat markers, Ableton’s capable of adjusting — “warping” they call it — the track to fix it such that it has a consistent tempo, something that Traktor lacks. So, I’m still saving for Ableton. ","date":"2015-12-21","objectID":"/posts/beatmatching-to-beat-grid-in-traktor/:2:0","tags":["Traktor","beatmatching","DJ","beat grid"],"title":"Beatmatching to beat grid in Traktor ","uri":"/posts/beatmatching-to-beat-grid-in-traktor/"},{"categories":null,"content":"Two factor or not two factor We’re living in the future right now: it’s almost 2016. Almost as unbelievably, 123-reg still haven’t implemented any kind of two factor authentication for their site. From their own blog: Two-factor authentication adds another layer of protection and you should use it where it is available. Well, yeah, I would. If I could. It’s stupid for me to not be using a registrar that provides two factor authentication. Get control of the domain and you could hijack my website or my email. It’s also stupid on their part not to implement this. It’s a security feature I expect as standard on important web services these days. And I’m sure that I’m not the only person who’s decided to leave or not use their service as a result. Moreover, the implication of no two factor support does have me questioning what else they can’t be bothered to do security-wise. Apart from this issue, I wouldn’t say I’d be exactly content, but I’d have the apathy towards transferring my domain out that many people have towards switching bank accounts, rather than any more urgent impetus to leave. As I don’t ever really use the 123-reg site itself, I don’t really have a problem with them not redesigning it in years. The only major outage I’ve knowingly experienced in over 12 years(!) of being with them is that email was hideously delayed a few years back. However, when reading about domain transfer, I also learned of some recent moneygrabbing tricks they’ve tried.1 Searching around for other domain registrars, it seems there is no single standout provider. You can find complaints about every one, so it’s a bit of a crapshoot in choosing one. Namecheap were, true to their branding, a name that was reasonably cheap. And, they appeared to have no more complaints than any other provider. But as a prelude to committing to moving, I figured I’d buy some other domain names with Namecheap and see what I made of the site. My first impressions were that the initial signup and buying services from Namecheap was pretty easy. Their dashboard also simple to navigate and fairly fresh looking. This is a marked improvement on the 123-reg control panel that is well overdue for a modern, clean redesign. The final thing to give me the confidence to start looking into domain transfer was the quick response I received from Namecheap in resolving an issue when extending my domain name expiry. So, I started delving into the murky waters of domain transfers. ","date":"2015-12-11","objectID":"/posts/as-easy-as-123-reg/:1:0","tags":["domain","Namecheap","registrar","transfer "],"title":"As easy as 123(-reg) ","uri":"/posts/as-easy-as-123-reg/"},{"categories":null,"content":"12(free)-reg When you dive in, you find the depths are actually not as bad as they first look. Certainly, the process isn’t actually that complicated. The confusing part is navigating around help pages that usually tell an incomplete story of what you need to do to move your domain. Neither 123-reg nor Namecheap did a great job here: I ended up reading several different pages to do something that’s, I imagine, a reasonably common procedure for their customers. To give you an idea, I probably spent about five to ten minutes actually making changes on the old and new domain registrar’s sites, and probably spent ten times that time reading around, and double and triple checking what I was doing. I didn’t want to bungle the whole thing and spend hours in dialogue with support to resolve it. ","date":"2015-12-11","objectID":"/posts/as-easy-as-123-reg/:2:0","tags":["domain","Namecheap","registrar","transfer "],"title":"As easy as 123(-reg) ","uri":"/posts/as-easy-as-123-reg/"},{"categories":null,"content":"Migrating .uk domains One important note is that .uk domains are handled a bit differently to others. The stages you need to carry out and avoid downtime aren’t much more than: set up new name servers on your new registrar, and copy over whichever existing DNS settings are needed from your current registrar to the new one; change name servers on your current registrar to those of the new registrar and wait for this to be updated everywhere; maybe you need to make WHOIS data public (not sure if this is absolutely necessary for UK domains); change the Internet Provider Security (IPS) tag on your current registrar to that of the new registrar; request a transfer in at your new registrar. Again, all that matters for .uk domains is the IPS tag. Anything you read about EPP authorisation code or lock status just doesn’t apply at all for them. Also note by .uk domains, this is any .uk domain, i.e. this applies for stevenmaude.co.uk, not just stevenmaude.uk. ","date":"2015-12-11","objectID":"/posts/as-easy-as-123-reg/:3:0","tags":["domain","Namecheap","registrar","transfer "],"title":"As easy as 123(-reg) ","uri":"/posts/as-easy-as-123-reg/"},{"categories":null,"content":"How to move from 123-reg to Namecheap First, set up FreeDNS on Namecheap. Add your existing domain as detailed here. If you want to potentially activate that more quickly, note that you can prompt for an activation email. This also means you don’t need to change name servers before you activate should you wish. You’ll receive a link to activate the FreeDNS service in that case. Go to Advanced DNS on Namecheap’s dashboard and copy over entries from the 123-reg Advanced DNS settings. There were a couple of 123-reg IP addresses in there which I didn’t move as they were no longer needed. Everything else I just copied over by hand. MX records are added in Namecheap by selecting “Custom MX” in “Mail Settings” which is in the Advanced DNS dashboard. Optional: if you were using web forwarding and want to preserve it, you can set this up on Namecheap’s Manage options for the domain (“Redirect Domain”) or go to Advanced DNS and add entries manually. See their help pages for more. Go to 123-reg’s control panel and select the option to add name servers to your domain. I added two of Namecheap’s FreeDNS servers. Possibly optional: you may need to make the WHOIS contact details public for the domain if they’re not already. Go to 123-reg’s control panel and do this. You may wish to change the address first to one which you’re happy with making publically accessible. Check the new name servers have been added and that they work. Windows, Linux and OS X all have a whois command you can use to see if the name servers are at least visible on the DNS server you’re accessing. You may see this change fairly quickly e.g. within a few seconds or minutes. My way of testing that the name servers were working as expected was to add a temporary web forwarding rule on the new registrar which didn’t exist on the old one. Once I could confirm that redirect worked, I could remove the rule. As I was confident things were working OK, I then removed the 123-reg name servers from my domain, leaving only the FreeDNS ones there. If you can verify that everything is working OK, you should now wait sometime for the settings to propagate everywhere. 123-reg currently recommend 48 hours. As mentioned, make sure you’ve waited for the name server changes to propagate before continuing. Do something exciting in the meantime. Or just sleep. Change the IPS tag on 123-reg to Namecheap’s IPS tag as detailed in the 123-reg help. Enter Namecheap’s IPS tag which you can find here. If they’ve changed the link, a search for “IPS tag site:namecheap.com” will probably get you to the right place to find it. Go through the Namecheap transfer, select your domain in the dashboard and select “Transfer in”. You’ll probably have to pay them some money. The process should hopefully begin and be complete within an hour or two.2 My transfer only took maybe half an hour or even less. You can see the status of your transfer in the Namecheap dashboard. Finally, check that the FreeDNS has changed to the normal DNS, and that all of your DNS records are still present. For some reason, in the advanced DNS, the mail settings had changed from “Custom MX” to “No Email Service”. Email still appeared to work in the meantime, and the records were all still there when I reselected “Custom MX” which was odd. While here, you may also wish to set the public WHOIS to private if your domain is for use by a “UK Individual”. And that’s it! I wasn’t charged for changing IPS tag; no idea if 123-reg changed their policy for everyone or it’s because the domain was registered long ago. ↩︎ It’s a little disconcerting that all this happens without receiving any email to the domain contact to confirm the move. I’m not sure what procedure’s in place to prevent someone else transferring your domain to the same registrar but under their control, once you’ve changed the IPS tag… ↩︎ ","date":"2015-12-11","objectID":"/posts/as-easy-as-123-reg/:4:0","tags":["domain","Namecheap","registrar","transfer "],"title":"As easy as 123(-reg) ","uri":"/posts/as-easy-as-123-reg/"},{"categories":null,"content":"Currently looking to test out installing Windows 10 and one stumbling block was just getting hold of an official disk image. (I’m cautious these days and want to see how setting up a dual boot encrypted Windows 10/Ubuntu goes on a spare drive prior to actually committing to an upgrade.) ","date":"2015-10-26","objectID":"/posts/getting-an-official-windows-10-disk-image/:0:0","tags":["Windows","ISO","disk"],"title":"Getting an official Windows 10 disk image","uri":"/posts/getting-an-official-windows-10-disk-image/"},{"categories":null,"content":"Start me up The obvious searches for installing Windows 10 inevitably lead you to this page where it tells you to install the Media Creation tool and use that. Critically, that’s not an option if you don’t have access to a current Windows install. (Yes, it should be easy enough to find a Windows PC somewhere around, but there might not be one to hand.) The other drawback is that, as far as I can see without testing, is that it only creates media for one particular version of Windows, so you’d have to run through the process twice for Home and Pro. The actual ISO disk image is buried on the site under “Tech Bench”. Though the filename differed, the 64-bit download from that page did match the SHA1 as listed on the MSDN site as of 2015-10-23: File Name: en_windows_10_multiple_editions_x64_dvd_6846432.iso Languages: English SHA1: 60CCE9E9C6557335B4F7B18D02CFE2B438A8B3E2 Note that there’s a 90-day Win10 Enterprise trial, but there was no checksum at all to verify that download, and you need a Microsoft account just to get to the download. It also had the following ambiguous information: In order to use Windows 10 Enterprise, you must sign in to your PC with a Microsoft account. The option to create a local account will be made available at the time of the final release. which was unclear as to whether this still applies or not. ","date":"2015-10-26","objectID":"/posts/getting-an-official-windows-10-disk-image/:1:0","tags":["Windows","ISO","disk"],"title":"Getting an official Windows 10 disk image","uri":"/posts/getting-an-official-windows-10-disk-image/"},{"categories":null,"content":"Clean installing Windows 10 The other question I had was which version, Home or Pro, gets installed from this single ISO. If it’s a clean install, how is the version to install (Home or Pro) determined? From what I remember reading, the answer should be that if you skip entering a product key on request by the installer (which would automatically install the appropriate version if you provide a key), the installer prompts you to choose the version to install. ","date":"2015-10-26","objectID":"/posts/getting-an-official-windows-10-disk-image/:2:0","tags":["Windows","ISO","disk"],"title":"Getting an official Windows 10 disk image","uri":"/posts/getting-an-official-windows-10-disk-image/"},{"categories":null,"content":" generators. When I first thought about moving from Blogger to Pelican, it was because I’d had more than enough of Blogger’s drawbacks. What hadn’t crossed my mind, because I had no experience of using them, were the problems that static site generators could have. At the time, my hunch was that it wasn’t possible for them to be worse than Blogger. As it turns out, that was the case. But, they are not entirely perfect and, going through this migration, I better understood what the good and the bad of static site generators are. If you’re thinking of making a similar migration, these points might give you an idea of what you might deal with in shifting. While I’m familiar with Pelican and Blogger, most of the points apply generally so could still be informative for you if you’re considering a move from, say, Wordpress to Jekyll. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:0:0","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Converting and writing content (Markdown) ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:1:0","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Bad Depending on what you used to create a blog previously, and how much content you had on your old blog, it may be time consuming to move content. Even armed with a fork of Pelican where someone helpful had added Blogger conversion, the output wasn’t perfect and still had plenty of residual cruft that had to be fixed up by hand.1 Markdown’s quick to write with, but only gives you a small subset of HTML and so doesn’t handle every eventuality. There are dialects of Markdown which expand this with more layout options that may be available in your blog software of choice. With Pelican at least, it’s possible to use Pandoc via a plugin. Pandoc is one of the more versatile Markdown converters, but does introduces another dependency. Pelican also offers reStructuredText support which has support for more formatting options than vanilla Markdown, but just isn’t as nimble to write. Instead, I opted to occasionally allow fall back to raw HTML in posts, mainly for handling images. This isn’t ideal, but is a decent compromise that’s at least compatible with different Markdown implementations. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:1:1","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Good All my content is in fairly plain Markdown which I love writing in. It also means that I’m far less locked into Pelican than I was ever tied into Blogger. Should I wish to switch to another static site generator, only the metadata and Python-Markdown specific tweaks may need adjusting. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:1:2","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Adding content to the site and hosting ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:2:0","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Bad My git repositories are structured in a way that I like: blog HTML, the theme I use, my Pelican configuration, my Markdown posts, and the post images are all separate. However, for making small adjustments to posts, this is a nuisance: you need to edit the post and remember to push that, then generate the HTML, then move that into the HTML repository and finally push that to GitHub. I usually do this via branches too, which is more work too (though this is my choice). Improving this workflow is on my to-do list. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:2:1","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Good My blog’s not trapped within Google’s servers. Certainly, I’m wary of how much Google’s hands are present in much of what I do on the web, whether through hardware devices I use, their search tools or through websites that use features like Google Analytics and Fonts (including mine). At least the blog’s now hosted elsewhere and Analytics and Fonts that the site’s using could, in the worst case, be removed entirely, or replaced with alternatives. In fact, the blog’s not really dependent on the whims of any service owner at all. If GitHub stop hosting, I can take the HTML and host it elsewhere. With Blogger, you’re at Google’s mercy: if they ever decided to drop Blogger, then you’d be forced to switch to a new blog platform. All the content is tracked within a git repository. This means it’s version controlled, and there’s no easy way to obliterate posts. With Blogger, at least once, I foolishly attempted to edit multiple posts in separate browser tabs and subsequently lost the content of one of them. Luckily, I had a window open with the web version of the post to recover it. Having only a single canonical copy of posts stored in Google’s servers was clearly not a good thing. (Wordpress is better in that it’s possible to store multiple revisions of posts.) You have much more control of the post URLs; Blogger posts always have the year and date in them. Blog hosting using my own domain name on GitHub is free of charge. This is also free on Blogger. If you use wordpress.com, you’ll need to pay; if you’re using your own Wordpress installation, using your own domain is free, but it’s likely you’ll need to pay for hosting. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:2:2","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Appearance and viewer experience ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:3:0","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Bad With Pelican (probably much less so for Jekyll), there’s not a huge range of themes. If you’re fussy like me, you may need to spend some time tweaking, or creating your own. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:3:1","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Good My site now functions without the viewer needing JavaScript. With Blogger, you used to only see half the page content. If JavaScript is disabled now, the only issue is that the mobile menu doesn’t open, which is far less of a problem. There’s no awful separate mobile version of the site. On Blogger, the template I used had an unfriendly feature where swiping would navigate away from the page to the previous or next article. This could be triggered quite unintentionally by trying to pinch to zoom. Often my posts are on programming. Having code snippets that look nice without me having to do anything is a great bonus. In Blogger, you’d need a JavaScript plugin for syntax highlighting. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:3:2","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Running the generator ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:4:0","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Bad Static site generators are not for the impatient. For Pelican, being comfortable with Python is a big help, though not essential. Pelican — and I imagine this applies to Jekyll too — isn’t difficult to get a basic blog up and running, but to get the result you want may take you considerable work. If you have a very large number of posts, the time it takes to build your site may become an issue. With 61 posts and 1 page, my site takes 2 to 3 seconds on my Core i5 PC. If the site starts taking minutes to build, that may become a chore. This would be particularly frustrating if editing posts and wanting to quickly preview how the changes look since, with Pelican, the entire site is rebuilt when running the development server. ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:4:1","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Good Pelican is nice to use, and I had very few problems in doing so. Most of my effort was spent on converting content and amending the theme to my liking. Though the community’s relatively small, it does seem like there are more blog posts and activity compared with when I first looked into it last year. Pelican, and many other static site generators, are open source software. This means they are free to use, and free to modify (and share those modifications). The implication is that, as long as there are developers interested in it, it’s likely that it will continue to be supported and developed. Even in the unlikely event that everyone else abandons it and you’re the only user, you can still continue modifying it. (This also applies to Wordpress' source too incidentally.) ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:4:2","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"Living with a static site When worrying about the limitations that static sites may have, think about the actual minimum you need on a web site to share and present written ideas and thoughts you have. It’s really not very much; do you really need everything that Wordpress offers? Overall, I’m pleased with the result. When I’d started thinking about moving, I’d got fed up of using clunky old Blogger, which feels distinctly unloved by Google. Being able to write directly in the format (Markdown) that the blog actually uses is a real convenience. (Note that Wordpress offers this option too.) Along with that, the blog looking fresher than it did is a bonus. These features, more than perhaps anything else, actually makes me want to add new content to my blog, which is more than I could say that Blogger ever did for my inspiration. You can see exactly how much work in the early commits here. ↩︎ ","date":"2015-07-18","objectID":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/:5:0","tags":["Blogger","Pelican","Wordpress","static site"],"title":"Thinking of using a static site generator for your blog?","uri":"/posts/thinking-of-using-a-static-site-generator-for-your-blog/"},{"categories":null,"content":"The case of the disappearing tag cloud As you can probably tell — if not from the posts mentioning Pelican, then the giveaway is the footer of each page — my site is currently built using Pelican. Noticing there was a new version this week, I eagerly updated and then tried building my site. No errors, I thought, so everything must be fine. Unfortunately, on loading the site, the tag cloud had vanished1. Over the next twenty minutes or so, I slowly figured out the cause: the Pelican code that creates this had been moved out to a separate plugin. (If only I’d checked the Pelican docs first, I would have saved the time…) From there, it’s a simple fix. Install the plugin, add it to your PLUGINS in pelicanconf.py and the tag cloud will be restored. ","date":"2015-07-12","objectID":"/posts/restoring-a-tag-cloud-dispersed-by-pelican-3.6.0/:1:0","tags":["Pelican","tag cloud","plugin"],"title":"Restoring a tag cloud dispersed by Pelican 3.6.0","uri":"/posts/restoring-a-tag-cloud-dispersed-by-pelican-3.6.0/"},{"categories":null,"content":"Installing Pelican plugins One small frustration is that the official plugin repository is, if maybe not monolithic, a substantial size. The appeal of a central repository does make some sense. It provides a single location for developers to add functionality to Pelican and means users can issue one command to get access to everything at once. But my hunch is that users probably only want a few of the numerous plugins: seventy at the time of writing. Also, when a plugin is updated by its author, they additionally have to put a pull request into the plugins repository to keep the submodule or files stored there in sync, which is extra work for everyone involved too. ","date":"2015-07-12","objectID":"/posts/restoring-a-tag-cloud-dispersed-by-pelican-3.6.0/:2:0","tags":["Pelican","tag cloud","plugin"],"title":"Restoring a tag cloud dispersed by Pelican 3.6.0","uri":"/posts/restoring-a-tag-cloud-dispersed-by-pelican-3.6.0/"},{"categories":null,"content":"My tag cloud plugin fork One solution for installing the plugin would be to write a simple shell script that does the cloning and just keeps the tag plugin. Instead, I felt that it would be both simpler and more sensible — rather than retrieving many plugins that I don’t need — to install the plugin via the existing requirements.txt in my blog configuration. (The other plugin I use, pelican-alias, is installed this way in my setup.) So, I made the plugin pip installable (available here) and added that to my requirements for what seems like a neater solution. The only downside is that I have to ensure that this plugin is updated when the original code is. For this plugin however, I suspect that updates would be irregular. If the site’s not changed since I wrote this, then it’s the one that shows up in the upper right sidebar. ↩︎ ","date":"2015-07-12","objectID":"/posts/restoring-a-tag-cloud-dispersed-by-pelican-3.6.0/:3:0","tags":["Pelican","tag cloud","plugin"],"title":"Restoring a tag cloud dispersed by Pelican 3.6.0","uri":"/posts/restoring-a-tag-cloud-dispersed-by-pelican-3.6.0/"},{"categories":null,"content":"If you look back, the first post on this blog was agonising over what blogging system to use. At the time, my reasoning for choosing Blogger was that it was quick to get started with. And that was true. Provided you already have a Google account, you can make a blog in a few clicks. There’s a simple interface for entering posts: you can enter raw HTML or use something that looks more like a word processor. Even if you know very little about how the web works, it makes publishing straightforward. It’s cost-free to use too, even if you’re using your own domain name. However, the limitations of Blogger soon became apparent: The default templates available look dated. (Though you could well modify them or install one from elsewhere). Furthermore, for the default template I used, which was fairly plain, it wasn’t even possible to read the site properly without JavaScript: half of the page content would not be displayed. Code snippets, which I’ve often used in posts, are also hard to read because there’s no syntax highlighting. The mobile template for Blogger interprets swiping left or right across the screen as meaning that you want to move to a different article. This is frustrating enough even when you know about it as it’s easy to trigger unintentionally. It’s more confusing for someone visiting your site for the first time. As mobile internet browsing increases, the needs of viewers on those devices becomes more important. Personally, I found writing posts on Blogger to be clumsy. The WYSIWYG editor is quite limited. If you want anything outside of the fairly limited feature set (for example, footnotes, or more control over images), you need to edit the HTML directly. The HTML that the editor creates seems to get polluted with unnecessary tags quite often, especially if you’re switching fonts (e.g. between code and non-code sections). I’ve also accidentally lost work due to having two copies of the post creator open, with one overwriting the contents of the other unintentionally on save. To work around this, towards the end of my Blogger use, I was writing posts in Markdown1 completely separately, converting them from Markdown to HTML, then pasting the HTML into Blogger, fixing up things and adding images. Writing posts in Markdown was convenient, the subsequent wrangling of the HTML output to Blogger was not. The current, published form of the blog posts is that which is stored in Google’s servers. Keeping a local copy and tracking any changes made in posts was a chore, so I never bothered. Blogger isn’t software available for you to run on your own computer, so you’re at Google’s mercy. If Google change something that you find particularly detrimental, or if they ever decided to drop it completely, you’ll be forced elsewhere. So, what are the alternatives? The main off the shelf solutions seem to be Wordpress or static site generators. Wordpress is very popular for building blogs, and actually resolves many of my Blogger complaints. One big plus is that it’s open source (see wordpress.org). This means you can run it anywhere you like, though you’ll have to pay to host it somewhere. Alternatively, you can host your blog at wordpress.com for free, albeit with possible ads and without the use of your own domain name. Resolving these problems is simple: just give them money! Wordpress is also a step up from Blogger in terms of customisability, with a much larger user community around it. Because of this, there’s far more in the way of themes and plugins to tweak your blog how you want it. Researching just now, I discovered Markdown is supported for writing posts, which is great. And some previous history of your posts are saved (25 revisions at the time of writing), with the ability to revert and compare them. But, having used Wordpress at work, there were still three big reasons why Wordpress didn’t seem a suitable replacement: Cost of hosting. This isn’t particularly high, but my blog is just a side project, so I’d rather keep ","date":"2015-06-06","objectID":"/posts/blogger-versus-wordpress-revisited/:0:0","tags":["Blogger,Wordpress"],"title":"Blogger versus Wordpress revisited","uri":"/posts/blogger-versus-wordpress-revisited/"},{"categories":null,"content":"What I thought I’d get done in a week, actually took about four months. But I’m finally free of Blogger and have moved to Pelican for creating my blog. I’ll be writing more about how I migrated soon. As I approached the end of that lengthy migration, I found myself writing new draft articles that I wanted to ensure didn’t actually make their way into my publication directory. ","date":"2015-05-31","objectID":"/posts/hiding-draft-articles-in-pelican/:0:0","tags":["draft,article,Pelican"],"title":"Hiding draft articles in Pelican","uri":"/posts/hiding-draft-articles-in-pelican/"},{"categories":null,"content":"Writing blog posts The way that I usually approach writing blog posts is that, as I reach the end of writing them, I’ll start reading in the final display format, i.e. as a web page. This helps spot any mistakes that I might miss from my eyes glazing over at a text editor, and helps me spot formatting problems. But, you might be working on multiple articles at once, so you want to publish while you still have draft articles in your content directory. Here’s a few ways to prevent posts from being accidentally published, without needing to do file juggling of your drafts in and out of the content directory. ","date":"2015-05-31","objectID":"/posts/hiding-draft-articles-in-pelican/:1:0","tags":["draft,article,Pelican"],"title":"Hiding draft articles in Pelican","uri":"/posts/hiding-draft-articles-in-pelican/"},{"categories":null,"content":"Marking as draft and .gitignore Any articles we stick in the content directory will end up in our output directory, right? Not if we do like the documentation says and add Status: draft to the metadata. Building your site will then place draft articles in a drafts directory, and the articles therein aren’t linked from anywhere. This might be useful if you want to share a link with someone to get feedback on a post before you officially publish it. Using a .gitignore file in your output directory is also an option if you’re using git to push new versions of your blog for publication. Add drafts/ to your .gitignore and you’d have to make a deliberate effort to add them for publication. ","date":"2015-05-31","objectID":"/posts/hiding-draft-articles-in-pelican/:2:0","tags":["draft,article,Pelican"],"title":"Hiding draft articles in Pelican","uri":"/posts/hiding-draft-articles-in-pelican/"},{"categories":null,"content":"Stopping Pelican from seeing the draft posts The other option is to just prevent Pelican from seeing draft posts completely. This does mean that they won’t appear in your local preview of your site, not even as an unlinked page. ","date":"2015-05-31","objectID":"/posts/hiding-draft-articles-in-pelican/:3:0","tags":["draft,article,Pelican"],"title":"Hiding draft articles in Pelican","uri":"/posts/hiding-draft-articles-in-pelican/"},{"categories":null,"content":"Using Pelican’s IGNORE_FILES Putting “draft” in the filename of any drafts is another option. Then, you can use the IGNORE_FILES setting in pelicanconf.py to specifically ignore the drafts. Note that if you specify a setting in your Pelican configuration files, the defaults get overridden. This means that you’ll need to copy over the existing settings from the defaults too. Otherwise, you’ll unintentionally change your configuration. For instance, the default IGNORE_FILES setting is IGNORE_FILES = ['.#*'] so we’ll need to add IGNORE_FILES = ['.#*', '*draft*'] to our Pelican settings. ","date":"2015-05-31","objectID":"/posts/hiding-draft-articles-in-pelican/:3:1","tags":["draft,article,Pelican"],"title":"Hiding draft articles in Pelican","uri":"/posts/hiding-draft-articles-in-pelican/"},{"categories":null,"content":"Hiding the files completely (in Ubuntu, at least) Under Linux, I thought that preceding the filename with a dot would hide posts from Pelican, but it doesn’t seem to. However, sticking a tilde on the end of the filename does seem to prevent Pelican from seeing the post. One side-effect is that if you use a text editor that creates backup files with a tilde on the end, your backup files will actually have another tilde on the end (e.g. my_draft_post.md~~). (Maybe marking files as hidden in Windows would have the same effect? Haven’t tried this.) ","date":"2015-05-31","objectID":"/posts/hiding-draft-articles-in-pelican/:3:2","tags":["draft,article,Pelican"],"title":"Hiding draft articles in Pelican","uri":"/posts/hiding-draft-articles-in-pelican/"},{"categories":null,"content":"!!! article-edit \"\" Edit 2015-07-24: Slightly involved story aside, the version of uBlock linked here, by the original developer, is now uBlock Origin. I’d recommend searching for that version if installing as an extension. It's available for both Firefox and Chrome/Chromium. !!! article-edit \"\" Edit 2015-05-18: The Chromium bug is fixed which should fix the uMatrix UI sluggishness, and uMatrix is finally available as a Firefox addon. !!! article-edit \"\" Edit 2015-01-09: HTTP Switchboard development is discontinued and has been replaced with uMatrix. The interface is largely unchanged. One problem at the time of writing is a Chromium bug which I think is the cause of the UI being really laggy to appear. Hopefully, this should be fixed soon. After tinkering around with Adblock’s filters to block some ads, I remembered that I’d read about an alternative to Adblock for Chromium-based browsers called HTTP Switchboard a few months ago and figured I should actually give it a try. One of the reasons that HTTP Switchboard was notable is that it promised to be much more CPU and memory friendly than Adblock Plus currently is. In fact, the adblocking part of HTTP Switchboard has now been separated out into another project, ublock. Both are still useful. I’ll explain why below. ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:0:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"Why are you blocking content at all? Many content-driven websites use advertising as their business model. My feeling is that, while sites are welcome to try and send content to my browser, I should have the freedom to choose what my browser actually loads. There’s no contract I’ve signed that states I must load and display the page as the site owner intends. (What if I’m using a text browser like links?) And there’s no obligation on my part to make their business model sustainable. Maybe you still think this seems selfish. Well, to some extent it is. But, my privacy and computer security are more important to me than the financial success or failure of an external organisation. That I put my priorities in this order is kind of obvious though, right? ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:1:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"Why shouldn’t sites and advertisers worry so much? Even with “despicable” users like me, there are two reasons I can think of as to why advertisers and sites dependent on them shouldn’t feel so glum: Adblocking users have made an active choice to block ads. Those users have done you a favour in removing themselves from your viewership as you don’t waste time targeting them; why chase “customers” running in the opposite direction from you? Browsing is increasingly done from mobile browsers. How many users are going to bother to root their devices or install alternative browsers just for adblocking? ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:2:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"But why worry about where displayed web content originates from? ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:3:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"Privacy One contentious issue with ads and cookies is privacy. As this Economist article highlights, the depth of what advertisers collect about your browsing habits is extensive, and it’s handled and passed around by many organisations. Unless you’re all for being advertised at, this isn’t for your benefit. Maybe advertisers are really all benign and just interested in deciding who’s best to try to sell to. Perhaps just as the executive in that article quoted claims, it really is just that, “We are not trolling for personal information. We are trying to figure out if you are a high-value customer and are in the market for a car.” But, the sheer amount of information being collected is worrying, especially when, as a user, I’ve no idea of what’s actually being tracked and what the intended end use is. ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:3:1","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"Malware As well as that, web advertising has been a conduit for the deployment of malware previously and will no doubt continue to be. (It’s a nice way of distributing it to users who are visiting, what to them, might seem like a trustworthy site. If they can get the malicious ad past the company providing the advertising, they don’t need to go to the trouble of hacking the site directly.) There have been one or two high-profile incidents this year too. In terms of potentially malicious content, it’s not only just ad providers that I’m concerned with, but allowing large numbers of arbitrary sites to run their scripts on your machine unchecked. Look at the recent JQuery compromise, for instance. Fortunately, only the site itself was affected and not the hosted libraries that many sites. It’s not inconceivable that an attacker may well deliberately go after to poison hosted libraries. It’s just way too big a jackpot for malware distributors to ignore. Even if such an attack might be spotted quite quickly, how many users could be affected in the interim? (I’m not even sure what the solution is here; often JQuery’s needed for sites to function as intended…) Minimising the number of third-party sites that you allow content to load from can’t be a bad thing. (There’s some risk wherever you go, but in some cases, I’ve seen sites loading in content from more sites than I can count on both hands.) ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:3:2","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"Controlling what your browser displays So, what can you do? There are browser extensions for Firefox and Chrome that give you more control over what your browser loads. Lots of sites are perfectly readable without them storing tracking information about you, without arbitrary JavaScript running. For those that don’t, it’s quite simple to whitelist them. The downside of this is that it can be a chore. In Firefox, I need a slew of extensions to do this: which sites are allowed to run scripts (NoScript), which cross-site requests are allowed (RequestPolicy), and which sites can store cookies (CookieMonster). Managing each one in turn, allowing permissions for cookies, scripts and third-party scripts, can be tedious. I’ve run NoScript for a long time, but these days with sites often hosting content and scripts somewhere other than on the domain you’re visiting, it’s much more work than it was. HTTP Switchboard was a big, all encompassing project for Chromium-based browsers that had features comparable to Adblock Plus, and the Firefox extensions, NoScript, RequestPolicy. Recently, the adblocking, pattern-based filtering has been split out into another project, ublock. ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:4:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"ublock By virtue of what HTTP Switchboard aims to do — give you full control of what is allowed by your browser — this makes its user interface slightly more complex, which I’ll soon get onto. Stripping out this sophistication seems like a smart move, since ublock’s incredibly easy to use. Really, it requires the same amount of setup as Adblock Plus does: not much at all. It comes with the most popular Adblock Plus filters accessible on install, and you can easily choose which of those filter lists are applied. Disabling ublock for a site is also straightforward. Click the red µ icon next to the address bar and then click the power icon. It does the same job as Adblock Plus, but with lower CPU and memory usage. If you’ve ditched an ad blocker for this reason, it’s worth looking at. Even if you’re still happy using Adblock Plus (or one of its forks), ublock is definitely worth spending the couple of minutes to try it out. ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:5:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"HTTP Switchboard On to HTTP Switchboard, which allows fine grained control of the types and locations of content that Chromium-based browser can access. Note that with the current version of HTTP Switchboard, there’s a little initial setup to disable the adblocking filtering that uBlock covers. HTTP Switchboard’s interface can feel initially overwhelming; more so than perhaps the likes of NoScript, RequestPolicy and CookieMonster that I use which all have similar menus to allow or deny permission for a site to run scripts or store cookies. These have a text explanation of what the option does. From left to right, examples of NoScript, CookieMonster and RequestPolicy permission menus when accessing Google Books. The problem with this menu format is, though it’s explicit and clear, it’s cumbersome when you’re dealing with several extensions. For each extension, you need to repeat the process of whitelisting every site that you’re allowing. When you’ve actually used HTTP Switchboard for a few minutes, its simplicity clicks. HTTP Switchboard opts for a table-like interface. Each site is represented as a row, and each type of content is shown as a column. Example of HTTP Switchboard's interface when accessing Google Books. This lets the user approve different permissions for different sites quickly. Just read down the list of sites, click on those you want to whitelist completely, and for those you don’t you can allow specific features only (e.g. loading images only). Likewise, if you want to enable one type of content for all sites, just click the appropriate column. If you want to save the permissions in future, click the padlock icon, and you’re done. It helps a lot that you can see all the sites that are trying to load content or save cookies in one glance, rather than consulting separate menus. There’s an autoreload option that refreshes the page when you change permissions, so once you’ve approved everything needed, the page should then work as expected. Out of the box, (I think) HTTP Switchboard was configured to allow most requests through, apart from those blacklisted by filters. What initially confused me was at what level the filters apply. (Do they apply to this site? To this domain? globally?). What I’d recommend is, like the extension developer, is to block everything by default and then whitelist only what you want to allow. There’s an option in the HTTP Switchboard settings under “more security” to create a new site-specific scope for every site you visit which is really useful. This creates a set of rules for each site you visit. Furthermore, the drop down (click the blue top-left box with the site name in) lets you select where the rule applies (to the site, to the domain or globally). This lets you block content from, say, a site like twitter.com when you’re visiting a third-party site, but allow it when you’re directly visiting Twitter. It also has some nice bonus privacy features, like removing HTTP referers when visiting non-whitelisted sites and a user-agent randomiser. The only thing that’s missing for me is guidance on what the sites likely are. If you’re happy to spend a few seconds researching, it’s not hard to figure this out. It might save a bit of time if there was a database of sites telling you “hey, you might need this, it’s a content delivery network” or “this is tracking how you’re using the site; you probably don’t need it for the site to function properly”. Yes, I can search, but it’s a bit clunky to do that. ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:6:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"A summary These are really great browser extensions, and credit to Raymond Hill for an awesome job on them. Every time I’m managing permissions with multiple extensions in Firefox now, I sigh a little. (Firefox’s awesome bar is far more useful to me than Chrome/ium’s omnibox though, so I tend to stick with Firefox.) Longer term, I’d really hope that a Firefox add-on developer picks up the baton to port these. (As a postscript, I should point out that as well as adblocking and control over requests, HTTPS Everywhere is another useful privacy-enhancing extension for Firefox and Chromium browsers. Where HTTPS browsing is available on sites but not always used, HTTPS Everywhere enables it for you.) ","date":"2014-09-28","objectID":"/posts/taking-control-of-chromium-and-chrome/:7:0","tags":["ad","µBlock","HTTP Switchboard","blocking","privacy","adblock","ublock"],"title":"Taking control of Chromium (and Chrome) with ublock and HTTP Switchboard","uri":"/posts/taking-control-of-chromium-and-chrome/"},{"categories":null,"content":"My Moto G has been nagging me to upgrade firmware So I did today. (I was persuaded by the Motorola system updater. Not because it’s a lovely looking bit of software, but because it makes nagging an art with its focus-stealing prompt. If you go for the “no” option, your phone kindly reminds you again in an hour every hour, just in case you’ve forgotten. This is regardless of what you’re doing at the time, even if you’re say using Google Maps for driving navigation which is quite hideous. Even the option to postpone the reminder for a week or a month would be nice. A rubbish, rubbish, rubbish user experience.) A simple upgrade is as easy as pressing “upgrade”. Because my phone was rooted and encrypted, the upgrade was likely to fail, so I had to restore it to stock firmware first. However, I wanted to then reroot it so that I could use XPrivacy. This lets you restrict the permissions that apps have. Google did have App Ops in Android which did a similar thing (without quite as much granularity, I believe) but promptly removed it. It would have been good of them to leave it in: it would have removed one reason I have for rooting. Android apps are just far more intrusive in their permissions than they usually need to be. No, I don’t want Twitter’s app to access my contacts, thanks; I’m more than capable of finding people on Twitter directly. No, there’s no reason for Soundhound to know where I am. No, even if my phone had NFC, I don’t want F-Droid to be able to use it. The assessment here is that one big reason for the removal of App Ops is due to it breaking apps. I disagree with that though. Using XPrivacy, I restrict almost every app and haven’t had any trouble. I can understand that if app permission controls were enabled by default, there’s the possibility that it could cause problems for users that don’t understand the implications of allowing/denying permissions, but if it was something like the developer options where you have to deliberately enable it, then they wouldn’t even know it was present. Anyway, here are the reference notes I made while upgrading for (the inevitable) next time. ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:1:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Preliminary setup with Ubuntu (or your choice of a Linux that has adb and fastboot) Install (to a live USB, or use an existing install on your laptop) Enable third party repositories in “Software and Updates”. Do sudo apt-get update. sudo apt-get install android-tools-adb android-tools-fastboot ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:2:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Backup data from phone Copy off any photos, documents or other files you want to save. If you want to backup your call logs and text messages, Call Logs Backup and Restore, and SMS Backup and Restore work pretty well for me. That said, they do request internet permissions which I block. ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:3:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Restore stock firmware Stock firmwares are available here and this post was a useful reference. Boot into the bootloader using power on and volume down key (or enable USB debugging mode when the phone’s on normally and do adb reboot-bootloader). sudo fastboot flash partition gpt.bin sudo fastboot flash motoboot motoboot.img sudo fastboot flash logo logo.bin sudo fastboot flash boot boot.img sudo fastboot flash recovery recovery.img sudo fastboot flash system system.img_sparsechunk1 sudo fastboot flash system system.img_sparsechunk2 sudo fastboot flash system system.img_sparsechunk3 sudo fastboot flash modem NON-HLOS.bin sudo fastboot erase modemst1 sudo fastboot erase modemst2 sudo fastboot flash fsg fsg.mbn sudo fastboot erase cache sudo fastboot erase userdata sudo fastboot reboot (sparsechunk files may have numbers 0, 1, 2 - just flash them in ascending order.) ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:4:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Upgrade Boot into stock and install update. To avoid heartache, make sure you check for system updates after you update, and install any further updates before proceeding. Repeat until no update is found. Otherwise, like me, you may discover after rooting again that there are still updates remaining after the first one. In that case, you’ll have to restore to stock again :( (I was on 4.4.2, but there was a minor update that didn’t change the version before the latest 4.4.4 update.) ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:5:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Unlock bootloader ** Only needed if not previously unlocked; while restoring stock firmware and upgrading, this didn’t get relocked. ** You need a bootloader unlock code from Motorola’s site. fastboot oem unlock Boot into fastboot mode: Enable developer mode again in settings: adb reboot bootloader Optional: get a copy of the original Motorola logo and do: sudo fastboot flash logo original_logo.bin to remove the annoying “bootloader is unlocked” warning at boot. See CyanogenMod’s site for more details. ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:6:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Install custom recovery Get custom recovery; I used TWRP. Get a recent fastboot .img; at the time of writing, 2.7.1.1 was the latest, but didn’t work. 2.7.1.0 worked fine. Install through bootloader using fastboot: sudo fastboot flash recovery modified_recovery.img ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:7:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Root the phone by installing SuperSU Boot from bootloader into TWRP recovery. Reboot system, say yes to install SuperSU. Boot up, run SuperSU installer; go to Play Store and choose update (not open). This will probably prompt to install a new binary; the normal install should work fine. ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:8:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Install XPrivacy Install XPrivacy installer. Follow each of the several instruction steps that are shown when you run it. ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:9:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Encrypt and change password again Create a screen lock PIN or password, then fully charge battery and encrypt. Change the password using Cryptfs Password so that you can have a separate screen unlock code and encryption passphrase. (This is another good reason to root your phone. Without this, your encryption password is identical to your screen unlock code. Unless you want to have either: a weak password, or a huge password that you have to enter every time you unlock your phone you’ll need to change it using CryptFS which requires root access.) ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:10:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Disable all the slow animations To make Android snappier, disable the animations by enabling the developer options. Go in Settings \u003e About phone, scroll to the bottom and keep pressing build number until it tells you the developer options are active. Now, in Settings \u003e Developer options, you can set “Window animation scale”, “Transition animation scale” and “Animator duration scale” all to off. ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:11:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Restore data, account, messages, call logs Reinstall SMS Backup and Restore, Call Log Backup and Restore, and restore the backups you made. Reinstall any other apps you want, including F-Droid if you want AdAway and that’s everything finally done. No more nagging for updates! ","date":"2014-08-25","objectID":"/posts/updating-and-rooting-moto-g/:12:0","tags":["Moto","firmware","upgrade","Android","system","root"],"title":"Updating and rooting Moto G","uri":"/posts/updating-and-rooting-moto-g/"},{"categories":null,"content":"Intel Outside (of my own office) One thing I’ve never had too much of a problem with on my Ubuntu 12.04 LTS install is networking. There’s been the odd time when it’s refused to connect to the network, despite everything else being fine. This has always been fixed with a reboot, though. The problem I experienced today was a lot more serious. I’d travelled to another office to work and tried to connect to their wifi. I knew the network should have been OK as no-one else seemed to be having an issue, and I’d entered the WPA password correctly. What I saw was that the wifi showed as connected, but I couldn’t connect to anything. Occasionally, I could try and ping a server, and when I bothered to wait long enough to see anything, what I saw was connection timeouts, horrible packet loss, and ping times of several seconds. (Pings are normally measured in milliseconds; I was seeing results of thousands of milliseconds.) I’d never seen this problem before. It quickly became apparent that this was specific to my hardware and my Ubuntu install. When I booted into Windows, it connected immediately without any problem, and a colleague was using a later version of Ubuntu on different hardware without a problem at all. It’s certainly difficult to try to fix such a vague problem. Running searches like “wifi not connecting on Ubuntu” inevitably brings up much noise relating to every wireless problem out there and not anything much of relevance, but I didn’t have a lot more to go on. Having to do lots of searching and skimming through web pages using a phone to search isn’t ideal either, but it did eventually lead me to the right answer. ","date":"2014-07-29","objectID":"/posts/my-wifis-connected-but-theres-no/:1:0","tags":["connection","Centrino","wifi","Ubuntu","Intel","fix"],"title":"My wifi's connected, but there's no internet…? (Intel Centrino versus Ubuntu)","uri":"/posts/my-wifis-connected-but-theres-no/"},{"categories":null,"content":"Fun with iwlwifi What worked was searching for issues relating to the wifi driver (iwlwifi). A common theme that came up was 11n_disable=1 which is a modprobe option that disables the wireless N feature of the Intel wireless adapter. (lshw -C network states the model is “Centrino Wireless-N 2230”.) By running the commands sudo rmmod iwldvm sudo rmmod iwlwifi sudo modprobe iwlwifi 11n_disable=1 sudo modprobe iwldvm I got normal wireless access back (minus wireless N speeds, but the choice between fast, but no wifi and slow wifi is not a difficult one to make). The rmmod commands remove the modules from the kernel, while modprobe adds them. We need to readd the wifi module with wireless N disabled. (The offending module is iwlwifi but I couldn’t rmmod it until I’d removed iwldvm as trying to do so showed that iwldvm was using iwlwifi. There is a force option for rmmod but it didn’t seem sensible to use it when there was an alternative.) ","date":"2014-07-29","objectID":"/posts/my-wifis-connected-but-theres-no/:2:0","tags":["connection","Centrino","wifi","Ubuntu","Intel","fix"],"title":"My wifi's connected, but there's no internet…? (Intel Centrino versus Ubuntu)","uri":"/posts/my-wifis-connected-but-theres-no/"},{"categories":null,"content":"Making this fix stick If you want this fix to apply every time you boot, you can just create a new .conf file: /etc/modprobe.d/wireless-n-fix-iwlwifi.conf, adding just this line to the file: options iwlwifi 11n_disable=1 (Thanks to this nice summary.) You can choose any valid filename you like, provided it ends in .conf; you’ll also need to edit this using sudo and your favourite text editor. ","date":"2014-07-29","objectID":"/posts/my-wifis-connected-but-theres-no/:3:0","tags":["connection","Centrino","wifi","Ubuntu","Intel","fix"],"title":"My wifi's connected, but there's no internet…? (Intel Centrino versus Ubuntu)","uri":"/posts/my-wifis-connected-but-theres-no/"},{"categories":null,"content":"Why hadn’t I seen this problem before? What seems to be the issue is that I think that the office was using a wireless N router, and there’s some kind of issue with this for the Intel Centrino wireless network adapter on my laptop under Ubuntu. Nowhere else I’ve been with this laptop seems to be using wireless N, so that explains why it was an issue for the first time today. It’s pretty poor that this seems to have been a long-running problem and yet it’s still present… but at least there’s a workaround. ","date":"2014-07-29","objectID":"/posts/my-wifis-connected-but-theres-no/:4:0","tags":["connection","Centrino","wifi","Ubuntu","Intel","fix"],"title":"My wifi's connected, but there's no internet…? (Intel Centrino versus Ubuntu)","uri":"/posts/my-wifis-connected-but-theres-no/"},{"categories":null,"content":"An ordinary afternoon and I’d wanted to work on some code locally on my machine. I knew the tests previously ran without any issue. So, after I’d cloned the repository locally, I ran nosetests to make sure everything was OK with my setup before I started. What I expected is the tests to run pretty quickly as before. What happened instead is that Python started hogging a CPU and I saw a lonely blinking cursor where I expected my test results. Something was happening, but none of the tests were seemingly running. That is, I wasn’t seeing any pass/fail indicators, which was strange. ","date":"2014-07-05","objectID":"/posts/arduous-lessons-in-python-why-main-is/:0:0","tags":["foolishness","name","main","Python","fix"],"title":"Arduous lessons in Python: why main() is useful","uri":"/posts/arduous-lessons-in-python-why-main-is/"},{"categories":null,"content":"What actually happened Something was running that evidently wasn’t what I intended. On inspection, one of the import statements in the test code had a structure somewhat like: def some_function(): do some things def another_function(): do some other things def really_long_winded_scraping_function(): do lots of things really_long_winded_scraping_function() Bear in mind that when you import a module, the statements are actually executed. If you’ve just got function and class definitions in there, then this means that they get defined, but they aren’t executed until you call them. However, if functions are called in the module’s flow of execution, then they’ll be called when you import it too. So, when this module was being imported by the test module, the really_long_winded_scraping_function() was being called. So, yes, eventually, the tests would have run, but they’d run after the code in the import had finished, which would have taken hours. ","date":"2014-07-05","objectID":"/posts/arduous-lessons-in-python-why-main-is/:1:0","tags":["foolishness","name","main","Python","fix"],"title":"Arduous lessons in Python: why main() is useful","uri":"/posts/arduous-lessons-in-python-why-main-is/"},{"categories":null,"content":"Fixing the problem Normally, I routinely use: if __name__ == '__main__': main() in Python code that I start writing. From the official documentation: “Within a module, the module’s name (as a string) is available as the value of the global variable __name__.” What it also does is assigns '__main__' to __name__ if the module is being executed directly. This gives you a way to specify different behaviour depending on whether the module itself is run or just imported. If you’re importing the module to reuse the code elsewhere, you can then just access the relevant functions and classes that you need, rather than running code that actually starts doing things. In this case, for some reason, we’d just quickly thrown some functions together, then found that the function that actually initiates all of the scraping wasn’t being called. So, we’d just quickly added a call to what was our at the bottom of the module, set the code going to do the scraping and not run the tests again. This was fine, until I forgot about this and came to run the tests again later! It’s a good case for running your tests regularly; the reason it took me so long was that we hadn’t run the tests since getting the main code working. If we had, I think I’d have been more likely to spot the connection between our fix to the code and the tests taking an age. (This isn’t a test specific problem. You’d notice it whenever you imported the code into another module. It just happened to be that the tests were the only other module that we’d imported the code into.) ","date":"2014-07-05","objectID":"/posts/arduous-lessons-in-python-why-main-is/:2:0","tags":["foolishness","name","main","Python","fix"],"title":"Arduous lessons in Python: why main() is useful","uri":"/posts/arduous-lessons-in-python-why-main-is/"},{"categories":null,"content":"Visiting Cambridge I visited Cambridge this week. Though I know Oxford fairly well, somehow I’d manage to avoid ever travelling to Cambridge before, so being in a lovely city and surrounded by things new to me (though mostly far older than me) was a pleasant experience. So, what was I even doing there anyway? The university careers service was holding an event for postdoctoral researchers to find out more about paths outside of university. I’d been invited to talk about data science. The day started with brief talks from the careers staff and one of the guests. In a short session, they divulged several ideas that I’d taken far longer to collect either from reading around, the careers programme I attended last year, or by a slow realisation during my career searching process last year.1 After a brief session where I and the other nine invited speakers introduced themselves to everyone, the rest of the day was spent talking with small groups of postdocs. Much discussion seemed to centre around what my work involves, and how I eventually found my job.2 3 This type of event seems a great way for people to get inspired about what kind of careers they could consider a move into, especially if they’re at a crossroads where all the exits are hazy. I think they can also provide a helpful confidence boost at a time when anxiety and indecision can easily take hold. If you’re chatting with someone who’s been in your shoes recently and managed to make a successful career move, why shouldn’t you believe that you can replicate it? A positive for me was that a few of the people I spoke to were already looking into data science as a career. And I’m also hoping that I managed to encourage those who, on speaking to them, seemed like they definitely had relevant skills, but they weren’t entirely sure whether it was an option for them to explore. ","date":"2014-06-22","objectID":"/posts/chatting-about-data-science-careers/:1:0","tags":["data","careers","university","research","science","Cambridge"],"title":"Chatting about data science careers","uri":"/posts/chatting-about-data-science-careers/"},{"categories":null,"content":"PhDs moving from research to industry There were two thoughts that I took away from the event. First, contrary to my preconceptions, it seemed that from the experiences of some of the other speakers that a long stay in university research wasn’t perhaps as detrimental as I expected. In other words, you’re not “trapped” at university if you have spent several years as a postdoc. (Though if you’ve already made up your mind to leave, it was mentioned that it’s advisable to plan your exit as soon as possible.) The other idea that I heard a couple of times is that there seems to be a disconnect between employers looking for well qualified PhDs and the candidates themselves. The jobs are there. The candidates are there. But, for whatever reason, they’re missing each other. From my experience as a researcher, I’d suggest a part of the problem is that PhDs and postdocs may be simply unaware of the range of opportunities available to them. For data science at least, there are several organisations whose specific goal is to help transition science PhDs into employment. This is fine, but that’s covers one discipline and small groups of fortunate researchers. Maybe it’s also a problem of communicating across this gap. Researchers are perhaps underequipped to successively sell themselves to employers, and maybe industry employers are not making it clear when PhDs with strong analytical backgrounds are exactly who they’re looking for. All in all, I’m sure the researchers attending took away some good ideas. It was definitely a productive and fun day out, and gave me the unusual opportunity to take a bit of time out to reflect on where I am in my career move. Thanks to Anne Forde of Cambridge University’s Careers Service and Kim Nilsson of S2DS for inviting me to participate! A great idea that was highlighted is to just look through postings on sites which are advertising jobs (maybe Nature Jobs, New Scientist if you’ve a science background and want to stay in science, and further afield otherwise) and investigate lots of jobs. And, another point that was emphasised, which is really important, is don’t dismiss yourself as being unsuitable for a job based on the title alone. You may find that you’re already more than capable of meeting the candidate requirements. More specifically, small bits of wisdom such as these. Figuring out what you like in your current work, what your strengths are and what your priorities in work are. Having these things clear in your mind really does help you filter prospective jobs or workplaces to those that you’re more likely to be interested in. It’s easy for the panic of “I need a job” to settle in, and to then focus on applying for vacancies, rather than going the perhaps necessary process of self-discovery instead. ↩︎ One good question that came up a couple of times and I didn’t have an instinctive answer for was: how do the longer term careers of data scientists progress? My excuse was that I’d just started out in my career and data science is still relatively new. Kim also mentioned that she saw lots of opportunities to specialise and find a niche. ↩︎ Perhaps it shouldn’t have surprised me so much. Checking over my own notes from last year, those were the main questions I was asking the data scientists that I chatted to in informational interviews. ↩︎ ","date":"2014-06-22","objectID":"/posts/chatting-about-data-science-careers/:2:0","tags":["data","careers","university","research","science","Cambridge"],"title":"Chatting about data science careers","uri":"/posts/chatting-about-data-science-careers/"},{"categories":null,"content":"A year ago, I started writing this blog. I didn’t really have any expectations of what I wanted to get out of it, but I think it’s been successful despite my lack of lofty ambitions. Back then, I hadn’t written any blog back then. So it’s a little strange that in the past year, I’ve had blog posts on another two other blogs aside from this one. It’s been quite a busy year too, so I’ve been sporadic in posting. Saying 52 posts in 52 weeks might sound well disciplined, but you can see that there have been weeks when I’ve not found time to write anything. (Which is a little lazy of me.) Other times I’ve been more inspired and written in bursts. That said, the fact I’ve still returned to it even after several weeks away is certainly auspicious. What I think I’ve got out of it is that I’m much more conscientious about scribbling notes down for my own reference, which I seldom did before having a blog. This has proven to be a useful reference. If I can’t quite remember how I did something, but know it’s on here, I can find it quickly. If I do write up things on here, it’s because it was a problem that I didn’t see an obvious answer to and took more than a cursory internet search or investigation on my own part. As well as that more practical side, it’s been a good excuse to work on my writing (a muscle that I don’t often flex at work). What about the coming year then? In this next year, two resolutions that I’d like to keep are: to try and maintain at least some kind of regularity to posting (even if my posts are clustered together sometimes) and migrate this blog to Pelican or something similar. For what it’s worth, I started looking into the second. It’s just a matter of ensuring I put the slight effort into seeing it through. Deciding what to do about comments, and actually going about exporting my existing content to Markdown or RestructuredText are the obstacles at the moment. Really though, it’s a question of just doing it rather than procrastinating and getting distracted writing blog posts about how I’m going to do things. ","date":"2014-05-10","objectID":"/posts/52-weeks-52-posts/:0:0","tags":["self-indulgence","musings"],"title":"52 weeks, 52 posts","uri":"/posts/52-weeks-52-posts/"},{"categories":null,"content":"Testing Testing is a useful programming practice which, in all honesty, I’m still learning much about. Tests can give you some reassurance as to the behaviour of your code, providing you’ve written your tests such that they’re testing appropriate things. They can also help you think about how you write your code, in a positive way: the process of making unit tests can require you to rethink how your code operates in order to make it testable. If you’re unfamiliar and want to know more, probably the single most useful resources I’ve found are two of Ned Batchelder’s PyCon talks, one from Pycon 2010, and one he recently gave. In the latter, he covers testing from the ground up. The former is a little less introductory, but is a good complimentary talk; he works through a few examples where writing tests can give a better organisation to code. ","date":"2014-05-10","objectID":"/posts/how-to-use-mock-in-python-to-mock/:1:0","tags":["mock","testing","test","unit","Python"],"title":"How to use mock in Python to mock methods on objects","uri":"/posts/how-to-use-mock-in-python-to-mock/"},{"categories":null,"content":"Testing and me My problems in creating tests are often two-fold. First, I’m not always sure what are good tests to write - for instance, unit tests should be isolated, but how much should I be mocking out in unit tests? For some functions, if you mock out all other function calls, you may have very little left to actually test there. Spotting which paths through the code you want to prioritise for integration tests is another issue. Ideally, you’d either work bottom up, starting with tests that mock out everything, then slowly adding in real code, or maybe top down, progressively mocking out units. However you approach it, the number of combinations of tests you could write will increase rapidly: even for moderately small programs, the number of possible combinations of mocked out functions and objects can be huge. For example, if A calls B and B calls C, I could write tests that use all three actual functions, two of them or just one (and perhaps even none of them). To exhaustively test every combination becomes unmanageable. I’m hoping the more I write tests, the more confident I’ll be at evaluating whether the tests I’m thinking of aren’t going to be that useful to implement. Where I know I should improve is the second problem I often encounter: how to actually implement tests. Often, I’ll find I know what I want to achieve but find difficulty in actually articulating this in code, usually because I’ve not found the functions I want in mock. ","date":"2014-05-10","objectID":"/posts/how-to-use-mock-in-python-to-mock/:2:0","tags":["mock","testing","test","unit","Python"],"title":"How to use mock in Python to mock methods on objects","uri":"/posts/how-to-use-mock-in-python-to-mock/"},{"categories":null,"content":"Mocking a method of an object with Python mock Anyway, here’s an example of this which might be useful if you’re trying to do something similar. In this case, I wanted to test a method of an object, but wasn’t sure how to do that using a mock. This is a really cut-down version of what my code was doing, but features the main thing I was testing: import sqlite3 def main(): table_name = 'some_table' sqlite_db = sqlite3.connect('database.sqlite') sqlite_db.execute(\"drop table if exists {};\".format(table_name)) # next, get some data and then save to db if __name__ == '__main__': main() As what the database saves depends on some input that the user supplies (fixed here), I wanted to ensure that the table is wiped before we add anything to it. Otherwise repeated runs would cause the table to become cluttered with data the user no longer wants.) What I specifically wanted to check was whether the database was getting the correct table name to drop. One way of checking this is using Python’s mock(a pip-installable package if you’re on Python 2, part of the standard library from Python 3.3). By replacing objects with mocks as stand-ins, we can avoid the need for actually connecting to a database in our test (which may be slow or unreliable, if it’s remote). Mocks aren’t just dummy objects though; it’s possible to inspect them after we execute some statements, allowing us to see, for instance, if the mock was called, and what arguments it was called with. In my case, I got particularly confused with the mocking out of the .execute method of the sqlite_db object. I’d tried all combinations of accessing the call via the mock_sqlite3_connect. What it took me ages to realise is that I had to mock out what sqlite3.connect() returns; you can’t (unless I’m mistaken) access calls on the object it returns otherwise. import unittest import code_to_test import mock class CodeToTestTestCase(unittest.TestCase): @mock.patch('code_to_test.sqlite3.connect') def test_database_drop_table_call(self, mock_sqlite3_connect): sqlite_execute_mock = mock.Mock() mock_sqlite3_connect.return_value = sqlite_execute_mock code_to_test.main() call = 'drop table if exists some_table;' sqlite_execute_mock.execute.assert_called_with(call) What’s happening here? First, we’re using a decorator, @mock.patch which replaces sqlite3.connect() in code_to_test with a mock, mock_sqlite3_connect. We can then make the return value of the mock_sqlite3_connect a mock itself. After that, all we have to do is actually call the main function which now will run with our mocks inside. Now, we can check that sqlite3.execute() was called correctly. If we save this as tests.py and run nosetests (pip install python-nose mock if you haven’t already), the test will pass. (What I think happens is that without us supplying a return value, the sqlite database is mocked, but there’s no easy way to access it within the test to check the method’s called. If you know I’m wrong and have an explanation, I’d love to hear it.) Given the solution, you might wonder why I’d been bashing my head against the keyboard for an hour or two to figure it out. It’s just knowing the vocabulary of what you need to do with mocks, whereas I’d got sidetracked thinking there must be a way to implicitly access methods of the object with the initial mock. Seeing and using patterns is definitely a good way to develop; next time I do something similar, I’ll be much quicker to get something going. ","date":"2014-05-10","objectID":"/posts/how-to-use-mock-in-python-to-mock/:3:0","tags":["mock","testing","test","unit","Python"],"title":"How to use mock in Python to mock methods on objects","uri":"/posts/how-to-use-mock-in-python-to-mock/"},{"categories":null,"content":"A quick reminder to myself more than anything. Had the issue of a strange ImportError: from itertools import tee. It’s a little odd because itertools is a Python built-in module. Running Python and doing the import from the directory where I was running code worked fine. However, running a script in that directory that was importing a package, that itself imported itertools gave this behaviour. I wasn’t really sure what was causing it, but on a whim I deleted the existing itertools.pyc file in that directory. When I looked, it turns out that itertools had a pyc file in the package directory. If you have inexplicable errors like this with no obvious explanation, it’s worth perhaps deleting the cached .pyc files. That is, unless this applies to your coding workflow. ","date":"2014-05-09","objectID":"/posts/beware-pythons-pyc-files/:0:0","tags":["Python","pyc"],"title":"Beware Python's pyc files","uri":"/posts/beware-pythons-pyc-files/"},{"categories":null,"content":"Even if you’re not particularly into technology, you probably wouldn’t have escaped hearing about Heartbleed last month. Without going into details here — the site written by researchers who disclosed the vulnerability has a detailed technical summary, and XKCD had a nice non-technical summary — it affected servers using OpenSSL and meant that it was possible for attackers to retrieve confidential information (for example, passwords) or even the private keys of servers. It’s old news now; a month’s passed since. What was striking about the whole incident is that, despite the initial fuss in both the specialist press and more general news sources, the communication of the status of web services was conspicuous by its absence. Out of the hundred or so sites that I have accounts on, just one of them emailed me to reassure me that they weren’t exposed to Heartbleed. The only clear indication I saw while visiting a site was on Soundcloud, who advised users to change passwords. From some of the lists of popular sites that were being maintained at the time, I know there was at least one site that I had an account on that were at some point exposed to Heartbleed. Yet, as far as I can tell, they didn’t bother to inform their users that. I’m not sure what their motivation was; an email or blog post/banner on site would have been really helpful. Maybe owners of sites wanted to avoid alarm, but I’d personally prefer a clear reassurance that everything is OK with them. It’s unrealistic to expect that some widespread security issue won’t happen again. If something does happen though, I do hope that sites do a better job in clarifying their position. ","date":"2014-05-04","objectID":"/posts/heartbleed-ill-communication/:0:0","tags":["Heartbleed","security","communication"],"title":"Heartbleed: ill communication","uri":"/posts/heartbleed-ill-communication/"},{"categories":null,"content":"Been playing around with third-party VSTs in FL Studio and found I couldn’t get the modulation wheel to work (though it works fine in the bundled plugins). Eventually I found the fix here. The short version: add a third-party VST to your project. Go to the browser menu and go to Current Project \u003e Generators \u003e Fruity Wrapper. Next, you should see Modulation Wheel in the list under Fruity Wrapper; right click it and choose “Link to controller”. Enable Omni, make sure Autodetect is enabled (it is by default for me), then move the wheel. That link also suggests starting with an empty project, doing this setup, then deleting the VST and saving the project as a template, which is handy to avoid repeating this every time you work on a new project. Just make a sub-directory in one of the existing FL Studio Templates directories and save the .flp file in there. ","date":"2014-04-27","objectID":"/posts/making-midi-keyboard-modulation-wheel/:0:0","tags":["MIDI","production","keyboard","FL","VST","Studio","music","audio"],"title":"Making a MIDI keyboard modulation wheel work with third-party VSTs in FL Studio","uri":"/posts/making-midi-keyboard-modulation-wheel/"},{"categories":null,"content":"The US PyCon took place over the last week or so and there are a huge selection of videos of the talks up at pyvideo.org. Still haven’t got through all of the ones I want to take a look at, but the ones I’ve checked already were pretty good. ","date":"2014-04-19","objectID":"/posts/us-pycon-2014-talks/:0:0","tags":["scraping","lxml","BeautifulSoup","regular","testing","Python","expressions"],"title":"US PyCon 2014 talks","uri":"/posts/us-pycon-2014-talks/"},{"categories":null,"content":"Regular expressions and testing Out of those I saw, I can definitely recommend two great mini tutorial talks; one on testing and one on regular expressions. What I loved about both is that they started from pretty much zero knowledge and built up on that, without seeming overwhelming. Luke Sneeringer’s talk on regular expressions was really helpful consolidation for me. Occasionally I’ve used regular expressions before, but I tend to play around with them in an online debugger like Regex101 until they work. I usually learn enough at the time to get the job done, then promptly forget everything until the next time. Hopefully, some of this talk will stick (and at least it’s on YouTube to refer back to, if not). Here, the start of the talk threw down what looked like intimidating expressions, then, over the course of a brisk twenty-five minute talk, gave the viewer the tools to go back and understand it. It was also a decent reference if you aren’t even using Python: there are a few bits of Python-specifics, but the focus was on the expressions, not so much code. Ned Batchelder’s talk on how to start with testing was great too. When I first was trying to start testing my code last year, I knew this was important but I never found a good step-by-step guide of how to get started from scratch. The talk starts with showing that automated tests are better than a coder manually running statements to check their code is working as it should (less work!). It then leads into showing why test runners are better than you just writing your own code to run tests. For instance, they help you to isolate individual tests, ensuring that each starts from a clean state and have failure and error handling built-in already. Finally, test doubles are covered. At first, painstakingly hand-crafted doubles are used, then this is simplified using the mock library. Doubles are particularly handy when your code is using external resources, for instance accessing a database or a web site; in these cases, your tests may run slowly, and you may not be able to guarantee the result (e.g. if a web page changes). Again, I’d seen, and have used, several of these ideas before, but to see them all presented in one place, and explained so well, makes it a good resource. ","date":"2014-04-19","objectID":"/posts/us-pycon-2014-talks/:1:0","tags":["scraping","lxml","BeautifulSoup","regular","testing","Python","expressions"],"title":"US PyCon 2014 talks","uri":"/posts/us-pycon-2014-talks/"},{"categories":null,"content":"Scraping Since I do a bit of web scraping, I also skimmed a couple of presentations given by Katharine Jarmul. Her tutorial looked pretty good from what I saw, covering the basics of using BeautifulSoup, lxml and Selenium for basic scraping. If you’re wondering where to start with scraping in Python, it’s worth a look. She also gave a talk comparing the performance of these scraping tools; lxml using XPath was much faster in her tests than both lxml using CSS selectors and BeautifulSoup. That said, even BeautifulSoup was still only taking a fraction of a second in the cases she looked at. Incidentally, a quick glance at the code shows that the HTML was being read locally (and I understand why given the variability inherent in accessing this information over networks and from a remote server). So I don’t think that this should have much of an influence on your choice of Python scraping library; use whatever you’re comfortable with to get the job done. The slow part is still likely to be accessing the content from the site. Especially if you’re being considerate and throttling your requests to a site. Finally, I’m guessing that this talk delving into rap lyrics is probably one of the few given at a coding conference where you can hear a snippet of N.W.A.’s “Straight Outta Compton”. If you watch any particularly thought-provoking or useful talks, please let me know! There’s way too many to practically sit through. ","date":"2014-04-19","objectID":"/posts/us-pycon-2014-talks/:2:0","tags":["scraping","lxml","BeautifulSoup","regular","testing","Python","expressions"],"title":"US PyCon 2014 talks","uri":"/posts/us-pycon-2014-talks/"},{"categories":null,"content":"I do a lot of listening to music and podcasts with a trusty old second generation iPod Nano running Rockbox. What XBMC is to video playback, Rockbox is for audio playback. (I should get around to writing up what makes it that much better than stock firmware.) Anyway, I thought I’d leave a note here about the issue I discovered today since it took me several searches to yield anything relevant and I spent a good half-hour or so diagnosing the problem. When I was listening to something I’d recorded myself, I noticed that the left and right channels were switched. After checking the original audio I recorded on my PC, different headphones, and several different versions of Rockbox I realised it was actually a Rockbox problem. Earlier versions don’t suffer from this issue (for example, the much older 3.7.1). In the end, searching Rockbox’s issue tracker eventually confirmed this: the swapping of channels is a known bug, though no-one’s fixed it yet. Not a big deal, and the easy fix, of course, is to just switch headphones around. ","date":"2014-04-08","objectID":"/posts/rockbox-ipod-nano-2g-and-inverted-audio/:0:0","tags":["Rockbox","Nano","iPod","bug"],"title":"Rockbox, iPod Nano 2G and inverted audio channels","uri":"/posts/rockbox-ipod-nano-2g-and-inverted-audio/"},{"categories":null,"content":"If you’ve read other posts here, you might notice that I’ve gone through a phase of encrypting new devices I have. My new phone is now no different. In trying to research what problems can arise if you encrypt Android, it did seem like encryption isn’t a widely used feature. But, as I was starting afresh with my phone anyway, it seemed a good time to test it out. On my phone, out of the box, it encrypted in just a few minutes. I didn’t bother running any before and after benchmarks, but it doesn’t feel like there’s any impact on the responsiveness of the phone. There are two downsides though. One is the irritating feature that the encryption password is just the screen unlock password you use. This isn’t great. It either means you need a painfully complicated screen lock passphrase, everytime you want to unlock your phone, or it means you’ll have a simple encryption password. That issue discussion highlights that it is possible to change the encryption password, but this requires you to enter a shell command as root. There’s an app that adds a convenient frontend to this process, though it still requires root. As the README there points out, watch out: changing your screen unlock code will change your encryption password to this too! The second problem is that encryption can impact on your use of recovery. ClockworkModRecovery doesn’t support encrypted devices. TWRP apparently does, but didn’t seem to work with my Moto G; it should prompt for an encryption password, but doesn’t and just fails to mount the phone’s partitions. That’s not a huge problem for me, but I expect that this is going to prevent my phone from receiving official updates. Perhaps reflashing a manufacturer recovery and updating will work? In the (only slightly inconvenient) worst case, I’ll just have to backup everything, do a fresh install, re-root and restore everything. It would be nice if encryption on Android was more user-friendly, though as the problems aren’t particularly onerous, the extra security is definitely welcome. ","date":"2014-04-05","objectID":"/posts/android-device-encryption-is-mostly-good/:0:0","tags":["encryption","Android"],"title":"Android device encryption is (mostly) good","uri":"/posts/android-device-encryption-is-mostly-good/"},{"categories":null,"content":"G300 to Moto G My G300 has been getting tired. It’s a little irritating that I know it’s inherently defective. There’s a flash memory issue which has resulted in dead phones if they had the wrong type of flash memory (which mine unfortunately has). I’m a little muddled from my reading, so I don’t know if this death sentence is caused by, or just exacerbated by the use of custom ROMs. Anyway, the safest thing to do was to stick with an older CyanogenMod version. It’s annoying that I was stuck on old Android and the phone’s slowness has started to frustrate me, leading me to decide to get a new one. I did consider a Nexus 5, but heard bad things about battery life which put me off; I thought I’d get frustrated with paying quite a lot (although they were on offer for £240 recently) and still not being quite satisfied. Instead, the value for money budget smartphone at the moment seems to be the Moto G, so I picked one up for a penny under £100. From first glance, it seems a great deal for what you get: nice 720p screen, front facing camera, quad core CPU; all a sizable upgrade over my G300 (which was around the same price about eighteen months ago). I haven’t had much chance to use it yet. Initially, I was having issues rooting the phone with Motorola’s server not recognising the phone’s unlock data. A couple of days later all went fine, but I’ve only just got time to start setting it up again. ","date":"2014-03-28","objectID":"/posts/phone-upgrades-and-privacy-downgrades/:1:0","tags":["Moto","G300","XPrivacy","Android","PDroid","Auto-Patcher"],"title":"Phone upgrades and privacy downgrades","uri":"/posts/phone-upgrades-and-privacy-downgrades/"},{"categories":null,"content":"PDroid to XPrivacy By setting it up, I mean the necessary business of moving existing data across from my old phone, as well as making sure my phone’s rooted and running an ad-blocker and something to control app permissions for accessing my data. I’m a big fan of PDroid on my current phone to restrict app permissions. Especially so these days with so many apps wanting unnecessary permissions. I don’t even install that many third-party apps, but I find it incredibly invasive when apps that don’t have any reasonable reason to, say, know your location want to find where you are. Since I last installed a custom ROM, I found that Auto-Patcher has been deprecated and XPrivacy seems to be the permission control app of choice for Android now. You still need root permissions to use it, though it doesn’t require ROM patching, which means you can use it with a stock, rooted ROM, and install it directly from Play Store. There’s a CyanogenMod port for Moto G, but it’s only nightly at the moment, so I think I’ll try and see how I get on with stock for now. ","date":"2014-03-28","objectID":"/posts/phone-upgrades-and-privacy-downgrades/:2:0","tags":["Moto","G300","XPrivacy","Android","PDroid","Auto-Patcher"],"title":"Phone upgrades and privacy downgrades","uri":"/posts/phone-upgrades-and-privacy-downgrades/"},{"categories":null,"content":"Blogger and blogging To me, it doesn’t seem that long ago, but I wrote the first post on this blog ten months ago. Funnily enough, it discussed what I’m thinking about right now: how best to blog. Writing using the Blogger editor is not a great experience. Not great, but adequate. It removes that the need for the user to know HTML. With that, it also removes the possibility to do anything beyond the limited formatting you get with that. (Some of my use cases, for instance, include highlighting code/command lines to offset them from text, and using footnotes.) I’ve had a couple of instances where, largely through my own fault, I’ve been switching between posts, hitting back and ended up wiping out my work. After those experiences, I decided to just draft posts in a text editor, then paste in and markup the HTML. I find it quite clunky to mark up text posts as HTML, even basic things like paragraphs need tagging, and having to open and close tags constantly starts getting distracting. After speaking with drj this week about him considering migrating his Wordpress blog to Jekyll, I had the inspiration of starting to write blog posts in Markdown and then converting them to HTML. ","date":"2014-03-22","objectID":"/posts/full-circle/:1:0","tags":["Wordpress","blog","static","Jekyll","blogging","site","Pelican"],"title":"Full circle","uri":"/posts/full-circle/"},{"categories":null,"content":"What’s Markdown? You might be asking that if you’re not a developer. I hadn’t heard of it at all before I started using GitHub, but I’m a convert now. It’s a no-nonsense tool that converts plain text to HTML. Grab your drink of choice and spend the whole five minutes you’ll need to learn to use it. There are several online editors like this one that you can test out. Adding formatting is really simple, for instance, *putting text between asterisks* in Markdown would make it italic. It means you can focus on writing, instead of getting distracted fiddling around with HTML tags. Furthermore, there are fantastic tools such as pandoc that will convert Markdown to a host of other outputs too, e.g. PDF via LaTeX, Word docx, LibreOffice, even the epub ebook format. ","date":"2014-03-22","objectID":"/posts/full-circle/:2:0","tags":["Wordpress","blog","static","Jekyll","blogging","site","Pelican"],"title":"Full circle","uri":"/posts/full-circle/"},{"categories":null,"content":"What next? After finding that I prefer writing posts in Markdown, my next thought was: why not remove the middleman and use a system, like Jekyll, that directly uses Markdown? I’m feeling this particularly so now, given that I’m starting to feel like Blogger is limiting. By default, the provided Blogger templates look dated. (And feature some odd design choices too: why is text in a h2 tag smaller than that in a h3 tag?) I’ve not had time to start trying to modify it or look for a better one. When I see my posts look beautiful in an online Markdown preview, and then sending them to the horrors of my (pretty much stock) Blogger template where they are rendered so unappealing is always going to make me sigh. The content is the most important thing, of course, but a poorly designed website is always going to distract. It’s also clunky to try to customise certain aspects of Blogger, and having my own blog setup might simplify this. Being completely in control of my blog, rather than having parts of it in hidden black boxes appeals too. One example is that someone reading an article commented that the mobile version of the blog was irritating because it disabled zooming; nothing that I could do really do about that. This isn’t to take away from Blogger being an easy way to get started. It’s simple to use and definitely helped me share my writing. Without having to worry about choosing a host and managing some blogging system, it was easy to get going. Login to your Google account and a few clicks will get your first post published. It being free of charge helps too. So, I now have a project whenever I have a free weekend next: look into static site blogging with Jekyll/Octopress (Ruby based) or perhaps Pelican (Python based) and hosting on GitHub Pages. I’ll definitely write up what I find. Finally, why consider these static site generators, and not my originally suggested move to WordPress? WordPress itself is free to use, but hosting WordPress somewhere costs money. It also gets me into the quagmire of shopping around for a host and hosting plan, and managing security updates. Once the static page generator is initially setup, I can deploy the site to GitHub Pages, then focus on writing again. ","date":"2014-03-22","objectID":"/posts/full-circle/:3:0","tags":["Wordpress","blog","static","Jekyll","blogging","site","Pelican"],"title":"Full circle","uri":"/posts/full-circle/"},{"categories":null,"content":"Even if you have no idea what open source software is, it’s likely you’ve either used some or, at the least, had some indirect experience with it. Maybe you’re using a Linux desktop to read this. Even if not, Linux is present in various embedded devices particularly Android. Maybe you’ve read books or documents that have been written in LaTeX, or maybe you’ve used LibreOffice (a cost-free alternative to Microsoft Office). Maybe you’ve watched videos with VLC media player and maybe someone’s used tools like Blender or VirtualDub in making that video, or Audacity to edit the audio. And what web browser are you running there? If it’s Firefox, that’s open source, and Chrome is based on an open source browser. The most important thing about open source software is not that it’s free of cost (closed source software can be freeware too, of course), but that the software’s source code is available. Assuming the licence is amenable to it, this often means that you’re free to modify it and even redistribute your changes. As I’d personally benefited from using so much open source software in the past, I felt that I’d like to start making contributions back if I could. However, it may not be immediately obvious how you can get started, particularly if you’re not a developer. Here are a few suggestions. ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:0:0","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"If you can code: ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:1:0","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Share your own projects GitHub and Bitbucket both allow you to publish your own code as Git repositories, making them available to anyone. The sharing aspect aside, it also provides you with a public backup (incidentally, Bitbucket offers free private repositories too), and means that you can access your code from any computer with an internet connection. ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:1:1","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Add features or fix bugs in other open source projects GitHub suggest that you look at popular or trending repositories which I guess may alert you to something that you didn’t know about. Each repository will probably have several open issues that you could consider working on. Another resource is openhatch which lists projects in need of fixes, though if the Python projects are anything to go by, the number of projects covered on there seemed to be fairly small. If you’re using existing software, you may well encounter issues yourself that you want to fix for your own use case. I think I’d find this more motivating than just trying to pick out some project ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:1:2","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"If you’re not a developer: From the outside looking in, it’s easy to think along the lines of, “software is code, so if I can’t code, I can’t contribute”. That couldn’t be more wrong. There’s much more to software projects than just the code, and there are still valuable ways for you to contribute that would be no less appreciated by the owners of a project. ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:2:0","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Write or translate At a very basic level, submitting documentation fixes or improvements is a great help as it helps other users get started. If no documentation, start writing some! If you speak other languages, the developers may want documentation or strings in the software to be translated. The fact projects are on GitHub and Bitbucket can look intimidating for you to make a contribution. But, on GitHub, it’s a particularly simple process to submit changes to text. If you’re logged in, click on a text file you want to edit, click Edit and it will create a separate copy of the project’s repository under your account. All you need to do is to make your changes, click “Propose file change”, then click “Send pull request” on the next page. The owner of the project will be notified that you’ve suggested some changes, which they can easily pull back into their project from their copy. ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:2:1","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Raise issues While you’re using an open source project, you might spot a bug or think of something which would make a neat feature. If you check the available issue trackers for anyone mentioning something similar and don’t find anything, it’s probably worth highlighting. From my own experience, I wrote a script that grabs tracklistings for BBC radio shows that I’d downloaded using get_iplayer. Someone used it, a user contacted me via GitHub’s issue tracker to say that it shouldn’t just save them to text files, but it should also tag the audio file too. This was something I hadn’t thought of, but it seemed a nifty feature so I was happy to work on adding it as I felt it improved the software. ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:2:2","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Design Maybe a project is looking for designers, for assets in the software (e.g. icons) or for their website? ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:2:3","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Contribute to the community Is there a piece of open source software that you know a lot about? Can you help out other users questions on an official forum, user group or mailing list, or somewhere on the Stack Exchange sites? Tell others about cool projects you’ve found, without marketing budgets, much open source software relies on word of mouth. For example, this week, I discovered waveshop which is a lightweight audio editor (more so than Audacity) thanks to this post on kvraudio ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:2:4","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Donate Some open source projects have contributions made by paid employees, but many projects are developed by enthusiasts without any funding and in their free time, so money must be a good thing, right? Jeff Atwood’s experience of making a sizable donation to an open source project suggests that it might not be so straightforward. Unfortunately, small donations don’t buy the developers more time which is one of the most important things they need to continue developing software. (Though sufficient money may be enough to hire developers or allow people to quit their day jobs to work on projects.) In some cases, certainly, developers may well welcome donations to acquire hardware for testing. ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:2:5","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Conclusion There are plenty of ways to get involved. The next time you’re downloading an open source project, maybe consider taking a look around their web pages or their repositories to see if there’s anything you can help with. ","date":"2014-03-21","objectID":"/posts/how-to-give-back-to-open-source-software/:3:0","tags":["documentation","open","software","source","coding"],"title":"How to give back to open source software","uri":"/posts/how-to-give-back-to-open-source-software/"},{"categories":null,"content":"Today I saw this clip which declares that desktop PCs are ready to be consigned to the dustbin. To some extent, they’re absolutely right. Laptops, tablets and smartphones are the mainstream computing devices right now. If you’re in the UK and on a train journey, take a look around. (You’ll probably need to drag your eyes away from your own tablet or smartphone first.) From my own experience, I know that my parents now spend most of their home computing time using tablets and don’t find this limiting. Then again, I’m quite the opposite. Last year, I reluctantly bought my first laptop for work, and still haven’t been converted to mobile devices. So, why such distinct differences between the needs of mainstream users and enthusiasts? Here’s what my experiences tell me. ","date":"2014-03-17","objectID":"/posts/the-death-of-desktop-pc/:0:0","tags":["laptop","tablet","desktop","PC"],"title":"The death of the desktop PC?","uri":"/posts/the-death-of-desktop-pc/"},{"categories":null,"content":"Upgradability First off, even before you purchase a laptop you run into problems if you’re particularly interested in the computer you end up with. There are a huge number of different machines out there. However, laptops have limited upgradability: they’re designed to fit into a compact form factor, making CPU, graphics or motherboard upgrades impractical or near impossible. So, you’re stuck with the fixed out-of-box specification. And scouring the market this way to both find a machine that meets your needs and is within your budget is not fun at all. Going the desktop self-build route however, you can specify every part from the ground up. Building your own desktop also means you can defer some of your spending rather than having to try and futureproof by buying a top-end laptop. You’ve got the option of using parts from older PCs as a stopgap, or settling for more inexpensive components that you can upgrade in a year or two. Laptops not being easily user-servicable means that many repairs that would be relatively simple in a desktop (take out broken part, install new part) may well involve sending your laptop to the manufacturer or repair shop, rather frustratingly. ","date":"2014-03-17","objectID":"/posts/the-death-of-desktop-pc/:1:0","tags":["laptop","tablet","desktop","PC"],"title":"The death of the desktop PC?","uri":"/posts/the-death-of-desktop-pc/"},{"categories":null,"content":"Usability Laptops come with built-in keyboards and trackpads which I find awkward. I almost always use other input devices with my laptop. Trackpads are just too imprecise and inefficient compared with mice, while laptop keyboards can be more cramped or may be missing the numeric keypad. (My laptop’s keyboard isn’t actually that terrible, though I’m still not a fan of chiclet keyboards.) Another issue with laptops is the display. At the cheap end, laptop screens are low resolution and low quality. Even at the high end, you’re inherently limited by the size of the device: high resolution displays are great, but if they’re on, say, a 13 inch screen, everything’s tiny. I much prefer working on larger (\u003e 22 inch) monitors, and I always favour plugging my laptop into an external monitor. Having to work around the limitations of a low resolution screen by constantly scrolling or shuffling windows around is distracting. ","date":"2014-03-17","objectID":"/posts/the-death-of-desktop-pc/:2:0","tags":["laptop","tablet","desktop","PC"],"title":"The death of the desktop PC?","uri":"/posts/the-death-of-desktop-pc/"},{"categories":null,"content":"Power Laptops are portable machines which has two consequences. Obviously, they need to be small, which means there’s no room for the several fans and large heatsinks that desktop cases can accommodate. They also need to be power efficient since they may well be running from battery power. Both of these factors limit how powerful laptops can be, meaning desktop hardware is the way to go for high-end consumer computing applications. This also limits them for gaming. For me, this isn’t a big concern; I don’t have much time for gaming these days and the PC games I have played have been indie titles with modest requirements (my almost seven year old desktop still handles these well). However, there’s a large community of PC gamers who won’t be satisfied by the compromises in performance they’d have to make on choosing a laptop. ","date":"2014-03-17","objectID":"/posts/the-death-of-desktop-pc/:3:0","tags":["laptop","tablet","desktop","PC"],"title":"The death of the desktop PC?","uri":"/posts/the-death-of-desktop-pc/"},{"categories":null,"content":"Phones and tablets versus conventional PCs Since they can run the same software, I’ve focused on desktops versus laptops as these devices are, in principle at least, interchangeable, and I’ve not mentioned phones and tablets. Apart from their portability, one driver of the success of phones and tablets is probably that they are much simpler to use and maintain than full-fledged PCs. Simplified touch interfaces and software management, along and a relative lack of malware (particularly on iOS) probably all contribute. Anecdotally, I get asked far fewer questions about tablets or phones than I ever did about Windows. At the same time, this simplicity is exactly why I find these devices so limiting. Compounding that, phones and tablets are usually even more restricted in storage and computing power than laptops. For uses like video and image editing, audio work, and software development, it’s hard to see phones and tablets displacing conventional computers right now; current mobile apps are typically far simpler than these uses demand. One possibility is that dual boot phones or tablets may eventually displace laptops completely; carry around your phone then plug into a monitor, keyboard and mouse when you want to do more than apps allow. Current events point to this perhaps taking a long time. Though they’re of little interest to mainstream users, I think the reasons I’ve given here suggest that desktops are far from redundant. Relative to the ubiquity they had a decade ago, they’ll occupy a much smaller niche compared to the market dominance they once had, but this is still an important one for computer enthusiasts. ","date":"2014-03-17","objectID":"/posts/the-death-of-desktop-pc/:4:0","tags":["laptop","tablet","desktop","PC"],"title":"The death of the desktop PC?","uri":"/posts/the-death-of-desktop-pc/"},{"categories":null,"content":"So, I was tinkering around with my Raspberry Pi (running Raspbian) over SSH and in doing so found that when I was looking up info about commands using man that although man was still running perfectly fine, I was getting this message: man: can't resolve /usr/share/man/man6/LS.6.gz: No such file or directory The fix is actually a simple one; just run sudo mandb which updates man’s database caches. Looking more closely, I found that a mandb job was actually in cron.daily which means that it should run daily. However, when I checked the syslog with: grep cron.daily /var/log/syslog there were no results indicating it hadn’t actually run! The reason for this is that my Pi’s not switched on long enough for the cron job to run. I usually leave it on for a couple of hours and then shutdown once I’ve finished with it. ","date":"2014-01-31","objectID":"/posts/ensuring-scheduled-cron-jobs-are-run/:0:0","tags":["Raspbian","anacron","Pi","Linux","Raspberry","cron","schedule"],"title":"Ensuring scheduled cron jobs are run using anacron","uri":"/posts/ensuring-scheduled-cron-jobs-are-run/"},{"categories":null,"content":"Installing anacron is the solution! If you install anacron: sudo apt-get install anacron, you should find an /etc/anacrontab containing lines like: 1 5 cron.daily run-parts --report /etc/cron.daily 7 10 cron.weekly run-parts --report /etc/cron.weekly @monthly 15 cron.monthly run-parts --report /etc/cron.monthly This means that anacron is going to run the jobs in /etc/cron.daily, /etc/cron.weekly and /etc/cron.monthly. It should do this at boot up by default (you can check /etc/init.d/anacron). There’s then a user-specified delay before anacron runs each task; here, it’s 5 minutes for /etc/cron.daily, 10 minutes for /etc/cron.weekly and 15 minutes for /etc/cron.monthly. If you then restart and wait five minutes or so, then do: grep cron.daily /var/log/syslog should show lines like: Jan 31 10:28:02 raspberrypi anacron[2105]: Will run job `cron.daily' in 5 min. Jan 31 10:33:21 raspberrypi anacron[2105]: Job `cron.daily' started Jan 31 10:33:21 raspberrypi anacron[2692]: Updated timestamp for job `cron.daily' to 2014-01-31 Jan 31 10:33:29 raspberrypi anacron[2105]: Job `cron.daily' terminated This shows that anacron is doing exactly what we want: running the cron.daily job five minutes after we boot. It wasn’t that obvious to me how the existing cron scheduler interacts with anacron (nor that easy to search for) but this site gave a very clear and helpful explanation of how the two interoperate. ","date":"2014-01-31","objectID":"/posts/ensuring-scheduled-cron-jobs-are-run/:1:0","tags":["Raspbian","anacron","Pi","Linux","Raspberry","cron","schedule"],"title":"Ensuring scheduled cron jobs are run using anacron","uri":"/posts/ensuring-scheduled-cron-jobs-are-run/"},{"categories":null,"content":"Having recently installed a new Windows PC I’d installed an Epson printer (SX515W). The printer itself was connected via wifi and behaving apart from one thing. It wasn’t displaying the ink levels when checking them in Epson Status Monitor (e.g. when sending a document to print). Instead, you just see the message “Searching…\"1 and eventually get a Communication Error. In my experience, printer manufacturers do seem to want to install a vast number of clunky utilities, setup helpers and other cruft, so I’m always wary about just installing everything just to get the thing running. Anyway, this post gave me the easy fix: install Epson’s Network Utility. That message made me revisit a favourite track from 2013. ↩︎ ","date":"2014-01-18","objectID":"/posts/fixing-no-ink-levels-being-displayed-in/:0:0","tags":["level","printer","ink","Epson","driver"],"title":"Fixing no ink levels being displayed in Epson Status Monitor","uri":"/posts/fixing-no-ink-levels-being-displayed-in/"},{"categories":null,"content":"2014/02/09: An edited version of this post was recently republished on DJ TechTools. ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:0:0","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"DJ software: complete with the kitsch in sync There’s a view — one I guess is primarily propagated by people who were DJing long before the likes of Traktor and Serato1 came along — that using software on laptops doesn’t really “count” as DJing because you don’t need to beatmatch. Algorithms for beat detection usually work well enough these days that software can usually have a good go at this for you. It’s a slightly odd opinion though, and one that I actually think does DJing a disservice. The implication of beatmatching being so important is that there’s little more to DJing than that. Just get the beats matching up or push the sync button, and, that’s it, congratulations, you are now a DJ and your membership card will be in the post! What’s strange to me is that you’d think, as DJs, they’d be aware of the importance of everything other than beatmatching that goes into even a simple A-B mix. Track selection and sequencing, transitions between tracks, EQing, harmonic mixing are things that sync won’t handle. Software’s not really going to easily handle this for you2 as a lot of it is down to personal preference and choice, as well as the type of music you’re playing. (Oh, sync is entirely optional too: no-one forces you to push that button.) ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:1:0","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"Why beatmatching is worth your time Having said all that, there are still good reasons why it’s still worth learning: Lets you use other equipment. Imagine you only ever rode a bicycle with stabiliser wheels; great, you can ride a bike all day long. Except the time you have to use a bike without stabilisers instead. It means it shouldn’t take be too much of a struggle to adjust if you end up playing on CDJs or vinyl/DVS. Sync is good, but not always perfect. Annoyingly, Traktor still uses a fixed beatgrid. (I think Serato actually has warp markers, but I’m unfamiliar with it, so feel free to correct me on this.) Provided a track has a well-defined bpm, which most electronic dance music does, it does pretty well at detecting it. However, a simple beatgrid can break down with tracks with live, unquantised percussion or where there are particularly unusual rhythms or switch ups.. Knowing how to handle cases where sync fails completely or where minor adjustments (e.g. with tempo bend) are necessary is essential to keep beats from drifting. No need to analyze and beatgrid tracks in advance. This isn’t a bad thing to take the time to do: with Traktor: tempo-based effects and looping can sound off if the track does not have the correct beatgrid. On the other hand, being able to throw a new track into software and immediately play it without worry is really convenient. Mixing into the end of someone else’s set. Some DJs will start afresh; some will beatmatch into it. Choice is a good thing. No DJ software beat matches to an external sound source as far as I’m aware. (I’m willing to be corrected again though; it’s not just me that’s thought of this.) Beatmatching is good practice for listening closely, especially when you’re trying to selectively “tune” in and out from the track being faded in. This is useful regardless of how you’re beatmatching (e.g. for EQ purposes). ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:2:0","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"What Traktor offers Traktor has several features to aid beatmatching. If you’re learning to do this manually, the natural inclination is to think, “OK, if I’m learning to do this myself, I shouldn’t use any of these tools .” But, when you’re learning to manually beatmatch, these features are still really useful. So what do we have? ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:3:0","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"Phase meter You can practice with the phase meter to see how far off you are after you think you’ve beatmatched correctly, or just check how far off the beat you are when hitting play or scratching in a track. The phase meter shows you how far ahead or behind the current track is relative to the track that’s currently the master. If the bar is in the middle of the phase meter, the track is in phase with the master. In the example above, the track is over a quarter of a beat ahead. (Note that both tracks you’re playing need to have correct beatgrids otherwise the phase meter is likely to be misleading.) Of course, you can always use your ears for this, but the phase meter gives you a quick visual indication of how well you’ve done. ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:3:1","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"BPM counter You can check BPMs immediately when you think you’ve matched tracks as well as possible3. One easy way is just to click the GRID option directly below a deck (you can also add a permanent BPM counter in Preferences \u003e Track Decks \u003e Deck Header.) This gives you a direct indication of how far off a match you actually were; if you’re only a couple of hundredths of a BPM out, you’re doing pretty well. Importantly, you can see that you’re improving even if you haven’t quite got the hang of it. To reuse the bike analogy, the learning curve is pretty steep. First, you fall off a lot, then you start riding albeit a little wobbly. I found beatmatching pretty similar: for quite a while, I couldn’t manage it, then all of a sudden I could (though quite badly). You then become steadier and grow in confidence with practice. (One gotcha with beatgridding is that if Traktor may well have detected a BPM half or double what it should be, e.g. tracks at 140 BPM might be detected as 70. If the BPMs of the two tracks are reportedly wildly different, but you seem to have beatmatched them nonetheless, that’s probably why.) ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:3:2","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"Sync One exercise often mentioned in beginner beatmatching tutorials is to mix two identical tracks. When I was starting out, I found this sounds cluttered and confused me a little. Unsurprisingly, it’s sometimes really hard to distinguish the two tracks as they sound the same. Using different tracks might be easier. However, the tracks you’re using might not be the same speed which leads to the chicken-and-egg problem of how can you beatmatch before you’ve learnt to beatmatch? In this case, the sync button should handle it for you with one push of a button. If you’re not sure what a “perfect” mix of the current two tracks should sound like, the sync button will give you that too. You can then nudge one track slightly out of phase with the other using tempo bend to easily hear the difference, say, when a monitored track is ahead versus the live track; also useful in developing beatmatching skills. ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:3:3","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"Recording It’s not always easy to judge how well your beatmatching is while you’re actually doing it. Recording your practice and listening back to see how it actually sounds to a listener is pretty simple with one click. Are the beats matched together perfectly or not? Did you let the beats drift at all through a blend? ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:3:4","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"A comment on MIDI controllers One design choice that I would criticise about many MIDI controllers is that their pitch faders tend to be on the small side. This makes it difficult to accurately beatmatch, since precise adjustments (hundredths of a BPM) are tricky. One workaround is to map a knob as a fine pitch control with a much smaller range. This lets you lock in to the rough BPM using the pitch fader, then use the knob to make very small corrections. ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:3:5","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"Summary As long as software doesn’t perfectly get beatmatching correctly with zero preparation, there’s still a need for beatmatching. Not only that, software can actually help you practice and improve too: I self-taught myself beatmatching with Traktor and a MIDI controller. Let me know if you’ve done the same. Also see open source alternatives like Mixxx and xwax. Mixxx in particular looks really promising. Unfortunately, my controller isn’t really supported (the pitch faders don’t work correctly when I last tried, using a old VCI-100 running DJ TechTools v1.4 firmware to improve the resolution of the jogwheels and pitch faders.) ↩︎ Note that there are key detection algorithms that can help guide harmonic mixing. There two big problems though. One is that they’re not perfect yet; the estimates there (on a small set of songs, admittedly) is around 60-70% accuracy. And, if they were, all that does is tell you the current track’s key. Even if you select your next track on the basis of key, it’s not going to decide this for you and there are several choices. Do you stick to the same key? Do you pick the relative major/minor, or the parallel major/minor or even ascend/descend by a perfect fifth? Or completely eschew all that and perhaps choose something in a different key, maybe bringing it in a non-melodic section? ↩︎ Again, you need to make sure that the tracks you’re using for practice have been correctly beatgridded. ↩︎ ","date":"2014-01-16","objectID":"/posts/beatmatching-five-reasons-why-digital/:4:0","tags":["sync","DJ","laptop","Traktor","beatmatching","digital"],"title":"Beatmatching; five reasons why digital DJs should learn, and how Traktor can help","uri":"/posts/beatmatching-five-reasons-why-digital/"},{"categories":null,"content":"I recently read “Data Science for Business” by Foster Provost and Tom Fawcett for the upcoming Data Science Book Club that Nichole Elizabeth DeMeré is putting together. As the title suggests, it’s pitched primarily at business people involved in data science projects. For me, it covered some familiar territory I’ve encountered either via introductory courses or through work (e.g. hierarchical and k-means clustering, text processing, classifiers and lift curves), and some I hadn’t (ROC curves, expected value in decision analysis; the latter of which I found really interesting in how it can help give a framework to problems). With it being business-oriented, the emphasis throughout is definitely on how the data science process both relates to (the focus is on marketing and churn) and fits into business (for example, if you don’t already have the right data to answer your question, is it worth you investing in doing so?) The authors made a smart choice in starting from absolute basics, avoiding a lot of complexity in the algorithms and sticking to giving as simplified a view as possible of things while still retaining the core ideas. The coverage was enough that someone knowing nothing about data science could come away thinking of how they could start to consider applying it. Even the sections marked as more complicated mathematically were usually a fairly straightforward read: you shouldn’t struggle to get by even with very little maths training. I read it sequentially, but — final few chapters aside which pull together some of the ideas discussed — they’re largely self-contained enough that it’s easy to dive into selected parts. A slight criticism that could have made the book feel more rounded for the non-developer or business reader was to show how data is actually processed. This was left a little abstract in the book. Results were shown, the process of how the algorithms give rise to the results was discussed, but what the user would do to actually get those results was skipped over. (They do include some interesting walkthroughs of the process of solving data science problems, just without the gritty details.) I’ve a little experience of using scikit-learn and didn’t find it hard to imagine how I could go about using the ideas that were being discussed. Without that, I might have found it a little more difficult to relate to otherwise. A few mentions of Weka are made throughout the book: with it being open source and GUI-driven, it would have been ideal for a worked example of how to load and process data. However, it’s a minor oversight; there are plenty of online tutorials for the interested and the authors have done a great job in communicating several, potentially intimidating, ideas with clarity. It’s pitched as a “broad, deep, but not-too-technical guide” which is a fair description. I’d definitely recommend it as an introductory book. A big chunk of it is available to preview on Google Books, if you want to check it out before buying. ","date":"2014-01-14","objectID":"/posts/book-review-data-science-for-business/:0:0","tags":["data","review","science","business","book"],"title":"Book review: \"Data Science for Business\"","uri":"/posts/book-review-data-science-for-business/"},{"categories":null,"content":"If you’ve used any recent version of Windows which is running an Aero theme, you’ve probably noticed that it doesn’t feel that responsive by default. For instance, when minimising or maximising a window, you get an animation that someone must have worked really hard on to get look nice. Unfortunately, this adds some (admittedly short) delay in responsiveness. My preference is for a PC to feel as swift as possible to use over fancy graphical decorations. If you go to the Start Menu, right click on Computer and click Properties, you can access the “Advanced system settings” which lets you access the Aero theme settings to disable these features. However, on changing these settings, you might find that, although they work, they don’t actually stick. When you next log on, the settings have been restored to the defaults. It may be an issue with running accounts as standard users, rather than administrators. To ensure the settings are kept, access this settings panel instead by typing systempropertiesperformance into the Start Menu search. The same panel appears, but the settings will actually be retained when you next log on. ","date":"2014-01-12","objectID":"/posts/making-aero-theme-settings-stick-in/:0:0","tags":["theme","Aero","Windows","lag","delay"],"title":"Making Aero theme settings stick in Windows (resetting on log off)","uri":"/posts/making-aero-theme-settings-stick-in/"},{"categories":null,"content":"I’d migrated a huge collection of files from a backup of a dusty old XP install to a shiny new Windows 7 install. I’d copied everything across to Win7 and then was deleting any unneeded files there. In a couple of cases, once I’d tidied directories out, I found that I couldn’t delete them from within Windows Explorer. Well, I could delete them momentarily before they instantly reappeared. It wasn’t quite the same problem as the one described here — I didn’t receive any error message on deletion for one thing — but the solution was the same: open a command prompt and use the rmdir command to prevent the pesky thing returning again once and for all. ","date":"2014-01-11","objectID":"/posts/how-to-delete-directories-that-respawn/:0:0","tags":["annoyance","respawn","folder","Windows"],"title":"How to delete directories that respawn in Windows 7","uri":"/posts/how-to-delete-directories-that-respawn/"},{"categories":null,"content":"So, it turns out that I was wrong about the motherboard being the cause of my recent Windows hibernate issues. The failure to resume on hibernate still occurred after that, although perhaps more rarely. There’s always the possibility that there’s some strange problem common to the chipset (both boards are based on Intel’s H87 chipset). However, since I’ve not seen anyone other reporting of this, I’m going to assume it’s another piece of hardware to blame. I spent several hours running memtest86+ on the RAM and it didn’t show up any problems. I also couldn’t find any reports of the graphics card (Radeon 7870) or the Catalyst drivers causing hibernate issues, so my suspicion has moved to the solid state drive. (Apart from the CPU itself and the DVD drive, there’s nothing else in the PC!) Because of this, I needed to return the SSD and this, of course, meant that I had to ensure that any data on it was wiped completely. I’d previously written about using DBAN to wipe drives. What I learned this week is that using the ATA Secure Erase function is the best way to go. This is particularly the case for SSDs due to the way in which they write data. How to use hdparm to do this is well explained here: an easy way is to access hdparm is via a Linux boot CD. The important things to remember are to make sure that you’ve selected the correct drive (the safest way is to only have the drive connected which you want to be wiped) and that you might have to “unfreeze” your drive by power cycling it to allow you to execute the hdparm commands successfully. For the SSD I was using, the secure erase took about 45 seconds. If you worry about whether the manufacturer has implemented this function correctly, you might also want to write over the drive with random data twice using shred or dd. ","date":"2014-01-04","objectID":"/posts/securely-erasing-ssd-drives/:0:0","tags":["hdparm","ATA","Secure","erase"],"title":"Securely erasing SSD drives","uri":"/posts/securely-erasing-ssd-drives/"},{"categories":null,"content":"Delving into the inside of PCs isn’t that new to me, I’ve swapped in and out pretty much every part at some point individually. But, building a PC entirely from scratch was new to me and I’d decided to do this for a Christmas gift since I couldn’t find any off-the-shelf PCs for the recipient that had the specification I was looking for. It’s rewarding when you throw a lot of individual pieces into a box, assemble them and find that it actually works when you power it up. Disappointingly, this victory was shortlived. One of the issues with buying lots of separate components is that you have to cross your fingers to some extent and hope that there aren’t any issues with any individual one (bad drivers, firmware issues) and that they all work well together. In my case, I found two cases of odd behaviour. First, case fans would properly start almost every time the machine was powered on, but wouldn’t always. When this happened, I could see the rear fan trying to twitch, but not doing anything useful. Forcing the fans to full speed in the BIOS and restarting made the fans operational again, and you could then reset them back to normal speed with them still running. Not great though. Second, I was getting strange, and random, failures to restore the system from hibernate under Windows, often with the message “Your system’s firmware did not preserve the system memory map across the hibernate transition”. Sometimes it would work, others not. It might have been linked with entering the BIOS settings first before booting, but I’m not entirely sure of that. Anyway, my hunch was that both of these issues seemed to be some problem with the motherboard1, I tried a BIOS update provided by technical support, it didn’t improve things. I’d wasted that much time on it, I just gave up and just ordered another board instead, and (fingers crossed) everything seems OK at the minute. ","date":"2013-12-31","objectID":"/posts/things-ive-learned-from-building-and/:0:0","tags":["Bitlocker","build","PC","learning","heatsink","motherboard","ASUS","assembly"],"title":"Things I've learned from building (and rebuilding) a PC.","uri":"/posts/things-ive-learned-from-building-and/"},{"categories":null,"content":"So, some things I learned: 1. Unless you constantly keep track of what’s going on in the world of PC components, the choice of PC components is a little overwhelming. For me, getting several use out of a PC isn’t unreasonable with some minor upgrades. My desktop is over six years old and I’m not that interested in keeping up with what’s going on if my current setup works well enough…2 For me, it kept feeling like I was always on the verge of making a poor purchasing choice. There are plenty of reviews online, but it takes time to delve into them. On the other hand, PCPartPicker didn’t exist the last time I’d bought lots of PC components and I found that useful for price comparisons. 2. When I first applied thermal compound to the heatsink, I used this two parallel lines method as I was using a heatpipe direct touch (HDT) cooler. After removing the heatsink from the CPU when rebuilding, I found that it had worked well, giving a good uniform coating across the CPU. 3. Small 70% isopropyl alcohol medical wipes do a great job of removing thermal compound: the alcohol evaporates really quickly and the wipes didn’t leave fibres everywhere. You’ll probably need a couple of wipes for your CPU and another couple for your heatsink; they’re sold in huge packs very cheaply (I paid a couple of pounds for a hundred). 4. I didn’t understand why there were so many complaints about Intel’s push pin cooler design until I had to install it the second time. You put the pins of the cooler through the motherboard then click them in place to open out the pins and lock them there. Sounds easy, until you find that all but one pin are through the board while one side of the final pin has bent completely out of shape. (Specifically, the force I was putting on the pin had pushed one of the two bits of pointed white plastic at the bottom of the pin to a near ninety degree angle against the motherboard. It was still fixable though.) 5. Motherboard and case manufacturers don’t write the greatest documentation. There were some steps missing entirely from the case instructions, e.g. how to pop out a front panel cover of the case to make room for a DVD drive which was less than obvious. I was lucky that I found a couple of YouTube videos where people were showing how they built their PCs using that case step-by-step. ASUS decided to helpfully show a nice large pinout of the front panel to motherboard wiring in the manual (e.g. HDD LED, power switch) so that you didn’t have to squint at the tiny print directly on the motherboard. Unfortunately, the manual didn’t bother to label the positive and negative sides of the LEDs meaning I still had to stare at the motherboard to figure this out… 6. Windows activation seems to tolerate motherboard changes when the boards have identical chipsets. It needed reactivating once I swapped the board, but this was just another online activation. 7. (Unsurprisingly,) formatting a Bitlocker encrypted drive removes the encryption. I’m not sure why I thought it might be otherwise, though Windows doesn’t bother to warn you about this. My use case was that I’d already encrypted an external backup drive for use with this PC. However, as I’d carried out a fresh install after the rebuild, I wanted to wipe the useless existing backups. (It was just the old Windows install there, no user files). The quickest way is to delete everything is to format. However, if it’s taken a long time to encrypt the drive, it’s probably better to just delete the existing files on the drive instead. (Windows 8 gives you the option of encrypting used space only which would have made this is a bit less painful. Fortunately, I’d tested on a small USB drive first to discover this rather than the 2 TB backup drive which had taken several hours to encrypt.) For reference, it was a Gigabyte H87-HD3. I couldn’t find any reports of the same problems, so the hibernate problem at least could have been some rogue driver or problem of a combination of hardware. I still ","date":"2013-12-31","objectID":"/posts/things-ive-learned-from-building-and/:1:0","tags":["Bitlocker","build","PC","learning","heatsink","motherboard","ASUS","assembly"],"title":"Things I've learned from building (and rebuilding) a PC.","uri":"/posts/things-ive-learned-from-building-and/"},{"categories":null,"content":"!!! article-edit \"\" Things have changed since this post. It now seems that Ubuntu (at least on 16.04) will stick old kernels for autoremote: you can just `sudo apt autoremove` to ditch them. You can see how apt's behaviour is configured in `/etc/kernel/postinst.d/apt-auto-removal`. Did a little quick maintenance to my Ubuntu install1 today. First, I’d noticed that the boot partition was starting to get a little full, meaning that cleaning out old kernel images was overdue. If it gets completely full, you won’t be able to upgrade to newer kernel versions. This is a fairly simple way from the command line; I’ll summarise it here. Check which kernel is currently in use: uname -r Do not remove the kernel image that uname returns! List the other kernels that are installed: dpkg --list | grep linux-image You’ll see a list that includes the kernel image in use, along with others that aren’t. These are safe to remove. To remove a kernel image, not the one listed from the uname command: sudo apt-get purge linux-image-x.x.x.x-generic I’ve seen it recommended that you keep one or two older images installed in case you need to revert to an older version for any reason, which seems sensible. You can also remove the corresponding kernel headers too: sudo apt-get purge linux-headers-x.x.x.x-generic Second, I’d read a couple of days ago that Ubuntu 14.04 LTS is going to get TRIM support. If you use a solid state drive, TRIM2 is a useful feature that stops the performance of your drive degrading over time. Great, but hang on, 12.04 LTS never did? Hmm, better fix that. If you’re using an unencrypted file system, then running fstrim on the relevant partitions manually or setting up a cron job to do so isn’t too difficult. If you’re using a dm-crypt encrypted file system as I am, it’s not quite as straightforward. It’s still only a two minute job, but it’s not quite as obvious what you need to do to get this working. There’s a nice guide here that explains each step; you enable TRIM for both dm-crypt itself and the Logical Volume Manager, and then create a cron job that TRIMs the drive weekly. Before choosing to do this, note that enabling TRIM with dm-crypt can be a security risk. There are potential issues in terms of leaking file system information, which may be a concern if you have hidden volumes. Incidentally, why have I not got some nice Ubuntu logo here to make this post look nicer? Incidents like this aren’t exactly encouraging, even if later admission of mistakes are made… ↩︎ Reading “TRIM” so many times just meant I had to listen to this while writing. ↩︎ ","date":"2013-12-24","objectID":"/posts/some-spring-winter-ubuntu-cleaning/:0:0","tags":["clean","free","LTS","TRIM","space","12.04","kernel","encryption","dm-crypt","Ubuntu","boot"],"title":"Some \u003cs\u003espring\u003c/s\u003e winter Ubuntu cleaning","uri":"/posts/some-spring-winter-ubuntu-cleaning/"},{"categories":null,"content":"It does seem ridiculous that Windows XP has been around for quite so long. Of course, you should be thinking about moving away from Windows XP as soon as possible since the extended support period is ending in April 2014, but there are still a huge number of XP machines around. (I’m really keen to migrate the remaining machines in my care sometime this month.) In the meantime, there’s a problem with Windows Update on XP machines which means that just checking for updates takes an incredibly long time to run (if it actually completes, I’ve never waited that long to see…) and causes svchost.exe to peg the CPU core it’s running on at 100% usage. This is actually reminiscent of a problem from a few years back, although I’m not particularly nostalgic about it. As a result, it’s a little more difficult to find relevant search results for, unless you restrict your searches to within the past month or year. From the sounds of it, there’s something rather horrible going on behind the scenes with how the Windows Update client actually determines which patches are required. Microsoft have been looking into it, but not actually resolved it yet. In the meantime, a fix is to manually install the latest Cumulative Security Update for whichever version of Internet Explorer is installed. Providing this is installed before the check for updates happens, it removes the need for the checks that are hogging the CPU and everything works well again. Someone’s been helpfully posting links to the latest updates in this forum thread, otherwise you can just visit Microsoft’s TechNet site directly and find the updates in the latest Security Bulletin. This will have to be carried out every month until Microsoft actually deploy a working fix. On the other hand, at most, there will only be another four sets of monthly updates to do this for, since April will be the last Patch Tuesday for XP. Just another reason (as if you needed one) to be upgrading from XP though… ","date":"2013-12-11","objectID":"/posts/windows-update-locking-up-on-xp-when/:0:0","tags":["hog","100%","XP","Windows","freeze","CPU","update","security","fix"],"title":"Windows Update locking up on XP when checking for updates: fixing the svchost.exe CPU usage problem","uri":"/posts/windows-update-locking-up-on-xp-when/"},{"categories":null,"content":"Today I plotted some data which I intend to use in a blog post. The Google Docs output I was working with wasn’t exactly that pleasant to look at, so I figured it was a good excuse to give Bokeh a try. I was installing from source and found that on doing pip install -r requirements.txt that the install of gevent was failing: gevent: gevent/libevent.h:9:19: fatal error: event.h: No such file or directory compilation terminated. error: command 'gcc' failed with exit status 1 Thanks to this post for the fix which was simply to apt-get install libevent-dev before doing the pip install. Not done much with yet apart from installing it and getting the bundled examples to run, but it’s quite nice that you can embed plots directly into HTML. Take a look at the demo here. !!! article-edit \"\" Edit 11/12/2013: I forgot to add that on trying out Bokeh, I couldn’t actually get one of their tutorials to work as it should. Rather than two plots on the same set of axes, I was getting two separate plots on two different sets of axes… As this is something I actually needed for work, I reverted back to matplotlib as a result. So Bokeh looks promising, but might be worth trying again in a few months. ","date":"2013-11-27","objectID":"/posts/installing-bokeh-on-ubuntu-1204-lts/:0:0","tags":["plot","Python","Bokeh"],"title":"Installing Bokeh on Ubuntu 12.04 LTS","uri":"/posts/installing-bokeh-on-ubuntu-1204-lts/"},{"categories":null,"content":"!!! article-edit \"\" This post is now out of date. If you have this problem on a more recent version of Ubuntu, you want my more recent post on this. Yesterday I had an important video call that I needed to carry out under Ubuntu as I need to demo some of my work. About two minutes before it was due to begin, I noticed that my pings to the home network router were ridiculously variable and high: PING 192.168.0.1 (192.168.0.1) 56(84) bytes of data. 64 bytes from 192.168.0.1: icmp_req=1 ttl=64 time=60.3 ms 64 bytes from 192.168.0.1: icmp_req=2 ttl=64 time=82.9 ms 64 bytes from 192.168.0.1: icmp_req=3 ttl=64 time=105 ms Awful. Unfortunately, two minutes before an important call is not the time to be tinkering with network settings, so I left it at the time. Fortunately, it didn’t seem to adversely affect the call. Later, I booted into Windows to find my pings were perfectly normal (1 ms)1. So, what’s the issue under Linux? What it turns out to be is wireless power management. You can easily test this by trying: sudo iwconfig wlan0 power off and remeasuring the pings: 64 bytes from 192.168.0.1: icmp_req=1 ttl=64 time=0.970 ms 64 bytes from 192.168.0.1: icmp_req=2 ttl=64 time=1.04 ms 64 bytes from 192.168.0.1: icmp_req=3 ttl=64 time=0.994 ms Much better! Thanks to this forum thread, I found an easy way to make this fix permanent, you need to do something like: sudo nano /etc/pm/power.d/wireless and add the following lines to the newly created file: #!/bin/sh /sbin/iwconfig wlan0 power off Next, do: sudo chmod +x /etc/pm/power.d/wireless so that this script is executable. Next time you reboot, the fix should still be in place. I have a feeling that I disabled any wireless power management on Windows when I first installed it a few months back; it may be that you’d suffer a similar problem in Windows too otherwise. ↩︎ ","date":"2013-11-20","objectID":"/posts/horrible-wireless-network-pings-in/:0:0","tags":["high","ping","latency","wireless","wifi","Ubuntu"],"title":"Horrible wireless network pings in Ubuntu and how to fix them","uri":"/posts/horrible-wireless-network-pings-in/"},{"categories":null,"content":"…this one is great. Normally I’m just relieved I’ve got something finally working and file the problem under ‘done’. Anyway, I’ve had a Logitech MX Revolution mouse1 for several years and recently spotted that it wasn’t charging. It has a series of battery LEDs on the mouse which should normally flash green when charging. It seems to be a common problem where this just flashes red. The cause appears to be some kind of battery contact issue: just slamming the underside of the mouse down against something seems to be the advice that many users have taken to resolve it. In my case, it took a couple of tries to do this — and I’d recommend doing slamming it against something soft — but it worked. It’s a fantastic mouse though unfortunately no longer manufactured, having been replaced by an inferior model. Why has the second wheel been replaced by buttons? Why do you have to press a button to toggle the scroll wheel from normal ratcheting scrolling to free wheel mode, rather than it elegantly autoswitching when you spin the wheel fast enough? Who knows. ↩︎ ","date":"2013-11-10","objectID":"/posts/its-not-often-that-i-think-of-technical/:0:0","tags":["LED","Revolution","charging","bash","mouse","Logitech","slam","MX","red","fix"],"title":"It's not often that I think of technical fixes as favourites, but…","uri":"/posts/its-not-often-that-i-think-of-technical/"},{"categories":null,"content":"In August this year, my academic postdoc contract ended and I moved on to working at a data science tech startup for a two month internship. In fact, I was offered a short-term contract to extend my stay until late autumn, and I was pleased to have that opportunity. By the time that I actually leave at the end of this month, I’ll have been there for over four months. I figured now would be a good time to reflect a little on what I’ve found, to structure my own thoughts and share them with anyone interested in making a similar career change. It’s been an enjoyable and interesting move. It’s been unusual to spend the entire working day doing things which I’d previously worked on only in my free time. The weeks have gone by really quickly, but it’s been a useful experience: I’ve learnt a lot and, importantly, I’ve more of an idea of what I should continue to learn more about after I leave. So, here are some remarks on my current working environment with some comparisons to my previous academic work. Common to many startups, the company I’m based at is relatively small. That in itself is not that different from my past academic experience1. On the other hand, a huge distinction is that most of the people involved with the company are usually in the same office. Even if not physically present, they are usually on IRC/Hangouts. What this means is that things seem to get resolved and decisions get made far more quickly. This is a real plus for me, since both working in this company and in a tech job are completely new. If there’s something I need help with or if I want to ask about some technical details of the company’s platform, OBit’s likely there’s someone in the room who can give me good advice. It was rarely the case that everyone I’d interact with in university (supervisors, collaborators) would be in direct earshot, and I might only see these people occasionally, maybe once a week or month. If some problem came up, I’d probably end up sending an email and the time to get a reply might be a few minutes, but might take a lot longer. Having near-instant feedback is helpful in keeping you focused on the main problem at hand, rather than getting too distracted by the yak shaving that might be necessary along the way to solving it. Because the company’s small, you get a feel of what’s going on throughout the entire company, from product development to customer demands. It’s also an informal, open environment where discussion doesn’t seem to be stifled and which helps communicate this information. We have daily stand-up meetings. Everyone stands up and, in turn, describes what they achieved in the last working day, what they have planned for the day ahead and mention anything that’s currently hindering them from making progress. This is something I really like. It lets you keep up with what everyone else is working on. It means you have to spend a minute or two deciding what you need to achieve in the day ahead (and sets this as a public commitment to yourself). And, importantly, it allows people find others who may be able to help them. We also hold retrospectives where the previous fortnight’s work is reviewed and we look at what needs to be focused on in the next fortnight. There’s then a vote on which issue particularly needs to be dealt with. Everyone’s opinion — yes, even that of temporary employees and interns — is counted equally here. Working in university, I’d have a vague idea of the goals my colleagues were attempting to head towards, but, in my experience, group meetings and discussions were relatively rare. I would have a rough idea of the goals people were working towards, but not particularly aware of what they were working on day-to-day. Someone might be struggling with some problem or some experiment, but might not be aware that someone else may already have independently resolved that issue. As the company produces a lot of code, another way in which you can see what others are doing is looking at the company’s so","date":"2013-11-03","objectID":"/posts/differences-between-working-in/:0:0","tags":["university","startup","academia","research","tech","job","career"],"title":"Differences between working in university research and a tech startup","uri":"/posts/differences-between-working-in/"},{"categories":null,"content":"As I’d previously mentioned, I’d had issues with a USB 3 drive enclosure when trying to set up a backup drive with Bitlocker on Windows 7. When I replaced the USB 3 enclosure with a slower, USB 2 enclosure, I found exactly the same problem. On trying to enable Bitlocker, I got the message: “A drive attached to the system is not functioning.” Hmm. What I discovered is that if I tried a different Bitlocker partition size, e.g. 307275 MB rather than 307200 MB, it worked! There were no other partitions present on the drive; this NTFS partition was the only one. Furthermore, this “fix” applied to the USB 3 enclosure, so it seems it’s an issue with my drive (Samsung ST1000LM024) or Windows itself. It’s a really peculiar issue: partition sizes of 307200, 307250 to 307255 and 307265 to 307269 MB gave me this error, but a 307270 MB partition would have been fine. Likewise, I got errors at 153600, 460800 and 76800 MB; it certainly doesn’t like multiples of roughly 150 GB… (it’s possible that this is a red herring though — I couldn’t realistically try every possible partition size). I haven’t observed any other issues having setup the drive, but this is unusual nonetheless, and I couldn’t find anyone documenting anything similar. ","date":"2013-10-13","objectID":"/posts/odd-behaviour-of-bitlocker-or-maybe-my/:0:0","tags":["partition","Bitlocker","backup","enclosure","bug","drive"],"title":"Odd behaviour of Bitlocker, or maybe my HDD…","uri":"/posts/odd-behaviour-of-bitlocker-or-maybe-my/"},{"categories":null,"content":"My SSH key was setup for GitHub and working fine, but I wanted to add a separate one for Bitbucket1. It was daunting playing around with this when everything was already working largely as I expected, but it was simple to get this as I wanted. First, backup your existing keys first in case something goes wrong and you accidentally overwrite them. (You’re backing up your data anyway, right?) Create a new key. If you’ve got an id_rsa key already, you can name the new key e.g. ~/.ssh/id_rsa.1 (or 2, 3…). (Initially, I put the new key in a separate Bitbucket directory and I don’t think that the ssh-agent would automatically pick it up there, so I moved it back to the .ssh directory.) Add the following to ~/.ssh/config: Host github.com User git IdentityFile ~/.ssh/id_rsa IdentitiesOnly yes Host bitbucket.org User git IdentityFile ~/.ssh/id_rsa.1 IdentitiesOnly yes IdentitiesOnly ensures that only the specified key is used for that host; if you don’t have this, you may end up failing to authenticate as too many incorrect keys have been passed to the server. If you’re using the key in this session, you might need to ssh-add it. In my case, on Ubuntu 12.04, on subsequent logins, the new key was made available automatically. Unlike GitHub, Bitbucket only matches commits when the commit contains an email address that has been confirmed on Bitbucket, so any commits you make aren’t matched to your user account. You can override this by setting aliases, but this is on a per repository basis and you can’t do this unless you’re an admin of the repository. ↩︎ ","date":"2013-10-08","objectID":"/posts/working-with-multiple-ssh-keys-setting/:0:0","tags":["GitHub","key","SSH","Bitbucket"],"title":"Working with multiple SSH keys: setting up git over SSH with GitHub and Bitbucket","uri":"/posts/working-with-multiple-ssh-keys-setting/"},{"categories":null,"content":"Today I’ve been spending far too long installing matplotlib into a Python virtualenv on Ubuntu 12.04, but I finally have it working as I want now. matplotlib lets you make nice data plots with a few lines of Python, but it has several dependencies and this makes it trickier than some packages to get going. The first obstacle is getting numpy installed. I suspect there were some dependencies I had to install to be able to successfully pip install numpy in my virtualenv. It was only a couple of hours ago, but I have forgotten completely if I had to install anything to get numpy working. Not very helpful, but if you search for any error messages that come up here, you should find the relevant Stack Overflow posts that helped me solve that initial problem. You also need freetype and libpng installed to be able to pip install matplotlib. I had libpng12-0 already installed, but needed libpng12-dev; likewise, I had libfreetype6 installed, but also needed the development package, libfreetype6-dev. That wasn’t too difficult to solve and matplotlib seemed to install fine. As described elsewhere, one problem I had was that matplotlib was generating plots with savefig, but not showing them when I was trying to show() them. I changed the backend to GTK as the solution suggested, but matplotlib then complained that with an ImportError that “Gtk* backend requires pygtk to be installed”. So, how to solve this. First of all, you’ll need to pip uninstall matplotlib in your virtualenv if you’ve installed matplotlib already. You also need to sudo apt-get install python-gtk2-dev. I found this post discussing using PyGTK in a virtualenv which helped fix this. Activate the virtualenv you want to use matplotlib in, then do: ln -sf /usr/lib/python2.7/dist-packages/{glib,gobject,cairo,gtk-2.0,pygtk.py,pygtk.pth} $VIRTUAL_ENV/lib/python2.7/site-packages This creates symbolic links to the required globally installed Python packages and solves the problem of them being inaccessible. You have to do this before you pip install matplotlib. Otherwise it won’t detect at compile time that you have the backend GTK dependencies in the virtualenv. Finally, you need to set the backend to GTKAgg in ~/.config/matplotlib/matplotlibrc (I had to copy matplotlibrc from /lib/python2.7/site-packages/matplotlib/mpl-data in the virtualenv.) If you run this test plot, it should now save it and display it in an interactive window. Update: Going away and coming back to this problem, I found that using the --system-site-packages option when creating a new virtualenv should also allow the GTK backend to work. The method I’ve detailed above has the advantage of copying over the minimum of required packages. ","date":"2013-09-30","objectID":"/posts/installing-matplotlib-in-virtualenv/:0:0","tags":["dependency","virtualenv","install","matplotlib","solution","Python","PyGTK"],"title":"Installing matplotlib in a virtualenv","uri":"/posts/installing-matplotlib-in-virtualenv/"},{"categories":null,"content":"If you use computers even a little, you’ll probably have acquired a few external USB drives. They’re cheap, reasonably quick and small, so it’s unsurprising that they’re so popular. The only issue is that you may well be storing personal data on them, meaning that if you lost them and they’re unencrypted, anyone can read their contents. Compared with the painful time I previously spent encrypting operating system drives, it’s a relief that encrypting non-system drives is far, far simpler. ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:0:0","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Bitlocker If only using Windows, and provided your copy of Windows is bundled with it, you could use Bitlocker. Bitlocker is straightforward to setup with self-explanatory instructions. Right clicking an external drive in Explorer brings up the Bitlocker option (as shown to the right). Next, you then choose a password to unlock the drive, then save or print a recovery key in case you forget your password. All that’s left is to encrypt the drive by finally choosing to Start Encryption. The next time you insert the drive, it will prompt you for the password to unlock it. Enter it and the drive functions just as normal. ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:1:0","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Issues with Bitlocker Note that you’ll need to use Microsoft’s Bitlocker To Go Reader for read-only access to Bitlocker-encrypted drives in older Windows version (Vista and XP). There are also some unofficial projects that support Bitlocker under Linux and OS X. Update: Bought a USB 3 drive so that I could backup faster and was told by Windows that “a drive attached to the system is not functioning” when trying to enable Bitlocker. This is a known issue, but I hadn’t read anything about it until just now. In my case, my drive wouldn’t even show up when plugged in the USB 2 port making my shiny new drive enclosure fairly useless. This is with the latest USB 3.0 eXtensible Host Controller Driver for Intel 7 Series (1.0.9.254, at the time of writing). Update 2: USB 3 works fine, but there’s some weird Bitlocker or drive specific issue behaviour… ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:1:1","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"TrueCrypt !!! article-edit \"\" Edit: see my earlier encryption post for notes on the current state of TrueCrypt; it shouldn’t be used anymore (on Windows, for certain), but one of its successors may be a suitable replacement. The other main option is TrueCrypt which is freely available, and has good cross-platform support, perhaps making it a better choice. Go to Volumes \u003e Create New Volume… and you’ll see this menu: You can optionally use an encrypted drive container, which is a virtual disk in a file, should you want to store unencrypted files on your drive alongside encrypted ones. My preference is just to encrypt the entire thing, so there’s no risk of me inadvertently storing something containing personal information in the unencrypted part. Here, TrueCrypt asks if you want to hide the volume. In my case, a standard volume is sufficient. Click Select Device, choose the drive you want to encrypt and click Next. If you’ve nothing on the drive already, you can format it. Otherwise, you’ll have to encrypt it in place which will take longer. On the next page after this, it allows you to specify the encryption algorithms, I leave these as default. It will then tell you the volume size so that you can double check you’ve specified the correct drive. After that, you enter a password, choose what kind of filesystem (e.g. FAT or NTFS) you want for the drive, move the mouse in the TrueCrypt for a while to generate random data and click Format. To mount the drive, you launch TrueCrypt, choose Select Device, select your TrueCrypt drive, select an empty drive letter and choose Mount. You’ll be prompted for your password, and then providing you enter the correct password, your drive will be mounted and you can access the files on it. When you’ve finished working with the drive, you should select it in TrueCrypt and choose Dismount. You can then safely eject the drive from Windows as you would normally, and then remove it from your PC. ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:2:0","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Issues with TrueCrypt The only requirement is that you need TrueCrypt to be running on a destination PC. This can either be via having it installed to the PC directly, or, for Windows PCs at least, having access to an appropriate portable version of TrueCrypt. The one big tradeoff compared with Bitlocker is that you require administrative privileges even when running the portable version on Windows. If TrueCrypt has been previously installed to a Windows PC by an administrator, then admin privileges are not required to mount drives. (On Linux, even with TrueCrypt installs, it asks for admin privileges since it requires them to mount a drive.) With Bitlocker, non-admins can access drives without any issue (aside from older Windows version needing the reader application ), and this might factor into your choice of encryption software. ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:2:1","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Encrypting backup drives If you’re encrypting everything else, it probably makes sense to considering encrypting your backups too. If your system is already encrypted completely, you could simply clone the entire drive with Clonezilla. I haven’t tested this and I’m not sure if there are issues with restoring the drive, particularly when if a drive’s been encrypted with TrueCrypt, though some users report this is fine. Using the backup tools provided by each individual OS is also an option. ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:3:0","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Windows Windows Backup and Restore needs slightly awkward workarounds to function with TrueCrypt drives; however, Bitlocker encrypted external drives work seamlessly with this feature. The File History feature in Windows 8 should work fine with TrueCrypt drives, on the other hand (it doesn’t use Volume Shadow Copy that TrueCrypt is incompatible with). ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:3:1","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Ubuntu For Ubuntu, I’m using the built-in Backup tool (Déjà Dup) which is based on Duplicity. At its core, this backup tool is very much end-user focused. It’s incredibly simple to set up. Although it could benefit from better scheduling and more options, e.g. exclusion of files versus simply choosing which directories to include, making backups easy for users to do is not a bad thing. Despite this, Déjà Dup has two handy features. First, it supports encryption natively, so you don’t even need to specially format a drive or partition to encrypt the backup. You choose an encryption password and it encrypts the backup. Second, it integrates nicely with Ubuntu. Given a backup, you are able to restore deleted files or previous versions directly from Nautilus. There are other backup alternatives like rsnapshot (though you’d have to handle encryption yourself), but Déjà Dup gave me a fuss-free way to get an encrypted backup running, which I really like. ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:3:2","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Summary If you’re carrying around sensitive or personal data on portable drives, it’s worth taking the time to make sure that the data’s secure in case you lose the drive. It’s simple to setup regardless of whichever OS you’re using. ","date":"2013-09-28","objectID":"/posts/how-to-secure-your-storage-and-backup/:4:0","tags":["Bitlocker","backup","encryption","USB","HDD","external","TrueCrypt","drive"],"title":"How to secure your storage and backup drives on Windows and Linux","uri":"/posts/how-to-secure-your-storage-and-backup/"},{"categories":null,"content":"Had an issue where YouTube’s HTML5 video wasn’t working in Chromium on Ubuntu 12.04 LTS, and didn’t really want to install Flash Player if I could avoid it. (HTML5 in Firefox was working fine.) All I got in Chromium was a black screen with a spinner going endlessly,. In the end, the solution was as simple as missing codecs: sudo apt-get install chromium-codecs-ffmpeg-extra It’ll uninstall the currently installed chromium-codecs package and replace it with this one. You might also have to restart Chromium if it’s running; it’s a couple days back since I did this, so can’t remember. ","date":"2013-09-22","objectID":"/posts/fixing-chromium-on-ubuntu-video/:0:0","tags":["video","YouTube","screen","black","Chromium","Ubuntu"],"title":"Fixing Chromium on Ubuntu video playback issues (no video, black screen)","uri":"/posts/fixing-chromium-on-ubuntu-video/"},{"categories":null,"content":"As I wrote previously, virtualenv in Python simplifies development life and dependency handling by allowing you to effectively use a fresh Python install for each project you work on. In turn, virtualenvwrapper simplifies the use of virtualenvs! (Thanks Morty for pointing it out!) Like my previous post, I’ll keep any exposition to the minimum. ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:0:0","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"How do you install virtualenvwrapper? On my Ubuntu 12.04 LTS system, I installed it via the apt package manager. It was a little bit more of a pain since it sticks it in a non-standard location:1 not the expected /usr/local/bin/virtualenvwrapper.sh but the unexpected /etc/bash_completion_d/virtualenvwrapper. Once installed, add the following two lines to your shell startup file e.g. .bashrc source /etc/bash_completion_d/virtualenvwrapper export WORKON_HOME=~/path/to/your/virtualenvs (Replace /etc/bash_completion_d/virtualenvwrapper in the source line with /usr/local/bin/virtualenvwrapper.sh depending on where the virtualenvwrapper script is actually located.) Save your newly modified .bashrc and re-source it by entering in your bash terminal: source ~/.bashrc virtualenvwrapper is now ready to go! ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:1:0","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"What does virtualenvwrapper offer? After using it briefly, the stand out features are: List your available virtualenvs (provided they are located in $WORKON_HOME): workon Quick switch to a virtualenv: workon virtualenvname Another really cool thing with virtualenvwrapper is that it allows you to specify preactivate and postactivate scripts. This is really useful. One handy shortcut is to make workon not only switch to a particular virtualenv, but to also change directory to whereever the virtualenv is. This makes a lot of sense: it’s probably more often than not that you’d want to switch to the virtualenv without working on that project. To set this up: do cd $VIRTUALENVWRAPPER_HOOK_DIR You’ll find the postactivate file in there. Add the following two lines to it: proj_name=$(echo $VIRTUAL_ENV|awk -F'/' '{print $NF}') cd ~/project_dir_name/$proj_name (taken from http://pynash.org/2013/02/18/quick-hit-virtualenvwrapper-auto-dir.html) Replace ~/project_dir_name with the path to your Python virtualenvs. Even in my case, where I only have a few projects right now, this shortcut saves you a lot of typing cd and activate commands. ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:2:0","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Make a new virtualenv in $WORKON_HOME (this also activates it): mkvirtualenv newvirtualenvname ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:2:1","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Delete a virtualenv: rmvirtualenv virtualenvname ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:2:2","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Other interesting virtualenv management features I’ve not used yet: ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:3:0","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Show installed packages in currently active virtualenv: lssitepackages ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:3:1","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Wipe third party packages from the current virtualenv: wipeenv ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:3:2","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Add directories to the PYTHONPATH of the current virtualenv: add2virtualenv directory1 directory2 ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:3:3","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Toggle the current virtualenv’s access to your globally installed Python packages: toggleglobalsitepackages ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:3:4","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"Run a command in all virtualenvs in WORKON_HOME: allvirtualenv command There are further features detailed in the documentation, but these tricks alone make taking the time to install it worthwhile. I also ran into a strange issue whereby in this confusion I removed virtualenvwrapper, reinstalled it, but the virtualenvwrapper file in bash_completion_d didn’t reappear. I had to apt-get purge virtualenvwrapper and then sudo apt-get install virtualenvwrapper to fix this. (Also note that if you have pip installed, you can do sudo pip install virtualenvwrapper, which presumably installs it to the standard location and should circumvent this problem entirely.) ↩︎ ","date":"2013-09-19","objectID":"/posts/explaining-python-virtualenvwrapper-in/:3:5","tags":["virtualenv","development","Python","Ubuntu"],"title":"Explaining Python virtualenvwrapper (in a couple of minutes)","uri":"/posts/explaining-python-virtualenvwrapper-in/"},{"categories":null,"content":"The beginner in the title of this post may or may not refer to you. It certainly describes me. Since I was about to setup a new PC, I decided it was about time to think a little more seriously about how to secure my data. If someone got hold of my PC or external drives, without encryption, it’s trivial for them to read anything from them. Data encryption that is difficult to break if currently implemented and used with strong passphrases is pretty much cost free these days, both financially and in terms of the hit on computer performance (the latter particularly so if you have a CPU that supports the AES instruction set. As Wikipedia points out, despite your CPU being one that supports this feature, you might find that the manufacturer has neglected to enable it; it’s worth checking for BIOS updates that deal with this.) Getting an encrypted PC running with just one operating system is relatively simple to setup. Complicating matters is the fact that I wanted to dual boot Windows with a Linux distribution — in my case, I’d chosen Ubuntu — and encrypt both. In the process, I ended up undergoing a ridiculous number of installs over several days to actually get this working as I wanted. If you’re struggling to do the same, I hope that some of the pointers here might save you some time rather than having to learn the hard way. There are a few subtle caveats documented here that I hadn’t seen collated together elsewhere. Also, many blog posts and articles I’d read were horribly out of date or didn’t have sufficient information for me to follow them. (A note to anyone who kindly shares guides: step-by-step guides with screen grabs are invaluable when a reader is way out of their depth.) Here’s details of the three approaches I tested. ","date":"2013-09-17","objectID":"/posts/a-beginners-guide-to-os-encryption-dual/:0:0","tags":["disk","Bitlocker","Linux","Windows","encryption","TrueCrypt","drive","Ubuntu"],"title":"A beginner's guide to OS encryption: dual booting and encrypting Windows and Ubuntu","uri":"/posts/a-beginners-guide-to-os-encryption-dual/"},{"categories":null,"content":"Approach 1: Encrypt Windows 8 Pro with Bitlocker; encrypt Ubuntu with LUKS I followed this guide with some success in that I was able to encrypt both systems. Instead of the Ubuntu minimal CD in those instructions, I used Ubuntu 12.04 LTS alternate CD; using the long term support version isn’t a bad idea if you’re not bothered about the latest shiny things. Why the alternate CD? What Ubuntu’s standard installer (as of 13.04) seems to offer is either the installation of Ubuntu unencrypted on a PC side-by-side with an existing Windows install. Alternatively, you can wipe the Windows install and install Ubuntu encrypted, but you can’t encrypt Ubuntu when Windows is already installed. However, I found Windows 8 was fairly broken, even ignoring the clunkiness of the user interface. All kinds of Volume Shadow Copy service errors were occurring. This meant that I couldn’t backup anything with the built-in Windows tools and the System Restore feature presumably wouldn’t work either. Even when Windows wasn’t Bitlocker-encrypted, I was still getting these errors. I couldn’t find much support for the problems I found, so perhaps it was some dual booting issue; if you have any ideas, please email or let me know via Twitter. ","date":"2013-09-17","objectID":"/posts/a-beginners-guide-to-os-encryption-dual/:1:0","tags":["disk","Bitlocker","Linux","Windows","encryption","TrueCrypt","drive","Ubuntu"],"title":"A beginner's guide to OS encryption: dual booting and encrypting Windows and Ubuntu","uri":"/posts/a-beginners-guide-to-os-encryption-dual/"},{"categories":null,"content":"Approach 2: Encrypt Windows 7 Enterprise/Ultimate with Bitlocker; encrypt Ubuntu with LUKS The first issue here is that the only versions of Windows 7 that support Bitlocker are Enterprise and Ultimate. If you have Windows 7 Pro, you’re out of luck. You may also find issues on trying to boot the Windows 7 installer on a PC with UEFI, in which case you may have to drop back to legacy mode to work round this. Here, I never even got to install Ubuntu and stopped at attempting to encrypt Windows for one big reason. Unlike Windows 8, if your PC — like mine — doesn’t have a Trusted Platform Module, the only way to unlock the Bitlocker drive on boot is to use a USB key. Without a TPM, you cannot use a password for Bitlocker in Windows 7. This isn’t ideal. If you forget the key, you’re locked out of your PC. It’s also something else to have to carry around (though I suppose it might be possible to use a phone as USB storage…). If I was just lazy and just left it plugged in the whole time, it could easily be stolen with the PC. So, it was best that I didn’t spend any more time considering this option. ","date":"2013-09-17","objectID":"/posts/a-beginners-guide-to-os-encryption-dual/:2:0","tags":["disk","Bitlocker","Linux","Windows","encryption","TrueCrypt","drive","Ubuntu"],"title":"A beginner's guide to OS encryption: dual booting and encrypting Windows and Ubuntu","uri":"/posts/a-beginners-guide-to-os-encryption-dual/"},{"categories":null,"content":"Approach 3: Encrypt Windows 7 (any version) with TrueCrypt; encrypt Ubuntu with LUKS !!! article-edit \"\" Edit October 2015: There are vulnerabilities in the Windows TrueCrypt driver (here and here), so it definitely can’t be recommended now. There are newer projects based on TrueCrypt which may be suitable replacements; I haven’t looked into them. The fact that TrueCrypt has its source available is a plus. I’m nowhere near smart enough to start understanding the internals of cryptographic software, but there are people out there who are. And I would certainly hope that they would raise the alarm if they spot anything suspicious.1 Unlike Bitlocker on Windows 7, TrueCrypt allows you to use a user-entered password to unlock the PC. However, at the moment, it doesn’t work with a PC booting in UEFI mode. Rather than duplicate information already available, I’ll just point you to Maarten Mastbroek’s excellent guide. My only comments on that guide are that you don’t need to worry too much about the 100 MB partition that Windows installs, you can simply leave it rather than use it as a /boot partition. Also, when installing TrueCrypt, you can tell it you have a single boot machine. The only other issue is that you should ensure that both installers are run in legacy (non-UEFI mode). If your PC supports UEFI, there should be BIOS options, or options that appear when you booting installation media, to select this. During my install, the Ubuntu installer never asked me where to install the GRUB bootloader. I suspect that having put Ubuntu onto a USB drive, it was booting straight into UEFI mode without asking me and this was fouling up the GRUB install. My workround was to burn a DVD copy of the Ubuntu installer and this gave me the option to select UEFI or legacy mode at boot. Alternatively, and maybe more easily, you can just delete the efi folder from the Ubuntu USB stick you prepare. ","date":"2013-09-17","objectID":"/posts/a-beginners-guide-to-os-encryption-dual/:3:0","tags":["disk","Bitlocker","Linux","Windows","encryption","TrueCrypt","drive","Ubuntu"],"title":"A beginner's guide to OS encryption: dual booting and encrypting Windows and Ubuntu","uri":"/posts/a-beginners-guide-to-os-encryption-dual/"},{"categories":null,"content":"Issues In terms of usability, apart from having to enter another password before I boot either OS, everything seems pretty much as if the drives weren’t encrypted. Otherwise, the only issues I have with this setup are: The only feature I haven’t got working yet is hibernate on Ubuntu with drive encryption (it works on Windows), though there’s a guide on how to do this which I may try when I have time to investigate it further. My setup doesn’t use UEFI. This isn’t a big deal for me, though. The Windows boot/system reserved and the Linux boot partitions remain unencrypted and this is a potential vulnerability. However, it should be sufficient to ensure my data is kept secure in the event of loss or theft. All in all, not too bad. ","date":"2013-09-17","objectID":"/posts/a-beginners-guide-to-os-encryption-dual/:4:0","tags":["disk","Bitlocker","Linux","Windows","encryption","TrueCrypt","drive","Ubuntu"],"title":"A beginner's guide to OS encryption: dual booting and encrypting Windows and Ubuntu","uri":"/posts/a-beginners-guide-to-os-encryption-dual/"},{"categories":null,"content":"SSD encryption? After all that effort, software encryption might not even be the best way to go. Instead, as SSDs have become more affordable, using hardware-based SSD encryption might be simpler. With the drive handling encryption, it should be completely transparent to the operating system, meaning no hit on performance (this might be an issue if you want to use encryption on an older PC), with the bonus of eradicating the possibility of any install pain. Sounds great, doesn’t it? Unfortunately, it’s no panacea. The implementation is not always clear. You are reliant on being able to use a strong ATA drive locking password, rather than the eight character one that some BIOSes only support. (Incidentally, this isn’t information that’s necessarily easy to discover prior to buying new hardware). Additionally, note that some manufacturers might have vendor specific workarounds, for example, having a standard master password that should be changed. This is all quite messy, but perhaps this will become more transparent from an end user perspective in future. Next time that I discuss encryption, I’ll be posting about external storage and backup encryption. Thankfully, this is a far, far easier problem to solve than OS encryption. If you’ve got any other approaches or tips for encrypting dual boot systems, I’d be interested to hear them. At the time of writing, this is a particularly relevant concern. ↩︎ ","date":"2013-09-17","objectID":"/posts/a-beginners-guide-to-os-encryption-dual/:5:0","tags":["disk","Bitlocker","Linux","Windows","encryption","TrueCrypt","drive","Ubuntu"],"title":"A beginner's guide to OS encryption: dual booting and encrypting Windows and Ubuntu","uri":"/posts/a-beginners-guide-to-os-encryption-dual/"},{"categories":null,"content":"Since starting working at ScraperWiki, I’ve been using Git version control and GitHub a lot to collaborate on projects I’m working on. Out of the box, connecting to GitHub via HTTPS is simple to get going and works perfectly fine. One problem is that you have to enter a username and password every time you do anything with the remote repository, such as pushing code to it. For me, this gets in the way of workflow. It’s a small, but distracting, obstacle that snaps me out of whatever thought process I was in. Over a full day’s work, this can happen several times. Instead, I prefer to connect to GitHub using an SSH connection. All I do now is enter a passphrase once per login session and then forget about it1. Another advantage is that you can forward access to your local SSH credentials even when developing in remote shell accounts. GitHub’s help section has several well written pages that detail clearly how to get this setup running. The only issue with those guides is that the individual steps are actually split over a few individual help pages. Here’s a combined summary of what you need to do. Generate an SSH key locally (not on a remote shell account), with a passphrase, add it to your GitHub account and test it. You should now change the clone URLs GitHub displays to SSH (see the image; where it says clone with HTTPS, SSH…, click on SSH). Now, the first time you try to perform some action on GitHub via ssh (e.g. git clone), you should only get asked for the passphrase once and subsequent connections will proceed without prompting. This worked out of the box for me on Ubuntu 12.04 LTS. If you have issues, you’ll need to look into how to configure ssh-agent on your system. Bear in mind that if you already have repositories that you have cloned via HTTPS using GitHub, you’ll likely want to update the git remotes to use the appropriate SSH URLs instead. These won’t be automatically changed. Otherwise you’ll still get asked for your username and password credentials: these local repositories will still be connecting via HTTPS URLs! (Optional; handy if you’re developing or git cloning on remote boxes) With everything now working locally, you need to setup ssh-agent so that your remote account can access your SSH key. When you now carry out an operation with GitHub on the remote machine you connected to via SSH, you shouldn’t get asked for your password on that machine either2. I am aware that you do have the option of using credential helper to cache your HTTPS password, but you would need to do this both locally and remotely if you are working on different machines. Also, the default setting, though it can be changed, is to cache the password for just 15 minutes. ↩︎ Again, the caveat regarding changing repository remote settings from HTTPS to SSH applies on remote machines too. ↩︎ ","date":"2013-09-07","objectID":"/posts/how-to-access-github-over-ssh-on-ubuntu/:0:0","tags":["GitHub","SSH","remote","shell","Ubuntu"],"title":"How to access GitHub over SSH on Ubuntu","uri":"/posts/how-to-access-github-over-ssh-on-ubuntu/"},{"categories":null,"content":"!!! article-edit \"\" Edit 2014/01/04: I’ve written a more recent post which describes how to use hdparm and ATA Secure Erase to wipe drives, which might be of interest if you’re thinking of using DBAN, and particularly if you’re erasing an SSD. Over last Bank Holiday weekend, I’d been trying to install a dual boot system and having lots of fun. Actually, for “lots of”, read none, though at least it works how I want it to now; maybe a subject for a later post. One issue I thought I might be having was remnants of partition tables still sticking on the drive, despite me doing a fresh install. I decided it would be sensible to obliterate everything and start from a completely empty drive as installing two operating systems can be a fairly lengthy process (and a tedious one, if you’ve done it several times as I had done). Darik’s Boot and Nuke (DBAN) is a free, Linux-based boot disk that lets you do just that. It needs to be used with great care since it can wipe data from drives irrecoverably. In my case, this wasn’t an issue as, if anything, I wanted to start from scratch. Unfortunately, on booting DBAN, I was presented with an “unrecognized device” showing up along with my hard disk. I was able to select my hard disk to erase it, but then the erase immediately failed. As this support thread describes, this can be caused by internal media (e.g. SD) card readers. So, anyway that you can deactivate the card reader might resolve this problem. Disconnecting the reader from the PC (not necessarily so easy if you’re working with a laptop) or temporarily disabling the reader in the BIOS are two options. In my case, the BIOS didn’t let me do this and I didn’t really want to bother getting out screwdrivers unless I absolutely had to. For me, the solution that worked was also pointed out in the same thread, entering the command dban nousb at the DBAN boot: prompt. Then, only my hard disk showed up and the process went smoothly. Using nousb means that you are unable to erase any USB devices with DBAN but, if you’re only interested in wiping the HDD, this isn’t an issue. ","date":"2013-08-29","objectID":"/posts/dariks-boot-and-nuke-unrecognized/:0:0","tags":["device","unrecognized","error","HDD","erase","DBAN"],"title":"Darik's Boot and Nuke: fixing an \"unrecognized device\" error","uri":"/posts/dariks-boot-and-nuke-unrecognized/"},{"categories":null,"content":"I’ve been busy the last couple of weeks. Recently, I started a late summer/early autumn internship at ScraperWiki, who are building a data science platform. I should have a blog post (it’s drafted!) describing how I’ve found working there so far posted soon on their blog. Anyway, I’ve been pairing with the very helpful and patient David Jones who has been helping me out hugely by sharing some of his knowledge with me. For the last two days, I’ve been learning a lot, and somehow forgetting even more, about vim. However, David also showed me how to use virtualenv today. This was far easier for me to get my head round, so I’d figured I’d write up a quick start guide while it was fresh in mind. ","date":"2013-08-22","objectID":"/posts/explaining-python-virtualenv-in-under/:0:0","tags":["virtualenv","development","Linux","programming","Python","Ubuntu","coding"],"title":"Explaining Python virtualenv (in under two minutes)","uri":"/posts/explaining-python-virtualenv-in-under/"},{"categories":null,"content":"What is virtualenv? As the homepage describes it, “virtualenv is a tool to create isolated Python environments”. These environments are also referred to as virtualenvs. ","date":"2013-08-22","objectID":"/posts/explaining-python-virtualenv-in-under/:1:0","tags":["virtualenv","development","Linux","programming","Python","Ubuntu","coding"],"title":"Explaining Python virtualenv (in under two minutes)","uri":"/posts/explaining-python-virtualenv-in-under/"},{"categories":null,"content":"Why is this useful for software development? It greatly simplifies dependencies on other modules for different software projects you are working on. Each virtualenv can have its own installed modules. If projects require different versions of the same module, it would be a real pain to manage this at a system level. You might need, say, two different versions of the same module for two different projects. Using virtualenv, it’s simple for every project you work on to have its own set of modules independently of any others. You can also use this to safely test new versions of modules with your software: make a new virtualenv and install the newer version of the module there. Furthermore, you can also setup virtualenvs that use different Python versions, particularly handy considering that the change from Python 2 to Python 3 is a substantial one. Another plus is that using virtualenvs stops you polluting your system’s actual Python installation with lots of packages that you’re using only to develop certain software. ","date":"2013-08-22","objectID":"/posts/explaining-python-virtualenv-in-under/:2:0","tags":["virtualenv","development","Linux","programming","Python","Ubuntu","coding"],"title":"Explaining Python virtualenv (in under two minutes)","uri":"/posts/explaining-python-virtualenv-in-under/"},{"categories":null,"content":"How do you install it? On Debian-based distributions, e.g. Ubuntu, you can use apt-get: sudo apt-get install python-virtualenv ","date":"2013-08-22","objectID":"/posts/explaining-python-virtualenv-in-under/:3:0","tags":["virtualenv","development","Linux","programming","Python","Ubuntu","coding"],"title":"Explaining Python virtualenv (in under two minutes)","uri":"/posts/explaining-python-virtualenv-in-under/"},{"categories":null,"content":"How do you use it? In a terminal, make a directory for the project you’re working on (or enter an existing directory), e.g. newproject. Creating a new virtualenv is as easy as entering virtualenv newproject When this command is entered, a Python distribution will be setup within the specified directory, in this case newproject. The current default behaviour of virtualenv is not to copy any system installed packages here, so it’s like a fresh install. To activate a virtualenv, you then change to your project directory (e.g. cd newproject), then enter the command: . bin/activate (notice the space between the dot and bin/activate). It’s easy to tell you’re running a virtualenv: the terminal will show (newproject) before the command prompt. While the virtualenv is active, any pip install packagename command will install a Python package into that particular virtualenv. This is another nice benefit of virtualenv: you don’t need to run *sudo* pip install packagename as you will already be working in a user-writeable directory. To deactivate a virtualenv, simply enter deactivate at the terminal. To use in future, just run the activate command again in the project directory: . bin/activate Simple, but incredibly useful. (Bonus note: Since creating a virtualenv creates directories such as /bin, /include, /lib and /local, you may wish to add these to a .gitignore file if you’re using git so that you don’t accidentally commit these files to a repository.) (Bonus note 2: After initially posting this, it was later pointed out to me that virtualenvwrapper is a handy way to manage virtualenvs.) ","date":"2013-08-22","objectID":"/posts/explaining-python-virtualenv-in-under/:4:0","tags":["virtualenv","development","Linux","programming","Python","Ubuntu","coding"],"title":"Explaining Python virtualenv (in under two minutes)","uri":"/posts/explaining-python-virtualenv-in-under/"},{"categories":null,"content":" You do three years, then a couple more, and then – my God, what next? A master's, a PhD … and never a job at the end of it. Sylvia Melchiorre In embarking on research work at university, there’s always a risk that you can get dragged along uncontrollably with the tide. When finishing up a three or four year PhD, it can be stressful enough without even considering the impending unemployment that will imminently follow. So, if you’re in that situation and your PhD supervisor suggests that you apply for a forthcoming postdoc contract, you may well, just as I did, do that since it’s an easy solution to this predicament. It’s a comfortable option, since it postpones any serious (and tricky) career decisions for a couple more years. In my case, all was well until this problem reared its head again as I reached the final six months of my postdoc contract. I had questioned what was next, but didn’t really know how to go about answering that question. I’d been in university so long that I had no idea of what my options were: what else, other than research, can you do with the skills you’ve acquired as a researcher?1 Despite that predicament, when I received an email advertising a careers programme for postdocs working in medical engineering, I initially passed it up as being something not very useful for me. My (admittedly misguided) impression was that it would be all time-wasting whiteboard exercises. Thankfully in the end, I did apply, but only after having my arm twisted by a friendwho went to a meeting outlining the programme and was greatly enthusiastic about it. In retrospect, I feel pretty stupid that I could have easily missed out, since it’s been greatly helpful in pointing out alternative careers and taught me a lot in how I should be presenting myself to prospective employers. In fact, I’ve been incredibly fortunate. My feeling is that this kind of intensive programme is rarely offered to PhD students or postdocs. What the programme involved was several small group meetings over a few months with advice and support on offer from the course organisers, and each other. Each week discussed a different theme and we covered just about every aspect of career management, from identifying possible careers, to writing CVs and cover letters, to networking and other job search approaches, to interviews. Given that comprehensive coverage of job searching, the title the course had (“Career Architect”) certainly wasn’t unreasonable. ","date":"2013-08-18","objectID":"/posts/how-universities-can-help-develop/:0:0","tags":["researcher","university","postdoc","PhD","job","career"],"title":"How universities can help develop researchers' post-university careers","uri":"/posts/how-universities-can-help-develop/"},{"categories":null,"content":"Why did this approach work so well? ","date":"2013-08-18","objectID":"/posts/how-universities-can-help-develop/:1:0","tags":["researcher","university","postdoc","PhD","job","career"],"title":"How universities can help develop researchers' post-university careers","uri":"/posts/how-universities-can-help-develop/"},{"categories":null,"content":"1. It provided time to think. Career planning is not something that I personally have never really taken time out to sit down and think about. It’s just one of those things that never seems a priority (though it definitely is!) If you’re busy in your existing job, you don’t get time to think about future moves. And, if you’re in the process of finding a new job, survival instinct usually kicks in and you immediately start searching for vacancies and sending applications, rather than spending a long time considering what you actually want. The course’s structure meant that it forced us to consider our career path head-on, rather than putting it off for another day. We had regular group meetings, and had to spend time between meetings working on exercises, which were designed to get us thinking about career options and help narrow down the possible choices. ","date":"2013-08-18","objectID":"/posts/how-universities-can-help-develop/:1:1","tags":["researcher","university","postdoc","PhD","job","career"],"title":"How universities can help develop researchers' post-university careers","uri":"/posts/how-universities-can-help-develop/"},{"categories":null,"content":"2. It provided time for the staff to understand our motivations. I’d previously considered consulting the university careers service. On reflection, I don’t think this would have been too effective. It’s hard for someone to advise you in a brief meeting without knowing you that well. They can certainly give you some basic pointers and point you to reading material to explore, but this isn’t going to be very specific advice geared to what you, as an individual, want. By contrast, the rapport that we developed with the staff over a period of weeks and months was a real benefit. They developed a great understanding of who we were and where we wanted to go, which helped them to give us personalised advice. ","date":"2013-08-18","objectID":"/posts/how-universities-can-help-develop/:1:2","tags":["researcher","university","postdoc","PhD","job","career"],"title":"How universities can help develop researchers' post-university careers","uri":"/posts/how-universities-can-help-develop/"},{"categories":null,"content":"3. It provided us with mentors who were willing to discuss ideas and approaches. Not only did we have the enjoyable regular meetings, but we had constant — and exemplary — email and phone support from the staff. Having the outside opinion of people who know a lot about career development is incredibly valuable: rather than dithering over some decision, say, for example, what to put on an application form, being able to ask somebody who’s going to have a well-considered opinion is a boon. Through this consultation, my CV was completely transformed in the space of a few days and several phone calls from being mediocre into something that I was happy to send out. ","date":"2013-08-18","objectID":"/posts/how-universities-can-help-develop/:1:3","tags":["researcher","university","postdoc","PhD","job","career"],"title":"How universities can help develop researchers' post-university careers","uri":"/posts/how-universities-can-help-develop/"},{"categories":null,"content":"4. It provided peer support. Job searching is usually carried out in isolation. Because of that, it’s easy to worry and lose perspective on your situation, especially when facing rejection. Knowing the similar trials that others are going through is a great way to put some perspective on your own troubles and really does help to keep you motivated, rather than disheartened. That outside perspective is incredibly helpful in stabilising your view of your situation which can easily become skewed if you’re experiencing rejection. In the six months before the course started, I’d applied for positions on and off, but I’d go into lulls of being incredibly fed up following rejections only to have to pick myself up and spend more time completing applications. Here, the continuous support, feedback and encouragement from peers (and staff) was a tremendous boost in keeping me motivated during the often frustrating task of applying for work. ","date":"2013-08-18","objectID":"/posts/how-universities-can-help-develop/:1:4","tags":["researcher","university","postdoc","PhD","job","career"],"title":"How universities can help develop researchers' post-university careers","uri":"/posts/how-universities-can-help-develop/"},{"categories":null,"content":"So, what should universities be doing? Well, as many PhD and postdoctoral researchers are unlikely to have a permanent academic career, universities should be making a decent effort to help researchers plan for a future outside university. Helping future alumni to hopefully go on to success in industry jobs is certainly a good advertisement for the university too. What I’ve learned is that career moves are a lot of work. The sooner career planning is started, the better. As glad as I am that I went through the process, it was a very intense five months' work and I would have much preferred to begun this much earlier. Retrospectively, it might even have been wise to started preparing when I first started my two year postdoc… Of course, it’s probably a little unreasonable to expect every researcher to be provided with such thorough and specialised one-to-one career coaching, though it would be nice. Nonetheless, providing ongoing peer support and the time for researchers to begin considering their careers shouldn’t be too difficult or expensive to arrange. Talks given by career advisers on things to consider in career decision making, and approaches to job searching would definitely be a start: I’ve recently learnt about several job search strategies that I hadn’t previously heard of. Also, hearing from speakers who’ve already made the progression from academic research to positions outside university could offer a helpful insight into how best to make this successful change. Perhaps these kinds of events for postdocs were already ongoing and I just wasn’t aware of them. (I knew there were lots of careers events for undergraduates, but job searching for experienced researchers is a distinct process.) In that case, it may be sensible for those in careers services to better publicise them or more strongly encourage researchers to take an afternoon or so out of the lab or office to attend. Finally, I should give a very special thank you to the three wonderful staff who ran the course at the University of Leeds and those who were responsible for it existing in the first place! Actually, as I learned through the course, more than you might think. As an experienced researcher, you’ve probably developed a lot of research, organisational, presentation, and team working skills that are readily transferable to work outside university, even if you move outside of the research area you’ve worked in for so long. ↩︎ ","date":"2013-08-18","objectID":"/posts/how-universities-can-help-develop/:2:0","tags":["researcher","university","postdoc","PhD","job","career"],"title":"How universities can help develop researchers' post-university careers","uri":"/posts/how-universities-can-help-develop/"},{"categories":null,"content":"In my last post, I described a few simple, but effective uses for a Raspberry Pi. Here are two more! ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:0:0","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"4. File server using Samba, and media server using MiniDLNA The Pi makes for a useful low cost file server. I used this guide to install Samba which enables file shares to be accessed via Windows (and also OS X). It’s pretty easy to follow step-by-step. Following that guide, you can also attach a USB drive, making for a cheap network-attached storage (NAS) solution. Furthermore, you can also install MiniDLNA to allow networked media devices to easily access video and audio on the Pi, though I’ve not tested this personally. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:1:0","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"5. get_iplayer: for UK users (or overseas users with e.g. VPN access) ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:0","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"What’s get_iplayer? For the benefit of those not in the UK, iPlayer is a BBC service that allows users to stream television and radio programmes for a limited time after they are broadcast. It also lets you download them for a limited time, although they are then bogged down with DRM, restricting how you can watch or listen. get_iplayer is a great open source project that really enhances the usefulness of iPlayer. First of all, it downloads TV and radio shows in DRM-free formats, allowing you to play them back on any device capable of playing those formats, without restriction. Second, it has a really useful PVR feature: you can queue up future programmes that are already in the BBC’s current schedule, and you can easily setup PVR searches (e.g. to look for a specific programme title) that can be configured to run periodically and download on any suitable matches. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:1","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Installation of get_iplayer I installed get_iplayer last year on my Pi running Rapsbian; I followed this guide. Since then however, it’s now much easier to install and maintain1, apparently you only need to enter this terminal command now: sudo apt-get install get-iplayer ffmpeg atomicparsley libmp3-info-perl For convenience, I changed the default directory that get_iplayer saves in so that it was within a directory that was publically shared via Samba, using a command like ./get_iplayer –-prefs-add –output=/home/shares/public/iPlayer (Note that for all these commands, I’m assuming that you have changed directory to whichever one get_iplayer is installed.) --prefs-add, unsurprisingly, adds settings to the get_iplayer defaults. I also used this command: ./get_iplayer --prefs-add --modes=best -–subtitles --subdir --nopurge --modes=best downloads the HD versions of a TV programme if it is available --subtitles downloads subtitles if available --subdir sorts programmes into subdirectories based on their title, while --nopurge prevents the deletion of programmes that were recorded more than 30 days ago. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:2","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Basic use There are plenty of usage examples in the documentation which is definitely worth a read and explains all the available options. A simple usage case would be to download all TV programmes that have “Top Gear” in the title or episode info: ./get_iplayer --get \"Top Gear\" By default, the type of programme is set to TV. This is equivalent to using the command line option --type=tv. If you are looking for radio shows instead, you need to specify --type=radio. Alternatively, you could use --type=tv,radio and get_iplayer will search both the TV and radio programme lists. If there are several programmes with the same title (e.g. Newsnight), you can search for all programmes first by: ./get_iplayer \"Newsnight\" It shows a list of matching programmes and programme numbers. When I ran this just now, it gave me several matches, but say that I wanted the broadcast from 01/07/2013. Among the nine matches this command gave me, it showed: 611: Newsnight - 01/07/2013, BBC Two, News, TV, default I can then get that specific programme by just --getting a programme number instead of a — title: ./get_iplayer --get 611 ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:3","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Adding searches to the PVR Adding searches to the PVR is very similar to downloading available shows. As the documentation shows, instead of the --get option, you use --pvr-add: ./get_iplayer --pvr-add=Top_Gear \"Top Gear\" This adds a search named Top_Gear which looks for programmes with Top Gear in the title or episode info. This doesn’t actually do anything until the PVR searches are executed by the command: ./get_iplayer --pvr Supposing that we grew tired of Clarkson, Hammond and May’s shenanigans, we can remove the PVR search entirely by --pvr-del: ./get_iplayer --pvr-del=Top_Gear or if we weren’t so sure, we could use --pvr-disable=\"Top Gear\" to disable the search and --pvr-enable=\"Top Gear\" to re-enable it at a later date. Further PVR management options are listed in the documentation. Additionally, there is also a web PVR interface, if you prefer that to using the command line to manage the PVR, although it’s advised to have it running on trusted networks for security reasons. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:4","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Scheduling the PVR to run However, it is more convenient to run the PVR regularly rather than have to run it manually. You can add the command to run the PVR to the cron task scheduler. Scheduling tasks by cron is carried out by entering crontab -e and then editing the file to start the job.` I had a problem where I could run the command get_iplayer --pvr and the PVR would work fine from a terminal window, but it failed when running via cron. This was fixed by finding the contents of my current PATH variable (by typing echo $PATH into the terminal) and then copying this into a line in my crontab that started PATH=. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:5","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Why I think the PVR is great What’s awesome about the PVR function is that the search terms are persistent. If the PVR is looking out for a particular show, you don’t even need to keep an eye out for when your favourite shows start a new series, in case you forget to record it. The PVR feature will automatically catch them for you, without you thinking about it. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:6","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Custom commands after downloading recordings This is another useful feature of get_iplayer. --command runs custom commands after downloading a recording and can pass various bits of information to those commands as arguments. Because of this feature, it’s easy to use your own scripts to process your recordings. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:7","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Example: automatic downloading of radio tracklistings Sometimes I’ll hear a track that a DJ plays on a radio show and I’ll want to make a note of it to look it up later. It’s not always that easy to do, occasionally they neglect to mention the title and artist, which can be frustrating when you’ve heard something amazing. At least these days tracklistings are usually posted online, but if I’m listening on my iPod Nano (running Rockbox), this means I still need to dig out my phone, fire up a web browser and look it up. It’s much easier if the tracklisting is stored together with the radio show for quick reference. To automate this, I wrote a Python script that downloads BBC tracklistings (bbc_tracklist.py). Once a radio show has been downloaded using get_iplayer, the corresponding tracklisting is saved to a text file in the same directory. All the bbc_tracklist.py script needs to know is the programme id, which get_iplayer can pass through to it as \u003cpid\u003e. For example: ./get_iplayer --get \"Gilles Peterson\" --type=radio --command \"/home/get_iplayer/bbc_tracklist.py \u003cpid\u003e \u003cdir\u003e \u003cfileprefix\u003e\" My script optionally takes arguments of the directory (to store the tracklisting with the downloaded show) and a filename (to give the text file the same base filename). These are provided by get_iplayer via \u003cdir\u003e and \u003cfileprefix\u003e. It also works in the same way with PVR searches. ./get_iplayer --pvr-add=Benji_B --type=radio \"Benji B\" --command \"/home/get_iplayer/bbc_tracklist.py \u003cpid\u003e \u003cdir\u003e \u003cfileprefix\u003e Once this has been setup in a radio PVR search, you can forget about the script entirely. It should execute automatically everytime a show has been downloaded. ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:8","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":"Conclusion get_iplayer does its job superbly well. It has many useful features and seems very stable. Comparing get_iplayer to the TV tuner my main PC has installed, get_iplayer has been used much more for “recording”. Why should this be? It doesn’t matter if you miss a show with get_iplayer since you can still download it several days after broadcast and, with the PVR feature, you probably won’t miss it. This isn’t possible with a TV card if you missed the only broadcast. get_iplayer also circumvents entirely the problems I occasionally have with my TV signal. Combined with a Samba or miniDLNA setup described above, you can leave the Pi running to periodically save programmes and then either playback files across the network or copy them to another device. Finally, shows downloaded by get_iplayer are ready to playback on many devices immediately; this isn’t always the case with TV captures which might require some post-record processing. Of course, I should mention that, since the video formats are unencumbered with DRM, it’s possible to play them with, of course, Raspbmc. :) So, there you have it. Five handy uses for a Raspberry Pi. If I find further uses for my Pi, I’ll definitely be writing about them. In the meantime, feel free to let me know via a comment what you’re using your Pi for! (That should also help keep it up to date as it should be upgraded when you run a sudo apt-get upgrade; my install is currently manually updated by running ./get_iplayer --update in the install directory.)` ↩︎ ","date":"2013-07-17","objectID":"/posts/uses-for-a-raspberry-pi-part-2/:2:9","tags":["server","Samba","radio","Pi","tracklisting","Raspberry","iplayer","Python","get_iplayer"],"title":"Uses for a Raspberry Pi (part 2)","uri":"/posts/uses-for-a-raspberry-pi-part-2/"},{"categories":null,"content":" Don’t buy one and put it in the cupboard; buy one and do an interesting thing with it and tell the world what you did. Eben Upton When I finally got my hands on the low priced Raspberry Pi computer, it was autumn. Although it had been released in February, its low price and the engagement of the Raspberry Pi Foundation with the user and developer community meant that people were clamouring for them and supplies were scarce. This explains why it was very difficult to get hold of for several months. Since then, I’ve found several uses for my Pi, and I thought it might be useful, in the spirit of the opening quote, to briefly discuss them. My uses for the Pi are fairly simple, especially in comparison to those that frequently adorn the official Raspberry Pi site, but I’ve still found them very handy nonetheless. I wouldn’t have been particularly unhappy if I’d spent £30 on a device that does just one of the tasks that I using the Pi for. It’s no surprise that a device that does all of these tasks, and can still do more, makes the Pi one of my favourite bits of recent hardware. I’m certainly not alone in that view: the driving force behind the Pi, Eben Upton, was quoted as initially hoping for 1,000 sales; a huge underestimate from the hundreds of thousandsit has sold. One of the most obvious uses for a Pi is to run XBMC, the fantastic open source media center software. The fact that XBMC was shown to run pretty smoothly prior to the Pi’s release no doubt played a part in the Pi’s early popularity, considering the feature set that XBMC alone offers. That said, my assumption is that anyone with a Pi already knows about XBMC — in the unlikely event that you don’t, make sure to try Raspbmc out — therefore I’ll skip over it here and leave that discussion to the many pages that discuss XBMC on the Pi. Instead, I’ll focus on other useful functions. For reference, at the time of writing, Raspbian is the officially recommended operating system and that’s what I’m running on my Pi. All of the uses for my Pi use it headless. I usually connect to the Pi via an SSH client, though you could attach a keyboard and TV/monitor to it and enter terminal commands directly. So, what can you use a Pi for? ","date":"2013-07-10","objectID":"/posts/uses-for-a-raspberry-pi-part-1/:0:0","tags":["adapter","Pi","Airprint","Raspberry","wireless","wifi"],"title":"Uses for a Raspberry Pi (part 1)","uri":"/posts/uses-for-a-raspberry-pi-part-1/"},{"categories":null,"content":"1. Access to useful Linux command line tools Even just installing a Linux distribution like Raspbian, without installing anything else, gives you access to several powerful programs (particularly the text processing commands) that you might not have ready access to, if you don’t already have another PC running Linux available. Yes, you can get access to some of these tools within Windows if you install something like GnuWin32 or Cygwin, or go to the trouble of installing Linux in a virtual machine, but it’s nice to just power on a device and start working with these tools immediately and without any further setup. Another example is that when I started using github, I found it easy to just run the git command line tool from the Pi. It just worked. In my case, git had already been installed, so I didn’t even need to type sudo apt-get install git. This was a lot simpler than me trying to decide on the best way to run git on Windows. ","date":"2013-07-10","objectID":"/posts/uses-for-a-raspberry-pi-part-1/:1:0","tags":["adapter","Pi","Airprint","Raspberry","wireless","wifi"],"title":"Uses for a Raspberry Pi (part 1)","uri":"/posts/uses-for-a-raspberry-pi-part-1/"},{"categories":null,"content":"2. Wireless adapter for a device without onboard wifi I installed a cheap Ralink wifi adapter (around £4 from eBay) to connect my Pi to a wireless network. Because of this, the Pi’s onboard Ethernet port was going unused. It’s relatively straightforward to configure the Pi to enable a device connected to it by Ethernet to use the Pi’s wifi connection to access the network Wifi is a common feature of networked devices these days, but the Xbox 360 is an exception to this rule. Earlier this year, I explained in a Stack Exchange post how to configure the Pi such that the Xbox can access the network. There’s no reason why the same procedure couldn’t be used for other networked devices without wifi. ","date":"2013-07-10","objectID":"/posts/uses-for-a-raspberry-pi-part-1/:2:0","tags":["adapter","Pi","Airprint","Raspberry","wireless","wifi"],"title":"Uses for a Raspberry Pi (part 1)","uri":"/posts/uses-for-a-raspberry-pi-part-1/"},{"categories":null,"content":"3. AirPrint server Earlier this year, I was asked, “How can I print something from my iPad?” My reply was along the lines of, “Not easily: we don’t have a printer that supports AirPrint.” That conversation inspired me to have a look around in case it was actually possible. After a quick search, I found airprint-generate, along with a step-by-step guide explaining how to install this on the Pi. Even with having to install CUPS and testing a couple of iDevices, I don’t think the whole setup took more than thirty minutes or so. With this AirPrint solution, printing from an iPad is integrated just as if we used an AirPrint-compatible printer; no third party app is necessary to print. As its price suggests, the Pi isn’t hugely powerful, but that doesn’t hinder it from being incredibly useful for applications where computing power isn’t critical, like the AirPrint server. (Here, even if it runs a little slow, the response time isn’t really an issue: would a user really notice the extra few seconds that a document might take to print, compared with a much faster PC?) The Pi is also very cheap to run, as well as being silent, making it ideal for server applications with modest computing power requirements. Even if you are starting from scratch, and need to run the Pi’s operating system installer first, it should only take you, at most, a couple of hours to get everything here running. In a single evening, you can complete a small project that you get something back from almost immediately, which is really encouraging. On that upbeat tone, that’s probably a good place to end this post! When I write about the Pi next, I’ll cover uses of the Pi as a file server (using Samba), and how I’ve found get_iplayer to be a great alternative to the BBC’s officially supported service. ","date":"2013-07-10","objectID":"/posts/uses-for-a-raspberry-pi-part-1/:3:0","tags":["adapter","Pi","Airprint","Raspberry","wireless","wifi"],"title":"Uses for a Raspberry Pi (part 1)","uri":"/posts/uses-for-a-raspberry-pi-part-1/"},{"categories":null,"content":"I had my Gmail account setup to be accessed via POP in Mozilla Thunderbird and thought it was about time to shift to using IMAP instead. In fact, it’s that long ago that I can’t actually remember why I didn’t just use IMAP in the first place. Why should you be bothered about how your email is accessed? As long as you can read it, what’s the difference? As explained in this Gmail blog post, when using POP, changes made on a local device (e.g. marking mail as read or sorting mail into folders) aren’t reflected on the mail server. When you access your mail from a different device (or client), you’ll find your mail organised as if you hadn’t made any of those changes at all. IMAP, on the other hand, synchronises any changes you make locally to the mail server, so your mail will be organised in the same way when using IMAP, no matter which device or client you use. The changes will propagate to the next device you use. With the prevalence of smartphones and mobile data connections becoming more usable, it’s increasingly the case that multiple devices, rather than a single PC, are used to access mail. So, it seemed a good move to make the switch now. The approach I used was the accepted answer here: setup the IMAP account in Thunderbird so that you have both the old POP account alongside a new IMAP account, and then just drag folders from the POP to the IMAP. It’s not much more complicated than that, but these are the specific steps I used for migration: Enabled IMAP access on Gmail. Moved everything in the current Gmail inbox — all this mail was already on my PC via the POP account — into the bin. (I actually did this after I migrated all my folders, but you could do it here while logged into Gmail via the web.) Backed up my current Thunderbird profile, in case anything goes wrong or you want to quickly revert back to using POP for some reason. Setup the Gmail IMAP account in Thunderbird. Stopped the POP account from checking for new messages, to avoid the possibility of the POP account redownloading all the sorted mail that was currently being copied from the local POP folders back to Gmail. (Untick the two boxes in Tools menu \u003e Account Settings \u003e Server Settings \u003e Check for new messages…) Dragged each folder from the POP account to the IMAP account. This will probably be by far the slowest step if you have plenty of mail. Don’t forget your Sent Mail folder. Once I was happy that everything had migrated across, I emptied the Gmail bin, and then removed the POP account from Thunderbird (Tools menu \u003e Account Settings \u003e Account Actions \u003e Remove Account). Overall, it was simple. The only time consuming part was actually moving the messages from the copies I had in the Thunderbird POP account to the IMAP account. The only other (optional) thing I did was keep a local copy of all my previous mail for quicker message loading and searching, and as a backup. Make sure that the appropriate account setting is ticked (Tools menu \u003e Account Settings \u003e Synchronisation \u0026 Storage \u003e “Keep messages for this account on this computer”). Next, right click on a mail folder, go to the Synchronisation tab and click Download Now. Repeat for each mail folder. ","date":"2013-07-07","objectID":"/posts/migrating-email-from-pop-to-imap/:0:0","tags":["Thunderbird","migration","POP","IMAP","Gmail"],"title":"Migrating email from POP to IMAP in Thunderbird","uri":"/posts/migrating-email-from-pop-to-imap/"},{"categories":null,"content":"Around six months ago, I wrote a Python script to scrape Nature’s job website. I haven’t been using it much lately, but I tested it today and found it had broken. After an hour or so spent on it this afternoon, it’s working again. I found it pretty useful when I was searching for science jobs earlier this year. It’s a lot easier to skim through a single HTML table than to click through tens of pages with twenty jobs on each page. (You can also include keywords for location, employer or job title to filter out. I was working from the principle of not really knowing what kind of job I was looking for, so wanted to see most jobs, but knew that I didn’t want, for example, all the positions for PhD training listed. Full instructions are on GitHub.) ","date":"2013-07-06","objectID":"/posts/scraping-natures-job-website/:0:0","tags":["scraping","Nature","naturejobs","web","Python","script"],"title":"Scraping Nature's job website","uri":"/posts/scraping-natures-job-website/"},{"categories":null,"content":"A couple of days ago, I read this O’Reilly report, Analyzing the Analyzers, by Harlan Harris and co-authors with a lot of interest. It’s a quick and pleasant read, and definitely worth looking at. Considering the many news articles I see mentioning big data, the authors make a sobering observation on this subject (p16-17): “Most data scientists rarely work with terabyte or larger data… True big data work seems limited to a relative small subset of data scientists.” Since it was a self-selecting survey, there is the possibility of bias to consider, but the drop off when comparing the percentage of respondents regularly working with terabyte data sets compared to gigabyte data sets is staggering. The survey was also completed around a year ago, so there is always the possibility that trends have changed since then, especially in the fast moving tech world. What the report highlighted for me was the diversity of skills that come under the umbrella of data scientist. They classified data scientists as having one of five specialisms: business, machine learning/big data, maths/operational research, programming, and statistics; all very different career paths. Related to this, I recently read a couple of posts on Ryan Swanstrom’s Data Science 101 blog (worth a look; there are some useful links there) stating that “data science is more than just statistics”. He concludes that taking a data science course is a better decision than taking statistics if you want to forge a data science career. Ryan distinguishes the two paths in terms of how they handle data. Data scientists consider existing data and find ways to do something with it, whereas statisticians may be involved in well-designed experiments from the outset. Although that’s quite a nice distinction, I think there’s a little more to it than that. A statistician may not know much about extracting data from different information sources, storing it and accessing it. On the other hand, a data scientist may be adept at these skills, but might not have the same depth of statistical knowledge. I certainly think that the topics covered in a data science course are going to be fairly widely applicable to several kinds of tech jobs, and will probably give a good grounding in several disciplines. One issue is that these courses are only just launching, so it will be a while before anyone can really see how effective they are in getting employment and on long-term career prospects compared with studying subjects such as statistics or computer science directly. Of course, even if you specialise, it is always possible to acquire some of the other necessary skills outside of the classroom. If you’re training to be a statistician, there’s nothing to stop you learning programming in your own time, or using tools like R or pandas for your work. Looking at the above report, the authors emphasise the need for a broad knowledge base (p19), which might imply that a more general data science course is a great idea. However, their very next point is that their opinion is that a strong specialism — “be it statistics, big data, or business communication” — makes for the best data scientists. Data science is more than just statistics, but it may not be that being a specialist in some field is disadvantageous either (particularly if most of your peers have data science qualifications and you have something unique to offer). In my case, after spending so long in education already, it would be difficult for me to justify taking more time out from employment to take one of the several data science courses that are springing up in the UK. So, for me, a final encouraging finding of this report is that one of the main routes to a data science career is from academic scientific research, and fits with DJ Patil’s view from several years ago that “the best data scientists tend to be ‘hard scientists’”. ","date":"2013-07-02","objectID":"/posts/types-of-data-scientist/:0:0","tags":["data","big","science","career"],"title":"Types of data scientist","uri":"/posts/types-of-data-scientist/"},{"categories":null,"content":"My phone doesn’t have a huge number of third party apps installed, but saw this chart today and it’s a little worrying nonetheless: Chart courtesy of Statista. I’m not sure exactly what “account info” describes. I did check the original report but it’s not defined there either. Also, I’m not sure whether the authors corrected for the fact that some apps might require some information — e.g. location access for some kind of mapping app — to actually be useful. As a result, these numbers could be an overestimate of the actual number of apps that are doing something unexpected with your data. Furthermore, it might explain why even a substantial proportion of paid apps require these permissions. (For ad-supported apps, location information might be requested for some targeted advertising component of the app.) It’s unlikely that Google will ever allow fine control of app permissions to enable the user to install an app without necessarily giving it access to e.g. your address book. The only options on stock Android are to take the app, warts and all, or do without. So, definitely worth considering installing PDroid to help deal with this issue. ","date":"2013-06-28","objectID":"/posts/leaky-phone-apps/:0:0","tags":["data","app","leaky","Android","Juniper","privacy","PDroid"],"title":"Leaky phone apps","uri":"/posts/leaky-phone-apps/"},{"categories":null,"content":"I’ve recently become a lot more interested in what data science involves — to the point of considering careers that use these skills — so when I noticed Coursera’s Introduction to Data Science course taught by Prof. Bill Howe at the University of Washington, I decided to take it. I finished the last assignment this weekend, so it’s a good time to write up my thoughts while they’re still fresh. The first week gave a quick overview of what data science is and why it’s becoming increasingly important. From then on, it covered a lot of topics. Starting with traditional SQL databases, we moved on to covering MapReduce algorithms, NoSQL databases, statistical topics relevant to large data sets, unsupervised and supervised machine learning algorithms, graph analytics, and looked at data visualisation considerations (particularly features that can help make visualisations easy for the viewer to comprehend). The lecture style was pretty brisk. Lectures were broken up into short 5-15 minute segments, but often information dense. This isn’t a bad thing. The fact the lectures are recorded means that you can repeat sections, or go and look something up for more details. Usually, the lectures weren’t too difficult to follow. There were a few occasions where I think a little too much was crammed in. One instance: rule learning by sequential covering was discussed in less than five minutes which was far too short to explain clearly. It took me another 20 minutes of reading and working through this explanation to actually grasp the concept properly. On the other hand, the emphasis on breadth rather than depth in the course fits well for an introductory course. It gives you sufficient grounding and the vocabulary to start understanding concepts you might encounter elsewhere, and points you in the right direction to look further. I was impressed at the considerable amount I learnt about technologies that I knew nothing about previously. The range of assignments was diverse: analysing tweets, using SQL databases and implementing some very simple MapReduce algorithms. There were also two open ended, assignments: using Tableau Desktop to do some data visualisation, and entering a Kaggle competition. In these, it was up to the student to choose a question and challenge themselves. This approach was a good way of handling the very varied academic backgrounds of students on the course. Entering a Kaggle competition was interesting. Even with the tutorials Kaggle provides, it’s a little intimidating if you haven’t tackled those kind of predictive model problems before. Because it was a part of the course, it gave me the motivation to have a go and try using scikit-learn; this is certainly a positive. I’m more aware now of the issues in working with other people’s data sets: often a large part of the problem is deciding how to handle missing data. (There were also a further two optional assignments, which I skipped due to a lack of spare time. One of these was a real world project, so there was no shortage of ways in which you could delve in more deeply should you wish.) Three of the assignments used Python and required some basic language knowledge to complete them. However, if you weren’t familiar with Python prior to the first assignment, being thrown in head first to it was probably not the most fun or easiest way to go about learning it. If you were going to take this course, it’s definitely worth working through a few tutorials beforehand to learn the basics in Python (data types, expressions, conditionals, loops, functions); these are good lectures and this is a good free book. There were some initial grading issues that meant the course didn’t get off to the start that was probably hoped. The first assignment was autograded but, with having to deal with Unicode strings, input data sets likely being unique for everyone, and opaque grading feedback, it made for a difficult experience. Completing the actual assignment itself didn’t require any comp","date":"2013-06-24","objectID":"/posts/courseras-introduction-to-data-science/:0:0","tags":["data","course","Coursera","analytics","science"],"title":"Coursera's Introduction to Data Science course","uri":"/posts/courseras-introduction-to-data-science/"},{"categories":null,"content":"I’m in the middle of taking the Coursera Data Science course which is currently discussing machine learning. The current assignment is to have a go at a Kaggle competition and write up the attempt. (Kaggle is a site where data modelling competitions are posted: participants are given some existing data set and the aim is to develop models to make predictions or decisions for unknown data.) This is why, earlier today, I was trying to install scikit-learn, a Python machine learning module. Unfortunately, when using pip to install it, I was getting the unhelpful error: “unable to find vcvarsall.bat”. This blog post pointed out the issue: you need a C++ compiler installed and configured correctly (e.g. MinGW or Visual Studio). Instead of having to do more installation and setup, the quickest solution was to use a portable Python distribution, like WinPython or Anaconda; these have several scientific modules installed already. Another option to using an existing Python distribution is to just install a precompiled package: there’s also Christoph Gohlke’s collection of compiled Python scientific packages available. ","date":"2013-06-09","objectID":"/posts/installing-python-modules-on-windows/:0:0","tags":["scikit-learn","install","Python"],"title":"Installing Python modules on Windows: unable to find vcvarsall.bat","uri":"/posts/installing-python-modules-on-windows/"},{"categories":null,"content":"As I mentioned previously, I’ve been looking for a new job over the past few months, since my current research contract is ending imminently. When I started looking, all that I knew was that I wanted to work somewhere other than a university, which doesn’t really narrow down options. I think I’ve a much clearer career direction now. However, the combination of imminent unemployment — at a time of high unemployment — and an uncertainty about what I actually wanted to do meant that, earlier this year, I was applying for anything that I thought that I had a good chance of getting. (In retrospect, I think this was crucial in helping me decide what I actually want to do.) One of these jobs was as a journal editor. Until now, my only dealings with journal editors have involved me submitting manuscripts to journals, and resolving any corrections. I hadn’t ever given much consideration as to what their role really involves. If you’re unfamiliar with the academic world, peer-reviewed journals are an important feature. Researchers send work to a journal and then, provided the editors and the peer reviewers deem it suitable, they’ll publish it. The reputation of the journal in which work is published is important: a researcher’s publication record has a massive influence on future funding applications. It can also hugely increase the audience of a research work: many scientific stories reported in mainstream media are often sourced from the big journals, like Nature or Science. Publishing in high profile journals is obviously both highly desirable and highly competitive. Editors are sent many manuscripts and then have the difficult task of reading through them and making quick decisions on whether to submit them for peer review. For the best journals, good papers are just not good enough. Maintaining the journal’s prestige means that they have to try to pick out the outstanding papers. As part of my editor application, I was sent three manuscripts covering three very different scientific topics. For this editing exercise, I had to summarise the main research finding and argue whether or not the paper should be sent for peer review. In one case, I understood quickly what their idea was, but the other two took me far longer to appreciate. A paper in your own area of expertise is fairly easy to consider. You instinctively have a feel for what is novel. On the other hand, with little experience of a subject, it’s incredibly difficult to appreciate the context and impact of a scientific work. You have to do your own background research to figure out what the current state of that area of research, where the paper you’re reading fits in, and therefore how important it is. This probably isn’t as much of a problem for editors of specialised journals: they have fairly narrow scopes. Editors in journals with a wider remit no doubt have a much tougher time here. So, if you’re an eager scientist sending your work to a high profile, multidisciplinary journal, what can you do? Well, apart from making sure that your research is clearly written up for other researchers, you also need to give consideration to the editor who will be the first hurdle to getting your work published. The background context, what your work contributes and the aspects distinguishing this work from other papers must all be absolutely clear. The editors reading your work may well be experienced scientists themselves, but they might be completely unfamiliar with that particular area of research. In the end, I had a pleasant interview which went well, although I didn’t actually get offered the job. (As I want to leave the physical sciences behind entirely, I think it would have been a misstep for me to take it if it were offered.) Nonetheless, the process was still a useful one when thinking about writing up my remaining papers. Finally, there’s a parallel of this process in submitting job applications. When applying for a job, you’ll often send a cover letter and CV. At some poin","date":"2013-05-25","objectID":"/posts/close-to-edit-what-researchers-can/:0:0","tags":["journal","research","job","academic","editing","publishing"],"title":"Close to the edit: what researchers can learn from journal editing","uri":"/posts/close-to-edit-what-researchers-can/"},{"categories":null,"content":"As my current work contract ends in a couple of months, I’m frantically looking for work which, of course, means reading many job postings. (Incidentally, I wrote a Python script earlier this year to help expedite my job hunting.) Usually, job specifications are pretty much just as you would expect from the job title. I’m not entirely sure whether the highlighted point in this vacancy was really necessary though: “So, you’re a maths expert. That’s great for this role! Oh, one final thing, if we happen to have a cake day at the office, are you going to be able to ensure that everyone gets an equal share?” ","date":"2013-05-23","objectID":"/posts/job-advertisements/:0:0","tags":["vacancy","advert","job","fraction"],"title":"Job advertisements","uri":"/posts/job-advertisements/"},{"categories":null,"content":"This week Firefox 21 was released and there are two features of it that have irritated me. First off, nowhere did I see an obvious prompt or mention that there’s now a feature (Firefox Health Report, FHR) that collects general browser, non-user data. It’s not a big deal since this comprises anonymous browser statistics only. On the other hand, the fact that I didn’t see any notification or an opt-in/opt-out prompt on upgrading irks me. (I only learned about it from The Register.) From Mozilla’s FHR FAQ: “We believe in giving users power over data, even data like this that isn’t personally identifiable or linkable to our users. FHR gives Firefox users control over data submitted by their browser. Users can use FHR data to better understand their browser’s performance or stability by comparing it to that of aggregate data from other browser configurations. At any time users can disable the feature or delete data associated with their browser.” Fine. It would have been nice to, you know, actually make certain that users were aware of this feature though. Secondly, and more directly annoying, left clicking on an MP3, which would show a save/open prompt, now starts playing the MP3 instead. (I’m assuming this feature is new in Firefox 21, since I can’t remember it being present previously. I’m also aware that you can right-click, and then choose Save Link As…, but it’s easier to just left-click.) My initial thought was to check the list of MIME types (Options \u003e Applications) to see if anything had changed. No, it still looked just as it did before: The about:config option to restore the original behaviour is to set media.windows-media-foundation.enabled to false, although this disable in-browser playback of some other media formats. Again, it’s not a big problem, but it’s counter-intuitive to let the user set an option that purportedly controls what happens when a link is clicked, and then ignore it for no discernable reason. ","date":"2013-05-17","objectID":"/posts/firefox-21-annoyances/:0:0","tags":["annoyance","Windows","Firefox"],"title":"Firefox 21 annoyances","uri":"/posts/firefox-21-annoyances/"},{"categories":null,"content":"I’m currently taking the “Introduction to Data Science” course via Coursera (probably a subject itself for another post as it’s been pretty interesting so far even though it’s only been going for just over two weeks). The lecture slides are provided as PDFs, but I like to annotate them with my own notes too. I had been using Print Screen and then just cropping the copied image. But, since I already had ImageMagick installed, I thought I should try automatically converting the PDF slides to individual images. A command like: convert -density 300 input.pdf outputname_%0d.png works pretty well to convert to PNGs. (If using the GraphicsMagick fork of ImageMagick, you’ll need to add +adjoin to the command line, otherwise you only get the first page of the PDF which is named outputname\\_%0d.png\".) ","date":"2013-05-16","objectID":"/posts/converting-pdfs-to-series-of-images/:0:0","tags":["PDF","ImageMagick","convert"],"title":"Converting PDFs to a series of images","uri":"/posts/converting-pdfs-to-series-of-images/"},{"categories":null,"content":"I like to uninstall software that I don’t really anticipate using on my PC in the near future. This is partly just to keep my PC free from clutter, but it also reduces the number of packages that I need to watch for security updates. Secunia’s software helps a lot with this, and there’s always Ninite and Chocolately too, but limiting the programs installed to those I actively use helps a lot. So, when I’d finish using GoToMeeting’s client for a one-off meeting, I was looking to uninstall it. However, there wasn’t an entry in the list of installed programs in the Windows Control Panel, contrary to Citrix’s FAQ: “If desired, an attendee may uninstall all GoToMeeting files using the Add/Remove Programs feature in the Windows control panel.” Fortunately, this issue has already been documented and the command to use is g2muninstall.exe /uninstall My install was actually located in Program Files (x86), rather than a user folder. (I’d assumed it needed administrative privileges when running the installer, so that might be why it placed it there.) Because of this, I needed to execute this command as an admin, although running this with a standard user account, the uninstaller was still happy to cheerily tell me that “GoToMeeting has been uninstalled”, even though it hadn’t done a thing. ","date":"2013-05-14","objectID":"/posts/uninstalling-gotomeeting/:0:0","tags":["uninstall","GoToMeeting","Citrix"],"title":"Uninstalling the Windows GoToMeeting client","uri":"/posts/uninstalling-gotomeeting/"},{"categories":null,"content":"!!! article-edit \"\" Edit 2014/03/28: Auto-Patcher is deprecated; see my more recent post. Over the past six months or so, I’ve been running the very slick port of CyanogenMod 9.1 to Huawei’s G300 phone by Dazzozo and collaborators as posted at g300.modaco.com. (It’s considerably better than the official ICS ROM provided by Huawei and Vodafone.) For several of the previous releases, a user had been supplying zip patches that could be flashed through recovery which allowed PDroid to function. Unfortunately, they seemed to have stopped providing them and this was one thing that was holding me back from upgrading my CM9 install to a newer version. If you’re unfamiliar with PDroid, it allows granular control of the data that each app on your phone can access. For example, you might install an app which requests location access permissions that aren’t really necessary to the app’s function. Using PDroid, you can override these permissions, blocking the app’s access to this data. (I’m not overly paranoid, but neither do I want my phone spewing out personal data for no good reason.) Other than basics of flashing ROMs, I know very little about Android. I assumed that patching a ROM in this way was fairly involved. It’s actually really simple: Download a zipped copy of the latest Auto-Patcher and a copy of the ROM you want to modify. Boot into Linux; I used a DVD copy of Ubuntu that I had lying around, an existing Linux install or virtual machine would work equally well. (There is a Windows version of Auto-Patcher, though I didn’t test this.) Install the Java JDK: sudo apt-get install openjdk-7-jre openjdk-7-jdk (you also require the cpio and patch packages to be installed, I found these were installed already, otherwise you need to sudo apt-get install cpio and sudo apt-get install patch). Extract the auto-patcher zip to a folder and copy the (still zipped) ROM to the same folder. As the Auto-Patcher instructions state, you need to open a terminal window, navigate to the auto-patcher folder and run ./batch.sh. Now, run ./auto_patcher \u003cROMNAME\u003e.zip \u003cMODTYPE\u003e \u003cROMTYPE\u003e. The MODTYPE is pdroid and, in my case, the ROMTYPE was cm9, so I used a command line like: ./auto_patcher cm-9-20130428-EXPERIMENTAL-u8815-R10.zip pdroid cm9 This process should result in two new zips in the Auto-Patcher folder: an update and a restore. All that is left to do is to copy both the zipped ROM and the update zip generated by Auto-Patcher to your phone’s storage. You can then use an Android custom recovery (such as ClockworkMod Recovery) to flash the ROM, followed by the update zip through an Android custom recovery. The restore zip that auto_patcher generates can be used, as the name suggests, to return the ROM to the unpatched state. Finally, you just need to install the PDroid app itself from the Play Store, and you’re finished! ","date":"2013-05-11","objectID":"/posts/patching-android-roms-for-pdroid-using/:0:0","tags":["Android","PDroid","Auto-Patcher"],"title":"Patching Android ROMs for PDroid using Auto-Patcher","uri":"/posts/patching-android-roms-for-pdroid-using/"},{"categories":null,"content":"I’ve been wanting to start writing more and I figured a blog would be a sensible outlet for that. The problem is that it’s very easy — especially for me — to become distracted with the implementation, and then lose focus on the writing itself. The reason for choosing Blogger is mainly that I can use it on my own domain for free (unlike paying for domain mapping with wordpress.com). Yes, I could run WordPress on my own server, but then I start paying server costs for something that I might give up on in a few months. It also gives me something else to maintain, particularly important in the light of recent incidents. Also, if I start considering running my own server, then there are even more options. Maybe I should be using WordPress, but maybe I should be using a custom Django system instead? Because of this, the conclusion I reached is that it’s certainly worth looking into the alternatives in the future, but, in the short term, it’s best to just start writing first. What I think I’m trying to say is: please excuse the mess. I’ll certainly make an effort to make this look better in the future, rather than just using a stock template. ","date":"2013-05-11","objectID":"/posts/blogger-versus-wordpress/:0:0","tags":["Wordpress","Blogger","blogging"],"title":"Blogger versus WordPress","uri":"/posts/blogger-versus-wordpress/"}]